{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Hierarchical BDH\n",
    "\n",
    "Fine-tune a pretrained Hierarchical BDH model on a new domain.\n",
    "\n",
    "## Fine-tuning Strategies\n",
    "\n",
    "| Strategy | What's Trained | Use Case |\n",
    "|----------|----------------|----------|\n",
    "| **Full** | All parameters | Maximum adaptation |\n",
    "| **Local-only** | Local model + cross-attn | Domain style, preserve knowledge |\n",
    "| **Global-only** | Global model | Document structure |\n",
    "| **Head-only** | LM head only | Quick adaptation |\n",
    "| **LoRA-style** | Added adapters | Parameter efficient |\n",
    "\n",
    "## Workflow\n",
    "1. Upload pretrained checkpoint\n",
    "2. Choose fine-tuning strategy\n",
    "3. Upload/select fine-tuning data\n",
    "4. Train and evaluate\n",
    "5. Download fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || (cd bdh && git pull)\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model\n",
    "\n",
    "Upload your pretrained checkpoint from a training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload pretrained checkpoint\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print('Upload your pretrained checkpoint (.zip or .pt):')\n",
    "    uploaded = files.upload()\n",
    "    uploaded_file = list(uploaded.keys())[0]\n",
    "except ImportError:\n",
    "    uploaded_file = 'checkpoints_hierarchical_small/best.pt'  # Edit for local\n",
    "    print(f'Using: {uploaded_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract if zip\n",
    "if uploaded_file.endswith('.zip'):\n",
    "    with zipfile.ZipFile(uploaded_file, 'r') as z:\n",
    "        z.extractall('.')\n",
    "    for root, dirs, files_list in os.walk('.'):\n",
    "        for f in files_list:\n",
    "            if f == 'best.pt':\n",
    "                checkpoint_path = os.path.join(root, f)\n",
    "                break\n",
    "else:\n",
    "    checkpoint_path = uploaded_file\n",
    "\n",
    "print(f'Loading: {checkpoint_path}')\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "print(f'Pretrained on: {ckpt.get(\"dataset\", \"unknown\")}')\n",
    "print(f'Original PPL: {ckpt.get(\"val_ppl\", \"unknown\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model from config\n",
    "model_config = HierarchicalBDHConfig(**ckpt['model_config'])\n",
    "model = HierarchicalBDH(model_config).to(device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "params = model.count_parameters()\n",
    "print(f'\\nModel loaded: {params[\"total\"]:,} params ({params[\"total\"]/1e6:.1f}M)')\n",
    "print(f'  Global: {params[\"global_model\"]:,}')\n",
    "print(f'  Local:  {params[\"local_model\"]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Fine-tuning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# FINE-TUNING STRATEGY\n",
    "# =============================================================\n",
    "STRATEGY = 'local_only'  # Options: 'full', 'local_only', 'global_only', 'head_only'\n",
    "# =============================================================\n",
    "\n",
    "def freeze_params(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_params(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "# Apply strategy\n",
    "if STRATEGY == 'full':\n",
    "    print('Strategy: FULL fine-tuning (all parameters)')\n",
    "    # Everything trainable by default\n",
    "    \n",
    "elif STRATEGY == 'local_only':\n",
    "    print('Strategy: LOCAL-ONLY (freeze global, train local + cross-attn)')\n",
    "    freeze_params(model.global_model)\n",
    "    freeze_params(model.patch_embedder)\n",
    "    # Local model and lm_head remain trainable\n",
    "    \n",
    "elif STRATEGY == 'global_only':\n",
    "    print('Strategy: GLOBAL-ONLY (freeze local, train global)')\n",
    "    freeze_params(model.local_model)\n",
    "    freeze_params(model.lm_head)\n",
    "    # Global model and patch_embedder remain trainable\n",
    "    \n",
    "elif STRATEGY == 'head_only':\n",
    "    print('Strategy: HEAD-ONLY (freeze all, train only LM head)')\n",
    "    freeze_params(model)\n",
    "    unfreeze_params(model.lm_head)\n",
    "\n",
    "# Count trainable params\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'\\nTrainable: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Fine-tuning Data\n",
    "\n",
    "Options:\n",
    "1. Upload your own text file\n",
    "2. Use a HuggingFace dataset\n",
    "3. Use built-in examples (code, legal, medical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# DATA SOURCE\n",
    "# =============================================================\n",
    "DATA_SOURCE = 'huggingface'  # Options: 'upload', 'huggingface', 'example'\n",
    "\n",
    "# For huggingface:\n",
    "HF_DATASET = 'wikitext'\n",
    "HF_CONFIG = 'wikitext-2-raw-v1'\n",
    "HF_SPLIT = 'train'\n",
    "HF_TEXT_COLUMN = 'text'\n",
    "HF_SAMPLE_SIZE = 10000  # Number of examples to use\n",
    "\n",
    "# For example:\n",
    "EXAMPLE_DOMAIN = 'code'  # Options: 'code', 'legal', 'scientific'\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "if DATA_SOURCE == 'upload':\n",
    "    # Upload custom text file\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print('Upload your training data (.txt):')\n",
    "        uploaded_data = files.upload()\n",
    "        data_file = list(uploaded_data.keys())[0]\n",
    "        with open(data_file, 'r') as f:\n",
    "            train_text = f.read()\n",
    "    except ImportError:\n",
    "        with open('data/finetune_data.txt', 'r') as f:\n",
    "            train_text = f.read()\n",
    "\n",
    "elif DATA_SOURCE == 'huggingface':\n",
    "    from datasets import load_dataset\n",
    "    print(f'Loading {HF_DATASET}/{HF_CONFIG}...')\n",
    "    \n",
    "    if HF_CONFIG:\n",
    "        dataset = load_dataset(HF_DATASET, HF_CONFIG, split=f'{HF_SPLIT}[:{HF_SAMPLE_SIZE}]')\n",
    "    else:\n",
    "        dataset = load_dataset(HF_DATASET, split=f'{HF_SPLIT}[:{HF_SAMPLE_SIZE}]')\n",
    "    \n",
    "    train_text = '\\n'.join(dataset[HF_TEXT_COLUMN])\n",
    "    print(f'Loaded {len(dataset)} examples')\n",
    "\n",
    "elif DATA_SOURCE == 'example':\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    if EXAMPLE_DOMAIN == 'code':\n",
    "        print('Loading Python code...')\n",
    "        dataset = load_dataset('codeparrot/codeparrot-clean', split='train[:5000]')\n",
    "        train_text = '\\n\\n# === FILE ===\\n\\n'.join(dataset['content'])\n",
    "    elif EXAMPLE_DOMAIN == 'legal':\n",
    "        print('Loading legal text...')\n",
    "        dataset = load_dataset('pile-of-law/pile-of-law', 'r_legaladvice', split='train[:2000]')\n",
    "        train_text = '\\n\\n'.join(dataset['text'])\n",
    "    elif EXAMPLE_DOMAIN == 'scientific':\n",
    "        print('Loading scientific papers...')\n",
    "        dataset = load_dataset('scientific_papers', 'arxiv', split='train[:1000]')\n",
    "        train_text = '\\n\\n'.join(dataset['article'])\n",
    "\n",
    "# Convert to bytes\n",
    "print('Converting to bytes...')\n",
    "all_data = text_to_bytes(train_text)\n",
    "\n",
    "# Split 90/10\n",
    "split_idx = int(0.9 * len(all_data))\n",
    "train_data = all_data[:split_idx]\n",
    "val_data = all_data[split_idx:]\n",
    "\n",
    "print(f'\\nData sizes:')\n",
    "print(f'  Train: {len(train_data):,} bytes ({len(train_data)/1e6:.1f}MB)')\n",
    "print(f'  Val:   {len(val_data):,} bytes ({len(val_data)/1e6:.1f}MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size, patch_size=8):\n",
    "        block_size = (block_size // patch_size) * patch_size\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(1, len(self.data) - self.block_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        return chunk[:-1], chunk[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# FINE-TUNING CONFIG\n",
    "# =============================================================\n",
    "config = {\n",
    "    'block_size': 512,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,  # Lower than pretraining!\n",
    "    'weight_decay': 0.01,\n",
    "    'max_steps': 2000,\n",
    "    'warmup_steps': 100,\n",
    "    'val_interval': 200,\n",
    "    'val_batches': 50,\n",
    "    'patience': 5,\n",
    "    'log_interval': 50,\n",
    "}\n",
    "# =============================================================\n",
    "\n",
    "patch_size = model_config.patch_size\n",
    "config['block_size'] = (config['block_size'] // patch_size) * patch_size\n",
    "\n",
    "print('Fine-tuning config:')\n",
    "for k, v in config.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = ByteDataset(train_data, config['block_size'], patch_size)\n",
    "val_dataset = ByteDataset(val_data, config['block_size'], patch_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Only optimize trainable parameters\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    ")\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < config['warmup_steps']:\n",
    "        return config['learning_rate'] * step / config['warmup_steps']\n",
    "    progress = (step - config['warmup_steps']) / (config['max_steps'] - config['warmup_steps'])\n",
    "    return config['learning_rate'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches: break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    return avg_loss, math.exp(avg_loss), avg_loss / math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate before fine-tuning\n",
    "print('Evaluating pretrained model on new domain...')\n",
    "pre_loss, pre_ppl, pre_bpb = evaluate(model, val_loader, config['val_batches'])\n",
    "print(f'Before fine-tuning: loss={pre_loss:.4f}, ppl={pre_ppl:.2f}, bpb={pre_bpb:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'step': [], 'train_loss': [], 'val_loss': [], 'val_ppl': [], 'val_bpb': [], 'lr': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "ckpt_dir = Path(f'checkpoints_finetuned_{STRATEGY}')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f'\\nFine-tuning with {STRATEGY} strategy...')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "running_loss = 0\n",
    "\n",
    "pbar = tqdm(range(config['max_steps']), desc='Fine-tuning')\n",
    "\n",
    "for step in pbar:\n",
    "    try:\n",
    "        x, y = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        x, y = next(train_iter)\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    lr = get_lr(step)\n",
    "    for pg in optimizer.param_groups: pg['lr'] = lr\n",
    "    \n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if (step + 1) % config['log_interval'] == 0:\n",
    "        pbar.set_postfix({'loss': f'{running_loss/config[\"log_interval\"]:.3f}', 'lr': f'{lr:.2e}'})\n",
    "        running_loss = 0\n",
    "    \n",
    "    if (step + 1) % config['val_interval'] == 0:\n",
    "        val_loss, val_ppl, val_bpb = evaluate(model, val_loader, config['val_batches'])\n",
    "        train_loss, _, _ = evaluate(model, train_loader, config['val_batches'])\n",
    "        \n",
    "        history['step'].append(step + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        history['val_bpb'].append(val_bpb)\n",
    "        history['lr'].append(lr)\n",
    "        \n",
    "        gap = val_loss - train_loss\n",
    "        print(f'\\nStep {step+1}: train={train_loss:.3f}, val={val_loss:.3f}, ppl={val_ppl:.2f}, gap={gap:.3f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'model_config': model_config.__dict__,\n",
    "                'val_loss': val_loss, 'val_ppl': val_ppl,\n",
    "                'finetune_strategy': STRATEGY,\n",
    "                'pretrained_on': ckpt.get('dataset', 'unknown'),\n",
    "            }, ckpt_dir / 'best.pt')\n",
    "            print(f'  \u2713 New best!')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement ({patience_counter}/{config[\"patience\"]})')\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            print('\\nEarly stopping')\n",
    "            break\n",
    "\n",
    "print(f'\\nFine-tuning complete! Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best fine-tuned model\n",
    "ft_ckpt = torch.load(ckpt_dir / 'best.pt')\n",
    "model.load_state_dict(ft_ckpt['model_state_dict'])\n",
    "\n",
    "# Evaluate\n",
    "post_loss, post_ppl, post_bpb = evaluate(model, val_loader)\n",
    "\n",
    "print('=' * 60)\n",
    "print('FINE-TUNING RESULTS')\n",
    "print('=' * 60)\n",
    "print(f'Strategy: {STRATEGY}')\n",
    "print(f'Trainable params: {trainable:,} ({100*trainable/total:.1f}%)')\n",
    "print()\n",
    "print(f'{\"Metric\":<15} {\"Before\":>12} {\"After\":>12} {\"Change\":>12}')\n",
    "print('-' * 55)\n",
    "print(f'{\"Loss\":<15} {pre_loss:>12.4f} {post_loss:>12.4f} {post_loss-pre_loss:>+12.4f}')\n",
    "print(f'{\"Perplexity\":<15} {pre_ppl:>12.2f} {post_ppl:>12.2f} {post_ppl-pre_ppl:>+12.2f}')\n",
    "print(f'{\"BPB\":<15} {pre_bpb:>12.3f} {post_bpb:>12.3f} {post_bpb-pre_bpb:>+12.3f}')\n",
    "print()\n",
    "improvement = (pre_ppl - post_ppl) / pre_ppl * 100\n",
    "print(f'Perplexity improved by {improvement:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].axhline(y=pre_loss, color='gray', linestyle='--', label='Pre-FT')\n",
    "axes[0].plot(history['step'], history['train_loss'], 'b-', label='Train')\n",
    "axes[0].plot(history['step'], history['val_loss'], 'r-', label='Val')\n",
    "axes[0].set_xlabel('Step'); axes[0].set_ylabel('Loss')\n",
    "axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PPL\n",
    "axes[1].axhline(y=pre_ppl, color='gray', linestyle='--', label='Pre-FT')\n",
    "axes[1].plot(history['step'], history['val_ppl'], 'g-', label='Val')\n",
    "axes[1].set_xlabel('Step'); axes[1].set_ylabel('Perplexity')\n",
    "axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gap\n",
    "gaps = [v - t for v, t in zip(history['val_loss'], history['train_loss'])]\n",
    "axes[2].fill_between(history['step'], gaps, alpha=0.5, color='orange')\n",
    "axes[2].set_xlabel('Step'); axes[2].set_ylabel('Gap')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Fine-tuning ({STRATEGY})', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / 'finetune_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt, max_tokens=150, temperature=0.8):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([list(prompt.encode('utf-8'))], device=device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(idx, max_new_tokens=max_tokens, temperature=temperature, top_k=40)\n",
    "    return bytes(out[0].tolist()).decode('utf-8', errors='replace')\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    'The ',\n",
    "    'In the ',\n",
    "]\n",
    "\n",
    "print('Generation samples:')\n",
    "for p in prompts:\n",
    "    print(f'\\nPrompt: \"{p}\"')\n",
    "    print('-' * 40)\n",
    "    print(generate(model, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config\n",
    "with open(ckpt_dir / 'finetune_config.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'strategy': STRATEGY,\n",
    "        'trainable_params': trainable,\n",
    "        'total_params': total,\n",
    "        'pre_ppl': pre_ppl,\n",
    "        'post_ppl': post_ppl,\n",
    "        'improvement_pct': improvement,\n",
    "        **config,\n",
    "    }, f, indent=2)\n",
    "\n",
    "with open(ckpt_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Zip and download\n",
    "import subprocess\n",
    "zip_name = f'finetuned_{STRATEGY}.zip'\n",
    "subprocess.run(['zip', '-r', zip_name, str(ckpt_dir)], check=True)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_name)\n",
    "    print('\u2705 Download started!')\n",
    "except ImportError:\n",
    "    print(f'Saved: {zip_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "T4", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
