{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêâ Testing BDH's \"Unlimited Context\" Claim\n",
    "\n",
    "The BDH paper claims efficient handling of unlimited context. Let's actually test this.\n",
    "\n",
    "## What We're Testing\n",
    "\n",
    "| Claim | Test | Success Criteria |\n",
    "|-------|------|------------------|\n",
    "| Linear attention scales | Memory vs context length | O(n) not O(n¬≤) |\n",
    "| Long-range dependencies | Needle-in-haystack | Find info at any position |\n",
    "| Hierarchical helps | Compare flat vs hierarchical | Better long-range retrieval |\n",
    "| Context utilization | Perplexity vs context | PPL improves with more context |\n",
    "\n",
    "## Hardware Requirements\n",
    "- GPU with 16GB+ VRAM recommended\n",
    "- A100 for 32K+ context tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install torch datasets transformers matplotlib tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo if needed\n",
    "import os\n",
    "if not os.path.exists('bdh.py'):\n",
    "    !git clone https://github.com/newsbubbles/bdh.git temp_bdh\n",
    "    !cp temp_bdh/bdh.py .\n",
    "    !cp temp_bdh/hierarchical_bdh.py .\n",
    "    !rm -rf temp_bdh\n",
    "\n",
    "from bdh import BDH, BDHConfig\n",
    "from hierarchical_bdh import HierarchicalBDH, HierarchicalBDHConfig\n",
    "print('Models loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Memory Scaling with Context Length\n",
    "\n",
    "BDH claims linear attention. Let's measure actual memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_scaling(model_class, config_class, context_lengths, batch_size=1, **config_kwargs):\n",
    "    \"\"\"Measure peak memory for different context lengths.\"\"\"\n",
    "    results = {'context_len': [], 'memory_mb': [], 'time_ms': []}\n",
    "    \n",
    "    for ctx_len in tqdm(context_lengths, desc='Testing context lengths'):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            config = config_class(**config_kwargs)\n",
    "            model = model_class(config).to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Create input\n",
    "            x = torch.randint(0, 256, (batch_size, ctx_len), device=device)\n",
    "            \n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                _ = model(x)\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Timed forward pass\n",
    "            start = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                for _ in range(3):\n",
    "                    _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = (time.perf_counter() - start) / 3 * 1000  # ms\n",
    "            \n",
    "            peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "            \n",
    "            results['context_len'].append(ctx_len)\n",
    "            results['memory_mb'].append(peak_mem)\n",
    "            results['time_ms'].append(elapsed)\n",
    "            \n",
    "            del model, x\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f'OOM at context length {ctx_len}')\n",
    "                break\n",
    "            raise\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test context lengths (powers of 2)\n",
    "context_lengths = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "\n",
    "# Adjust based on your GPU\n",
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "if gpu_mem >= 40:\n",
    "    context_lengths.extend([32768, 65536])\n",
    "    print('A100 detected - testing up to 64K context')\n",
    "elif gpu_mem >= 20:\n",
    "    context_lengths.append(32768)\n",
    "    print('Large GPU detected - testing up to 32K context')\n",
    "else:\n",
    "    print(f'Testing up to 16K context (GPU has {gpu_mem:.1f}GB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Standard BDH\n",
    "print('Testing Standard BDH memory scaling...')\n",
    "bdh_results = measure_memory_scaling(\n",
    "    BDH, BDHConfig, context_lengths,\n",
    "    n_layer=4, n_embd=256, n_head=4, mlp_internal_dim_multiplier=64\n",
    ")\n",
    "print(f'Standard BDH tested up to {max(bdh_results[\"context_len\"])} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hierarchical BDH\n",
    "print('Testing Hierarchical BDH memory scaling...')\n",
    "hier_results = measure_memory_scaling(\n",
    "    HierarchicalBDH, HierarchicalBDHConfig, context_lengths,\n",
    "    local_n_layer=2, global_n_layer=2, n_embd=256, n_head=4,\n",
    "    patch_size=16, mlp_internal_dim_multiplier=64\n",
    ")\n",
    "print(f'Hierarchical BDH tested up to {max(hier_results[\"context_len\"])} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Memory scaling\n",
    "ax = axes[0]\n",
    "ax.plot(bdh_results['context_len'], bdh_results['memory_mb'], 'b-o', label='Standard BDH', linewidth=2)\n",
    "ax.plot(hier_results['context_len'], hier_results['memory_mb'], 'r-s', label='Hierarchical BDH', linewidth=2)\n",
    "\n",
    "# Add reference lines for O(n) and O(n¬≤)\n",
    "x_ref = np.array(bdh_results['context_len'])\n",
    "y_linear = x_ref / x_ref[0] * bdh_results['memory_mb'][0]\n",
    "y_quadratic = (x_ref / x_ref[0])**2 * bdh_results['memory_mb'][0] * 0.1\n",
    "ax.plot(x_ref, y_linear, 'g--', alpha=0.5, label='O(n) reference')\n",
    "ax.plot(x_ref, y_quadratic, 'k--', alpha=0.5, label='O(n¬≤) reference')\n",
    "\n",
    "ax.set_xlabel('Context Length (tokens)')\n",
    "ax.set_ylabel('Peak Memory (MB)')\n",
    "ax.set_title('Memory Scaling with Context Length')\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Time scaling\n",
    "ax = axes[1]\n",
    "ax.plot(bdh_results['context_len'], bdh_results['time_ms'], 'b-o', label='Standard BDH', linewidth=2)\n",
    "ax.plot(hier_results['context_len'], hier_results['time_ms'], 'r-s', label='Hierarchical BDH', linewidth=2)\n",
    "ax.set_xlabel('Context Length (tokens)')\n",
    "ax.set_ylabel('Forward Pass Time (ms)')\n",
    "ax.set_title('Compute Time Scaling')\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('context_scaling.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print('\\n' + '='*60)\n",
    "print('MEMORY SCALING ANALYSIS')\n",
    "print('='*60)\n",
    "for name, results in [('Standard BDH', bdh_results), ('Hierarchical BDH', hier_results)]:\n",
    "    if len(results['context_len']) >= 2:\n",
    "        # Compute scaling exponent (log-log slope)\n",
    "        x = np.log2(results['context_len'])\n",
    "        y = np.log2(results['memory_mb'])\n",
    "        slope = np.polyfit(x, y, 1)[0]\n",
    "        print(f'{name}: Memory scales as O(n^{slope:.2f})')\n",
    "        if slope < 1.2:\n",
    "            print(f'  ‚úì Near-linear scaling!')\n",
    "        elif slope < 1.5:\n",
    "            print(f'  ~ Slightly superlinear')\n",
    "        else:\n",
    "            print(f'  ‚úó Superlinear scaling (approaching quadratic)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 2: Needle in a Haystack\n",
    "\n",
    "Can BDH actually *use* long context? We'll hide information at various positions and test retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_needle_haystack_data(needle_text, haystack_length, needle_position_ratio):\n",
    "    \"\"\"\n",
    "    Create a needle-in-haystack test case.\n",
    "    \n",
    "    Args:\n",
    "        needle_text: The important text to hide (bytes)\n",
    "        haystack_length: Total length of the sequence\n",
    "        needle_position_ratio: Where to place needle (0.0 = start, 1.0 = end)\n",
    "    \"\"\"\n",
    "    # Create random 'haystack' bytes (printable ASCII range)\n",
    "    haystack = torch.randint(32, 127, (haystack_length,))\n",
    "    \n",
    "    # Convert needle to bytes\n",
    "    needle_bytes = torch.tensor([b for b in needle_text.encode('utf-8')])\n",
    "    needle_len = len(needle_bytes)\n",
    "    \n",
    "    # Calculate insertion position\n",
    "    max_pos = haystack_length - needle_len\n",
    "    insert_pos = int(max_pos * needle_position_ratio)\n",
    "    \n",
    "    # Insert needle\n",
    "    haystack[insert_pos:insert_pos + needle_len] = needle_bytes\n",
    "    \n",
    "    return haystack, insert_pos\n",
    "\n",
    "# Test\n",
    "needle = \"The secret code is DRAGON42.\"\n",
    "test_seq, pos = create_needle_haystack_data(needle, 1000, 0.3)\n",
    "print(f'Needle placed at position {pos}')\n",
    "print(f'Context around needle: {bytes(test_seq[pos-10:pos+len(needle)+10].tolist()).decode(\"utf-8\", errors=\"replace\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_needle_retrieval(model, needle_text, context_length, needle_position, device):\n",
    "    \"\"\"\n",
    "    Test if model can predict the next byte after seeing the needle context.\n",
    "    Uses teacher forcing up to the needle, then checks prediction.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create test sequence\n",
    "    sequence, needle_pos = create_needle_haystack_data(\n",
    "        needle_text, context_length, needle_position\n",
    "    )\n",
    "    sequence = sequence.unsqueeze(0).to(device)  # (1, T)\n",
    "    \n",
    "    # Get model predictions for the entire sequence\n",
    "    with torch.no_grad():\n",
    "        logits = model(sequence)  # (1, T, vocab)\n",
    "    \n",
    "    # Check predictions at positions right after needle content\n",
    "    needle_bytes = [b for b in needle_text.encode('utf-8')]\n",
    "    needle_end = needle_pos + len(needle_bytes)\n",
    "    \n",
    "    # Calculate perplexity over the needle region\n",
    "    # (how well does the model predict the needle given prior context?)\n",
    "    needle_logits = logits[0, needle_pos:needle_end-1]  # predictions for needle bytes\n",
    "    needle_targets = sequence[0, needle_pos+1:needle_end]  # actual next bytes\n",
    "    \n",
    "    loss = F.cross_entropy(needle_logits, needle_targets)\n",
    "    ppl = torch.exp(loss).item()\n",
    "    \n",
    "    # Also check: does model assign high prob to needle content?\n",
    "    probs = F.softmax(needle_logits, dim=-1)\n",
    "    correct_probs = probs[range(len(needle_targets)), needle_targets].mean().item()\n",
    "    \n",
    "    return {\n",
    "        'position_ratio': needle_position,\n",
    "        'absolute_position': needle_pos,\n",
    "        'context_length': context_length,\n",
    "        'needle_ppl': ppl,\n",
    "        'avg_correct_prob': correct_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model (or train a quick one)\n",
    "# For proper testing, you should use a trained checkpoint!\n",
    "\n",
    "# Option 1: Load checkpoint\n",
    "checkpoint_path = 'checkpoints_hierarchical_small/best.pt'  # Update this\n",
    "\n",
    "import os\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f'Loading checkpoint from {checkpoint_path}')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Detect model type from checkpoint\n",
    "    if 'global_model' in str(checkpoint.get('model_state_dict', checkpoint).keys()):\n",
    "        config = HierarchicalBDHConfig(**checkpoint.get('config', {}))\n",
    "        model = HierarchicalBDH(config).to(device)\n",
    "        model_type = 'Hierarchical BDH'\n",
    "    else:\n",
    "        config = BDHConfig(**checkpoint.get('config', {}))\n",
    "        model = BDH(config).to(device)\n",
    "        model_type = 'Standard BDH'\n",
    "    \n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f'Loaded {model_type}')\n",
    "else:\n",
    "    print('No checkpoint found. Using untrained model (results will be random).')\n",
    "    print('For meaningful results, train a model first!')\n",
    "    config = HierarchicalBDHConfig(\n",
    "        local_n_layer=2, global_n_layer=2, n_embd=256,\n",
    "        patch_size=16, n_head=4\n",
    "    )\n",
    "    model = HierarchicalBDH(config).to(device)\n",
    "    model_type = 'Hierarchical BDH (untrained)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run needle-in-haystack tests\n",
    "needle = \"The magic number is 42.\"\n",
    "\n",
    "# Test different context lengths and needle positions\n",
    "context_lengths = [512, 1024, 2048, 4096]\n",
    "positions = [0.1, 0.25, 0.5, 0.75, 0.9]  # relative position in context\n",
    "\n",
    "results = []\n",
    "for ctx_len in tqdm(context_lengths, desc='Context lengths'):\n",
    "    for pos in positions:\n",
    "        try:\n",
    "            result = test_needle_retrieval(model, needle, ctx_len, pos, device)\n",
    "            results.append(result)\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f'OOM at ctx_len={ctx_len}')\n",
    "                torch.cuda.empty_cache()\n",
    "                break\n",
    "            raise\n",
    "\n",
    "print(f'Completed {len(results)} tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize needle-in-haystack results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Create heatmap of perplexity by position and context length\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot_ppl = df.pivot(index='context_length', columns='position_ratio', values='needle_ppl')\n",
    "pivot_prob = df.pivot(index='context_length', columns='position_ratio', values='avg_correct_prob')\n",
    "\n",
    "# PPL heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(pivot_ppl.values, cmap='RdYlGn_r', aspect='auto')\n",
    "ax.set_xticks(range(len(pivot_ppl.columns)))\n",
    "ax.set_xticklabels([f'{p:.0%}' for p in pivot_ppl.columns])\n",
    "ax.set_yticks(range(len(pivot_ppl.index)))\n",
    "ax.set_yticklabels(pivot_ppl.index)\n",
    "ax.set_xlabel('Needle Position (% of context)')\n",
    "ax.set_ylabel('Context Length')\n",
    "ax.set_title('Needle Perplexity (lower = better)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Annotate\n",
    "for i in range(len(pivot_ppl.index)):\n",
    "    for j in range(len(pivot_ppl.columns)):\n",
    "        val = pivot_ppl.values[i, j]\n",
    "        ax.text(j, i, f'{val:.1f}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "# Probability heatmap\n",
    "ax = axes[1]\n",
    "im = ax.imshow(pivot_prob.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(pivot_prob.columns)))\n",
    "ax.set_xticklabels([f'{p:.0%}' for p in pivot_prob.columns])\n",
    "ax.set_yticks(range(len(pivot_prob.index)))\n",
    "ax.set_yticklabels(pivot_prob.index)\n",
    "ax.set_xlabel('Needle Position (% of context)')\n",
    "ax.set_ylabel('Context Length')\n",
    "ax.set_title('Avg Probability of Correct Byte (higher = better)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Annotate\n",
    "for i in range(len(pivot_prob.index)):\n",
    "    for j in range(len(pivot_prob.columns)):\n",
    "        val = pivot_prob.values[i, j]\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('needle_haystack.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print('\\n' + '='*60)\n",
    "print('NEEDLE-IN-HAYSTACK ANALYSIS')\n",
    "print('='*60)\n",
    "print(f'Model: {model_type}')\n",
    "print(f'Needle: \"{needle}\"')\n",
    "print()\n",
    "\n",
    "# Check if PPL degrades with position (sign of limited context use)\n",
    "for ctx_len in df['context_length'].unique():\n",
    "    subset = df[df['context_length'] == ctx_len]\n",
    "    early_ppl = subset[subset['position_ratio'] <= 0.25]['needle_ppl'].mean()\n",
    "    late_ppl = subset[subset['position_ratio'] >= 0.75]['needle_ppl'].mean()\n",
    "    ratio = late_ppl / early_ppl if early_ppl > 0 else float('inf')\n",
    "    \n",
    "    status = '‚úì' if ratio < 1.5 else '~' if ratio < 2 else '‚úó'\n",
    "    print(f'Context {ctx_len}: Early PPL={early_ppl:.2f}, Late PPL={late_ppl:.2f}, Ratio={ratio:.2f} {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 3: Context Utilization (Perplexity vs Context Length)\n",
    "\n",
    "Does giving the model more context actually help? A model that truly uses long context should have lower perplexity with more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset with long documents\n",
    "print('Loading PG-19 (books) for long-context testing...')\n",
    "dataset = load_dataset('pg19', split='test', streaming=True)\n",
    "\n",
    "# Get a few long documents\n",
    "long_texts = []\n",
    "for item in dataset:\n",
    "    text = item['text']\n",
    "    if len(text) > 50000:  # At least 50KB\n",
    "        long_texts.append(text)\n",
    "    if len(long_texts) >= 5:\n",
    "        break\n",
    "\n",
    "print(f'Loaded {len(long_texts)} long documents')\n",
    "print(f'Lengths: {[len(t)//1000 for t in long_texts]} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_ppl_vs_context(model, text, context_lengths, device, stride=512):\n",
    "    \"\"\"\n",
    "    Measure perplexity using different amounts of context.\n",
    "    \n",
    "    For each context length, we slide a window and measure PPL\n",
    "    on predicting the last `stride` tokens given the prior context.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    text_bytes = torch.tensor([b for b in text.encode('utf-8')], dtype=torch.long)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for ctx_len in tqdm(context_lengths, desc='Testing context lengths'):\n",
    "        losses = []\n",
    "        \n",
    "        # Slide window through text\n",
    "        for start in range(0, len(text_bytes) - ctx_len, stride * 4):\n",
    "            chunk = text_bytes[start:start + ctx_len].unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(chunk)\n",
    "            \n",
    "            # Only measure loss on last `stride` tokens (given full context)\n",
    "            pred_logits = logits[0, -stride-1:-1]\n",
    "            targets = chunk[0, -stride:]\n",
    "            \n",
    "            loss = F.cross_entropy(pred_logits, targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            if len(losses) >= 10:  # Limit samples per context length\n",
    "                break\n",
    "        \n",
    "        avg_loss = np.mean(losses)\n",
    "        results.append({\n",
    "            'context_length': ctx_len,\n",
    "            'avg_loss': avg_loss,\n",
    "            'ppl': np.exp(avg_loss),\n",
    "            'bpb': avg_loss / np.log(2),\n",
    "            'n_samples': len(losses)\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test context utilization\n",
    "context_lengths = [256, 512, 1024, 2048, 4096]\n",
    "\n",
    "# Adjust for GPU memory\n",
    "if gpu_mem >= 40:\n",
    "    context_lengths.extend([8192, 16384])\n",
    "\n",
    "# Test on first long document\n",
    "print(f'Testing on document of length {len(long_texts[0])//1000}KB')\n",
    "context_results = measure_ppl_vs_context(model, long_texts[0], context_lengths, device)\n",
    "\n",
    "# Display results\n",
    "print('\\n' + '='*60)\n",
    "print('CONTEXT UTILIZATION RESULTS')\n",
    "print('='*60)\n",
    "for r in context_results:\n",
    "    print(f\"Context {r['context_length']:5d}: Loss={r['avg_loss']:.4f}, PPL={r['ppl']:.2f}, BPB={r['bpb']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot context utilization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ctx_lens = [r['context_length'] for r in context_results]\n",
    "ppls = [r['ppl'] for r in context_results]\n",
    "bpbs = [r['bpb'] for r in context_results]\n",
    "\n",
    "ax.plot(ctx_lens, ppls, 'b-o', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Context Length (bytes)', fontsize=12)\n",
    "ax.set_ylabel('Perplexity', fontsize=12)\n",
    "ax.set_title(f'Does More Context Help? ({model_type})', fontsize=14)\n",
    "ax.set_xscale('log', base=2)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add BPB on secondary axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(ctx_lens, bpbs, 'r--s', linewidth=2, markersize=8, alpha=0.7)\n",
    "ax2.set_ylabel('Bits per Byte', color='red', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Annotate improvement\n",
    "if len(context_results) >= 2:\n",
    "    improvement = (context_results[0]['ppl'] - context_results[-1]['ppl']) / context_results[0]['ppl'] * 100\n",
    "    ax.annotate(f'{improvement:.1f}% PPL improvement\\nfrom {ctx_lens[0]} to {ctx_lens[-1]} context',\n",
    "                xy=(ctx_lens[-1], ppls[-1]), xytext=(ctx_lens[-2], ppls[0]),\n",
    "                arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                fontsize=10, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('context_utilization.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print('\\n' + '='*60)\n",
    "print('CONTEXT UTILIZATION ANALYSIS')\n",
    "print('='*60)\n",
    "if len(context_results) >= 2:\n",
    "    ppl_improvement = (context_results[0]['ppl'] - context_results[-1]['ppl']) / context_results[0]['ppl'] * 100\n",
    "    bpb_improvement = (context_results[0]['bpb'] - context_results[-1]['bpb']) / context_results[0]['bpb'] * 100\n",
    "    \n",
    "    print(f'PPL improvement: {ppl_improvement:.1f}%')\n",
    "    print(f'BPB improvement: {bpb_improvement:.1f}%')\n",
    "    \n",
    "    if ppl_improvement > 10:\n",
    "        print('‚úì Model significantly benefits from longer context!')\n",
    "    elif ppl_improvement > 5:\n",
    "        print('~ Model shows some context utilization')\n",
    "    else:\n",
    "        print('‚úó Model does not effectively use longer context')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 4: Hierarchical vs Flat Comparison\n",
    "\n",
    "Does the hierarchical structure actually help for long-range dependencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test requires both a standard BDH and hierarchical BDH checkpoint\n",
    "# trained on similar data for fair comparison\n",
    "\n",
    "standard_checkpoint = 'checkpoints_small/best.pt'  # Update paths\n",
    "hierarchical_checkpoint = 'checkpoints_hierarchical_small/best.pt'\n",
    "\n",
    "models_to_compare = {}\n",
    "\n",
    "# Load standard BDH\n",
    "if os.path.exists(standard_checkpoint):\n",
    "    ckpt = torch.load(standard_checkpoint, map_location=device)\n",
    "    config = BDHConfig(**ckpt.get('config', {}))\n",
    "    standard_model = BDH(config).to(device)\n",
    "    standard_model.load_state_dict(ckpt.get('model_state_dict', ckpt))\n",
    "    models_to_compare['Standard BDH'] = standard_model\n",
    "    print('Loaded Standard BDH')\n",
    "else:\n",
    "    print(f'Standard checkpoint not found: {standard_checkpoint}')\n",
    "\n",
    "# Load hierarchical BDH\n",
    "if os.path.exists(hierarchical_checkpoint):\n",
    "    ckpt = torch.load(hierarchical_checkpoint, map_location=device)\n",
    "    config = HierarchicalBDHConfig(**ckpt.get('config', {}))\n",
    "    hier_model = HierarchicalBDH(config).to(device)\n",
    "    hier_model.load_state_dict(ckpt.get('model_state_dict', ckpt))\n",
    "    models_to_compare['Hierarchical BDH'] = hier_model\n",
    "    print('Loaded Hierarchical BDH')\n",
    "else:\n",
    "    print(f'Hierarchical checkpoint not found: {hierarchical_checkpoint}')\n",
    "\n",
    "print(f'\\nModels to compare: {list(models_to_compare.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on long-range dependency task\n",
    "if len(models_to_compare) >= 2:\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for name, m in models_to_compare.items():\n",
    "        print(f'\\nTesting {name}...')\n",
    "        results = measure_ppl_vs_context(m, long_texts[0], [512, 1024, 2048, 4096], device)\n",
    "        comparison_results[name] = results\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    for (name, results), color in zip(comparison_results.items(), colors):\n",
    "        ctx_lens = [r['context_length'] for r in results]\n",
    "        ppls = [r['ppl'] for r in results]\n",
    "        ax.plot(ctx_lens, ppls, '-o', color=color, label=name, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Context Length', fontsize=12)\n",
    "    ax.set_ylabel('Perplexity', fontsize=12)\n",
    "    ax.set_title('Standard vs Hierarchical BDH: Long-Range Performance', fontsize=14)\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hierarchical_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Need both Standard and Hierarchical checkpoints for comparison.')\n",
    "    print('Train both models first, then re-run this section.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Conclusions\n",
    "\n",
    "Run this cell after completing all tests to get a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('BDH \"UNLIMITED CONTEXT\" CLAIM EVALUATION')\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "# Summarize findings\n",
    "findings = []\n",
    "\n",
    "# Memory scaling\n",
    "if 'bdh_results' in dir() and len(bdh_results['context_len']) >= 2:\n",
    "    x = np.log2(bdh_results['context_len'])\n",
    "    y = np.log2(bdh_results['memory_mb'])\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "    if slope < 1.3:\n",
    "        findings.append(('Memory Scaling', 'PASS', f'O(n^{slope:.2f}) - near linear'))\n",
    "    else:\n",
    "        findings.append(('Memory Scaling', 'FAIL', f'O(n^{slope:.2f}) - superlinear'))\n",
    "\n",
    "# Context utilization\n",
    "if 'context_results' in dir() and len(context_results) >= 2:\n",
    "    improvement = (context_results[0]['ppl'] - context_results[-1]['ppl']) / context_results[0]['ppl'] * 100\n",
    "    if improvement > 10:\n",
    "        findings.append(('Context Utilization', 'PASS', f'{improvement:.1f}% PPL improvement'))\n",
    "    else:\n",
    "        findings.append(('Context Utilization', 'PARTIAL', f'Only {improvement:.1f}% improvement'))\n",
    "\n",
    "# Print findings\n",
    "for test, status, detail in findings:\n",
    "    emoji = '‚úì' if status == 'PASS' else '~' if status == 'PARTIAL' else '‚úó'\n",
    "    print(f'{emoji} {test}: {status}')\n",
    "    print(f'  {detail}')\n",
    "    print()\n",
    "\n",
    "print('='*70)\n",
    "print('VERDICT')\n",
    "print('='*70)\n",
    "passes = sum(1 for _, s, _ in findings if s == 'PASS')\n",
    "total = len(findings)\n",
    "print(f'{passes}/{total} tests passed')\n",
    "\n",
    "if passes == total:\n",
    "    print('\\nüéâ BDH shows strong evidence for efficient long-context handling!')\n",
    "elif passes >= total // 2:\n",
    "    print('\\nü§î Mixed results - some claims supported, others need more investigation')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è Claims not well supported by these tests')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}