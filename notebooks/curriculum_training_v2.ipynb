{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Curriculum Training\n",
    "\n",
    "This notebook trains BDH through a graduated curriculum:\n",
    "\n",
    "| Phase | Content | Target Examples | Training Ratio |\n",
    "|-------|---------|-----------------|----------------|\n",
    "| 1 | Primitives (simple functions) | 10,000 | 1.5x |\n",
    "| 2 | Composition (loops, recursion) | 20,000 | 1.2x |\n",
    "| 3 | Algorithms (sorting, searching) | 30,000 | 1.0x |\n",
    "| 4 | Systems (OOP, patterns) | 15,000 | 1.5x |\n",
    "| 5 | Language (WikiText-2) | ~50,000 | 0.8x |\n",
    "\n",
    "**Training Ratios** ensure balanced learning - smaller phases get more epochs per example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and install dependencies\n",
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || echo 'Repo exists'\n",
    "%cd bdh\n",
    "!pip install -q torch transformers datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Curriculum Data\n",
    "\n",
    "This uses the proper generators in `scripts/data_pipeline/` to create:\n",
    "- **Phase 1-4**: Synthetically generated Python code with variations\n",
    "- **Phase 5**: WikiText-2 natural language\n",
    "\n",
    "Use `--quick` for testing, remove it for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate curriculum data (use --quick for testing, remove for full)\n",
    "!python scripts/generate_full_curriculum.py --quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data was generated\n",
    "!python scripts/generate_full_curriculum.py --stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from model import BDHModel\n",
    "from tokenizer import BDHTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BDHTokenizer()\n",
    "print(f'Vocab size: {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import BDHConfig\n",
    "\n",
    "# Model config - adjust based on GPU memory\n",
    "config = BDHConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    n_layers=8,\n",
    "    d_ff=2048,\n",
    "    max_seq_len=512,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = BDHModel(config)\n",
    "model = model.cuda()\n",
    "\n",
    "# Count parameters\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {params:,} ({params/1e6:.1f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curriculum Training Loop\n",
    "\n",
    "Training proceeds through phases with:\n",
    "- **Balanced epochs**: Smaller phases get more epochs (via training ratio)\n",
    "- **LR warmup per phase**: Fresh warmup when switching phases\n",
    "- **Checkpoint saving**: Best model saved per phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class CurriculumDataset(Dataset):\n",
    "    \"\"\"Dataset for curriculum JSONL files.\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.examples = []\n",
    "        \n",
    "        with open(jsonl_path) as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                # Handle both 'code' and 'text' fields\n",
    "                text = data.get('code') or data.get('text', '')\n",
    "                if text:\n",
    "                    self.examples.append(text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "        tokens = self.tokenizer.encode(text)[:self.max_len]\n",
    "        \n",
    "        # Pad to max_len\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens = tokens + [self.tokenizer.pad_token_id] * (self.max_len - len(tokens))\n",
    "        \n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        return tokens[:-1], tokens[1:]  # input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 16,\n",
    "    'base_lr': 1e-4,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'base_epochs': 5,  # Base epochs, multiplied by training ratio\n",
    "}\n",
    "\n",
    "# Phase configuration with file paths and training ratios\n",
    "PHASES = [\n",
    "    {\n",
    "        'name': 'Phase 1: Primitives',\n",
    "        'file': 'data/curriculum/phase1_primitives/phase1_primitives.jsonl',\n",
    "        'ratio': 1.5,  # 1.5x epochs\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 2: Composition',\n",
    "        'file': 'data/curriculum/phase2_composition/phase2_composition.jsonl',\n",
    "        'ratio': 1.2,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 3: Algorithms',\n",
    "        'file': 'data/curriculum/phase3_algorithms/phase3_algorithms.jsonl',\n",
    "        'ratio': 1.0,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 4: Systems',\n",
    "        'file': 'data/curriculum/phase4_systems/phase4_systems.jsonl',\n",
    "        'ratio': 1.5,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 5: Language',\n",
    "        'file': 'data/curriculum/phase5_language/phase5_train.jsonl',\n",
    "        'ratio': 0.8,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_phase(model, phase_config, tokenizer, training_config, device='cuda'):\n",
    "    \"\"\"Train model on a single curriculum phase.\"\"\"\n",
    "    \n",
    "    phase_name = phase_config['name']\n",
    "    jsonl_path = Path(phase_config['file'])\n",
    "    ratio = phase_config['ratio']\n",
    "    \n",
    "    if not jsonl_path.exists():\n",
    "        print(f'WARNING: {jsonl_path} not found, skipping phase')\n",
    "        return {}\n",
    "    \n",
    "    # Calculate epochs for this phase\n",
    "    epochs = int(training_config['base_epochs'] * ratio)\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print(f'{phase_name}')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = CurriculumDataset(jsonl_path, tokenizer)\n",
    "    print(f'Loaded {len(dataset):,} examples')\n",
    "    print(f'Training for {epochs} epochs (ratio: {ratio}x)')\n",
    "    \n",
    "    # Split train/val (90/10)\n",
    "    val_size = max(1, len(dataset) // 10)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # DataLoaders (num_workers=0 for Colab compatibility)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=training_config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=training_config['batch_size'],\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Optimizer with fresh state for each phase\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=training_config['base_lr'],\n",
    "        weight_decay=training_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # LR scheduler with warmup\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = min(training_config['warmup_steps'], total_steps // 5)\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        return 1.0\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), \n",
    "                training_config['max_grad_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                logits = model(inputs)\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.view(-1, logits.size(-1)),\n",
    "                    targets.view(-1),\n",
    "                    ignore_index=tokenizer.pad_token_id\n",
    "                )\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['lr'].append(scheduler.get_last_lr()[0])\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            checkpoint_path = f'checkpoints/best_{phase_name.lower().replace(\" \", \"_\").replace(\":\", \"\")}.pt'\n",
    "            Path('checkpoints').mkdir(exist_ok=True)\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': model.config.__dict__,\n",
    "                'phase': phase_name,\n",
    "                'val_loss': best_val_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f'  \u2192 New best model saved!')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full curriculum training\n",
    "print('#' * 60)\n",
    "print('STARTING CURRICULUM TRAINING')\n",
    "print('#' * 60)\n",
    "\n",
    "all_history = {}\n",
    "\n",
    "for phase in PHASES:\n",
    "    history = train_phase(model, phase, tokenizer, TRAINING_CONFIG)\n",
    "    all_history[phase['name']] = history\n",
    "\n",
    "print('\\n' + '#' * 60)\n",
    "print('CURRICULUM TRAINING COMPLETE')\n",
    "print('#' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_path = 'checkpoints/bdh_curriculum_final.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': model.config.__dict__,\n",
    "    'training_history': all_history,\n",
    "}, final_path)\n",
    "print(f'Final model saved to {final_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate text from prompt.\"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor([tokens], device='cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            logits = model(tokens)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    'def add(',\n",
    "    'def fibonacci(',\n",
    "    'class Stack:',\n",
    "    'The quick brown',\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\n--- Prompt: {repr(prompt)} ---')\n",
    "    output = generate(model, tokenizer, prompt)\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
