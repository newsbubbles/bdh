{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token-Level BDH\n",
    "\n",
    "**Goal**: Test if BDH's sparse attention works at the token level (GPT-2 tokenizer) vs byte level.\n",
    "\n",
    "## Hypothesis\n",
    "BDH's sparse encoding might:\n",
    "1. Work better at token level (richer semantic units)\n",
    "2. Work worse (designed for byte-level patterns)\n",
    "3. Work similarly (architecture is general)\n",
    "\n",
    "## Architecture Changes\n",
    "- vocab_size: 256 â†’ 50257 (GPT-2)\n",
    "- Embedding dimension might need adjustment\n",
    "- Everything else stays the same\n",
    "\n",
    "## Comparison\n",
    "- BDH byte-level on WikiText-2\n",
    "- BDH token-level on WikiText-2\n",
    "- Measure: perplexity, BPB (converted), generation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import dataclasses\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q datasets transformers matplotlib tqdm tiktoken\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla BDH - works with any vocab size\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class BDHConfig:\n",
    "    n_layer: int = 6\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    n_head: int = 4\n",
    "    mlp_internal_dim_multiplier: int = 128\n",
    "    vocab_size: int = 256  # Will be overridden\n",
    "\n",
    "\n",
    "def get_freqs(n, theta, dtype):\n",
    "    def quantize(t, q=2):\n",
    "        return (t / q).floor() * q\n",
    "    return 1.0 / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n)) / (2 * math.pi)\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases, v):\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        return (v * torch.cos(phases)).to(v.dtype) + (v_rot * torch.sin(phases)).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        assert K is Q\n",
    "        _, _, T, _ = Q.size()\n",
    "        r_phases = torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype).view(1, 1, -1, 1) * self.freqs\n",
    "        QR = self.rope(r_phases, Q)\n",
    "        scores = (QR @ QR.mT).tril(diagonal=-1)\n",
    "        return scores @ V\n",
    "\n",
    "\n",
    "class BDH(nn.Module):\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        \n",
    "        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))\n",
    "        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.attn = Attention(config)\n",
    "        self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.lm_head = nn.Parameter(torch.zeros((D, config.vocab_size)).normal_(std=0.02))\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        C = self.config\n",
    "        B, T = idx.size()\n",
    "        D = C.n_embd\n",
    "        nh = C.n_head\n",
    "        N = D * C.mlp_internal_dim_multiplier // nh\n",
    "        \n",
    "        x = self.embed(idx).unsqueeze(1)\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        for level in range(C.n_layer):\n",
    "            x_latent = x @ self.encoder\n",
    "            x_sparse = F.relu(x_latent)\n",
    "            yKV = self.attn(Q=x_sparse, K=x_sparse, V=x)\n",
    "            yKV = self.ln(yKV)\n",
    "            y_latent = yKV @ self.encoder_v\n",
    "            y_sparse = F.relu(y_latent)\n",
    "            xy_sparse = x_sparse * y_sparse\n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "            yMLP = xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)\n",
    "        \n",
    "        logits = x.view(B, T, D) @ self.lm_head\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('BDH defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 tokenizer\n",
    "gpt2_enc = tiktoken.get_encoding('gpt2')\n",
    "GPT2_VOCAB_SIZE = gpt2_enc.n_vocab  # 50257\n",
    "\n",
    "print(f'GPT-2 vocab size: {GPT2_VOCAB_SIZE}')\n",
    "print(f'Byte vocab size: 256')\n",
    "\n",
    "# Test encoding\n",
    "test_text = \"Hello, world! This is a test.\"\n",
    "byte_tokens = list(test_text.encode('utf-8'))\n",
    "gpt2_tokens = gpt2_enc.encode(test_text)\n",
    "\n",
    "print(f'\\nTest: \"{test_text}\"')\n",
    "print(f'  Bytes: {len(byte_tokens)} tokens')\n",
    "print(f'  GPT-2: {len(gpt2_tokens)} tokens')\n",
    "print(f'  Compression ratio: {len(byte_tokens) / len(gpt2_tokens):.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading WikiText-2...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "\n",
    "# Byte-level encoding\n",
    "train_bytes = torch.tensor(list(train_text.encode('utf-8')), dtype=torch.long)\n",
    "val_bytes = torch.tensor(list(val_text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Token-level encoding (GPT-2)\n",
    "print('Tokenizing with GPT-2...')\n",
    "train_tokens = torch.tensor(gpt2_enc.encode(train_text), dtype=torch.long)\n",
    "val_tokens = torch.tensor(gpt2_enc.encode(val_text), dtype=torch.long)\n",
    "\n",
    "print(f'\\nDataset sizes:')\n",
    "print(f'  Train bytes:  {len(train_bytes):,}')\n",
    "print(f'  Train tokens: {len(train_tokens):,}')\n",
    "print(f'  Val bytes:    {len(val_bytes):,}')\n",
    "print(f'  Val tokens:   {len(val_tokens):,}')\n",
    "print(f'\\n  Compression: {len(train_bytes) / len(train_tokens):.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch functions\n",
    "def get_batch_bytes(split, batch_size, block_size):\n",
    "    data = train_bytes if split == 'train' else val_bytes\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "def get_batch_tokens(split, batch_size, block_size):\n",
    "    data = train_tokens if split == 'train' else val_tokens\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte-level config (original BDH)\n",
    "BYTE_CONFIG = BDHConfig(\n",
    "    n_layer=6,\n",
    "    n_embd=256,\n",
    "    n_head=4,\n",
    "    mlp_internal_dim_multiplier=128,\n",
    "    dropout=0.1,\n",
    "    vocab_size=256,  # Byte vocab\n",
    ")\n",
    "\n",
    "# Token-level config (larger vocab, may need more capacity)\n",
    "TOKEN_CONFIG = BDHConfig(\n",
    "    n_layer=6,\n",
    "    n_embd=256,\n",
    "    n_head=4,\n",
    "    mlp_internal_dim_multiplier=128,\n",
    "    dropout=0.1,\n",
    "    vocab_size=GPT2_VOCAB_SIZE,  # Token vocab\n",
    ")\n",
    "\n",
    "# Compare parameter counts\n",
    "byte_model = BDH(BYTE_CONFIG)\n",
    "token_model = BDH(TOKEN_CONFIG)\n",
    "\n",
    "print('Model Comparison:')\n",
    "print(f'  Byte-level:  {count_parameters(byte_model):,} params')\n",
    "print(f'  Token-level: {count_parameters(token_model):,} params')\n",
    "print(f'\\nDifference mainly from embedding & lm_head:')\n",
    "print(f'  Byte embed:  256 x 256 = {256*256:,}')\n",
    "print(f'  Token embed: 50257 x 256 = {50257*256:,}')\n",
    "\n",
    "del byte_model, token_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CONFIG = {\n",
    "    'max_steps': 3000,\n",
    "    'batch_size': 32,\n",
    "    'block_size': 256,  # Shorter for token-level (covers more text)\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'eval_interval': 100,\n",
    "    'eval_iters': 20,\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, get_batch_fn, eval_iters, batch_size, block_size):\n",
    "    model.eval()\n",
    "    losses = {'train': [], 'val': []}\n",
    "    for split in ['train', 'val']:\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch_fn(split, batch_size, block_size)\n",
    "            _, loss = model(x, y)\n",
    "            losses[split].append(loss.item())\n",
    "    return {k: np.mean(v) for k, v in losses.items()}\n",
    "\n",
    "\n",
    "def train_model(name, config, get_batch_fn, train_config):\n",
    "    print(f'\\n{\"="*60}')\n",
    "    print(f'Training: {name}')\n",
    "    print(f'Vocab size: {config.vocab_size}')\n",
    "    print('='*60)\n",
    "    \n",
    "    model = BDH(config).to(device)\n",
    "    n_params = count_parameters(model)\n",
    "    print(f'Parameters: {n_params:,}')\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config['learning_rate'],\n",
    "        weight_decay=train_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    history = {'step': [], 'train_loss': [], 'val_loss': [], 'ppl': []}\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(train_config['max_steps']), desc=name)\n",
    "    \n",
    "    for step in pbar:\n",
    "        model.train()\n",
    "        x, y = get_batch_fn('train', train_config['batch_size'], train_config['block_size'])\n",
    "        _, loss = model(x, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % train_config['eval_interval'] == 0:\n",
    "            losses = estimate_loss(model, get_batch_fn, train_config['eval_iters'],\n",
    "                                   train_config['batch_size'], train_config['block_size'])\n",
    "            ppl = math.exp(losses['val'])\n",
    "            \n",
    "            history['step'].append(step)\n",
    "            history['train_loss'].append(losses['train'])\n",
    "            history['val_loss'].append(losses['val'])\n",
    "            history['ppl'].append(ppl)\n",
    "            \n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'val': f\"{losses['val']:.3f}\",\n",
    "                'ppl': f'{ppl:.2f}',\n",
    "            })\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_losses = estimate_loss(model, get_batch_fn, 50,\n",
    "                                  train_config['batch_size'], train_config['block_size'])\n",
    "    \n",
    "    results = {\n",
    "        'name': name,\n",
    "        'vocab_size': config.vocab_size,\n",
    "        'params': n_params,\n",
    "        'final_val_loss': final_losses['val'],\n",
    "        'final_ppl': math.exp(final_losses['val']),\n",
    "        'best_val_loss': best_val_loss,\n",
    "    }\n",
    "    \n",
    "    return model, history, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train byte-level model\n",
    "byte_model, byte_history, byte_results = train_model(\n",
    "    'BDH-Byte', BYTE_CONFIG, get_batch_bytes, TRAIN_CONFIG\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train token-level model\n",
    "token_model, token_history, token_results = train_model(\n",
    "    'BDH-Token', TOKEN_CONFIG, get_batch_tokens, TRAIN_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics\n",
    "print('\\n' + '='*70)\n",
    "print('BYTE vs TOKEN LEVEL COMPARISON')\n",
    "print('='*70)\n",
    "\n",
    "print(f\"{'Metric':<25} {'Byte-Level':<20} {'Token-Level':<20}\")\n",
    "print('-'*70)\n",
    "print(f\"{'Vocab Size':<25} {byte_results['vocab_size']:<20} {token_results['vocab_size']:<20}\")\n",
    "print(f\"{'Parameters':<25} {byte_results['params']:,<20} {token_results['params']:,<20}\")\n",
    "print(f\"{'Final Val Loss':<25} {byte_results['final_val_loss']:<20.4f} {token_results['final_val_loss']:<20.4f}\")\n",
    "print(f\"{'Final Perplexity':<25} {byte_results['final_ppl']:<20.2f} {token_results['final_ppl']:<20.2f}\")\n",
    "print('='*70)\n",
    "\n",
    "# Note about perplexity comparison\n",
    "print('\\nNote: Perplexities are NOT directly comparable!')\n",
    "print('  - Byte PPL: uncertainty over 256 possible bytes')\n",
    "print('  - Token PPL: uncertainty over 50,257 possible tokens')\n",
    "print('  - Token PPL is naturally higher due to larger vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to comparable metric: Bits per Character (BPC)\n",
    "# For tokens: BPC = (loss * tokens) / characters\n",
    "\n",
    "# Estimate average characters per token\n",
    "chars_per_token = len(train_text) / len(train_tokens)\n",
    "print(f'Average characters per token: {chars_per_token:.2f}')\n",
    "\n",
    "# Byte BPC = loss / log(2) (since 1 byte = 1 char for ASCII)\n",
    "byte_bpc = byte_results['final_val_loss'] / math.log(2)\n",
    "\n",
    "# Token BPC = (loss / log(2)) / chars_per_token\n",
    "token_bpc = (token_results['final_val_loss'] / math.log(2)) / chars_per_token\n",
    "\n",
    "print(f'\\nBits per Character (BPC) - COMPARABLE METRIC:')\n",
    "print(f'  Byte-level:  {byte_bpc:.3f} BPC')\n",
    "print(f'  Token-level: {token_bpc:.3f} BPC')\n",
    "print(f'\\n  Difference: {byte_bpc - token_bpc:+.3f} BPC')\n",
    "\n",
    "if token_bpc < byte_bpc:\n",
    "    print('  >>> Token-level is MORE efficient!')\n",
    "else:\n",
    "    print('  >>> Byte-level is MORE efficient!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "ax1.plot(byte_history['step'], byte_history['val_loss'], 'b-', label='Byte-level', linewidth=2)\n",
    "ax1.plot(token_history['step'], token_history['val_loss'], 'r-', label='Token-level', linewidth=2)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Validation Loss')\n",
    "ax1.set_title('Training Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(byte_history['step'], byte_history['ppl'], 'b-', label='Byte-level', linewidth=2)\n",
    "ax2.plot(token_history['step'], token_history['ppl'], 'r-', label='Token-level', linewidth=2)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Perplexity')\n",
    "ax2.set_title('Perplexity (not directly comparable)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# BPC comparison bar chart\n",
    "ax3 = axes[2]\n",
    "ax3.bar(['Byte-level', 'Token-level'], [byte_bpc, token_bpc], color=['steelblue', 'coral'])\n",
    "ax3.set_ylabel('Bits per Character')\n",
    "ax3.set_title('BPC Comparison (lower is better)')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('BDH: Byte-Level vs Token-Level', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('byte_vs_token_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "prompts = [\n",
    "    \"The history of\",\n",
    "    \"In recent years,\",\n",
    "    \"Scientists have discovered\",\n",
    "]\n",
    "\n",
    "print('='*70)\n",
    "print('GENERATION COMPARISON')\n",
    "print('='*70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\nPrompt: \"{prompt}\"')\n",
    "    print('-'*50)\n",
    "    \n",
    "    # Byte-level generation\n",
    "    byte_input = torch.tensor([list(prompt.encode('utf-8'))], device=device)\n",
    "    byte_output = byte_model.generate(byte_input, max_new_tokens=100, temperature=0.8, top_k=40)\n",
    "    byte_text = bytes(byte_output[0].tolist()).decode('utf-8', errors='replace')\n",
    "    print(f'Byte-level: {byte_text}')\n",
    "    print()\n",
    "    \n",
    "    # Token-level generation\n",
    "    token_input = torch.tensor([gpt2_enc.encode(prompt)], device=device)\n",
    "    token_output = token_model.generate(token_input, max_new_tokens=30, temperature=0.8, top_k=40)\n",
    "    token_text = gpt2_enc.decode(token_output[0].tolist())\n",
    "    print(f'Token-level: {token_text}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('CONCLUSIONS')\n",
    "print('='*70)\n",
    "\n",
    "print(f'''\n",
    "1. Parameter Efficiency:\n",
    "   - Byte-level: {byte_results['params']:,} params\n",
    "   - Token-level: {token_results['params']:,} params\n",
    "   - Token model has {token_results['params']/byte_results['params']:.1f}x more params (mainly embedding)\n",
    "\n",
    "2. Compression Efficiency (BPC):\n",
    "   - Byte-level: {byte_bpc:.3f} bits/char\n",
    "   - Token-level: {token_bpc:.3f} bits/char\n",
    "   - {'Token' if token_bpc < byte_bpc else 'Byte'}-level is more efficient\n",
    "\n",
    "3. BDH Architecture:\n",
    "   - Works at both byte and token level\n",
    "   - Sparse encoding scales with vocab size\n",
    "   - {'Token-level benefits from richer semantic units' if token_bpc < byte_bpc else 'Byte-level may be more natural for BDH sparse patterns'}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_data = {\n",
    "    'train_config': TRAIN_CONFIG,\n",
    "    'byte_results': byte_results,\n",
    "    'token_results': token_results,\n",
    "    'byte_history': byte_history,\n",
    "    'token_history': token_history,\n",
    "    'comparison': {\n",
    "        'byte_bpc': byte_bpc,\n",
    "        'token_bpc': token_bpc,\n",
    "        'chars_per_token': chars_per_token,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('byte_vs_token_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print('Results saved to byte_vs_token_results.json')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('byte_vs_token_results.json')\n",
    "    files.download('byte_vs_token_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}