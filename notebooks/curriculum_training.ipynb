{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Curriculum Training Notebook\n",
    "\n",
    "This notebook runs the full curriculum learning pipeline for BDH:\n",
    "\n",
    "**5-Phase Curriculum:**\n",
    "1. **Primitives** - Simple, heavily commented single functions\n",
    "2. **Composition** - Loops, recursion, control flow\n",
    "3. **Algorithms** - Classic CS algorithms (sorting, searching, DP)\n",
    "4. **Systems** - OOP, design patterns, utilities\n",
    "5. **Language** - Natural language (WikiText-2)\n",
    "\n",
    "**Hypothesis:** Training on progressively complex code before natural language\n",
    "will build better generalization circuits than training on language directly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive for persistent storage\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone repo if not exists\n",
    "    !git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || echo 'Repo exists'\n",
    "    %cd bdh\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q torch datasets tqdm\n",
    "else:\n",
    "    print('Running locally')\n",
    "    # Ensure we're in project root\n",
    "    import os\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f'\u2705 GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('\u26a0\ufe0f No GPU detected, training will be slow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate/Download Curriculum Data\n",
    "\n",
    "This section generates all 5 phases of curriculum data.\n",
    "- Phases 1-4: Synthetically generated Python code\n",
    "- Phase 5: WikiText-2 natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Create data directories\n",
    "DATA_DIR = Path('data/curriculum')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for phase in ['phase1_primitives', 'phase2_composition', 'phase3_algorithms', \n",
    "              'phase4_systems', 'phase5_language']:\n",
    "    (DATA_DIR / phase).mkdir(exist_ok=True)\n",
    "\n",
    "print('Data directories created:')",
    "!ls -la data/curriculum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Primitives\n",
    "Simple single-purpose functions with heavy comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Primitives Generator\n",
    "# Simple, heavily commented functions\n",
    "\n",
    "PHASE1_TEMPLATES = {\n",
    "    'arithmetic': [\n",
    "        '''def add(a, b):\n",
    "    \"\"\"Add two numbers together.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "    \n",
    "    Returns:\n",
    "        The sum of a and b\n",
    "    \"\"\"\n",
    "    # Add the two numbers\n",
    "    result = a + b\n",
    "    return result''',\n",
    "        '''def subtract(a, b):\n",
    "    \"\"\"Subtract b from a.\n",
    "    \n",
    "    Args:\n",
    "        a: Number to subtract from\n",
    "        b: Number to subtract\n",
    "    \n",
    "    Returns:\n",
    "        The difference a - b\n",
    "    \"\"\"\n",
    "    # Calculate the difference\n",
    "    result = a - b\n",
    "    return result''',\n",
    "        '''def multiply(a, b):\n",
    "    \"\"\"Multiply two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First factor\n",
    "        b: Second factor\n",
    "    \n",
    "    Returns:\n",
    "        The product of a and b\n",
    "    \"\"\"\n",
    "    # Multiply the numbers\n",
    "    result = a * b\n",
    "    return result''',\n",
    "        '''def divide(a, b):\n",
    "    \"\"\"Divide a by b.\n",
    "    \n",
    "    Args:\n",
    "        a: Dividend\n",
    "        b: Divisor (must not be zero)\n",
    "    \n",
    "    Returns:\n",
    "        The quotient a / b\n",
    "    \"\"\"\n",
    "    # Perform division\n",
    "    result = a / b\n",
    "    return result''',\n",
    "        '''def square(x):\n",
    "    \"\"\"Calculate the square of a number.\n",
    "    \n",
    "    Args:\n",
    "        x: Number to square\n",
    "    \n",
    "    Returns:\n",
    "        x squared (x * x)\n",
    "    \"\"\"\n",
    "    # Square the number\n",
    "    return x * x''',\n",
    "    ],\n",
    "    'comparison': [\n",
    "        '''def is_positive(n):\n",
    "    \"\"\"Check if a number is positive.\n",
    "    \n",
    "    Args:\n",
    "        n: Number to check\n",
    "    \n",
    "    Returns:\n",
    "        True if n > 0, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if greater than zero\n",
    "    return n > 0''',\n",
    "        '''def is_even(n):\n",
    "    \"\"\"Check if a number is even.\n",
    "    \n",
    "    Args:\n",
    "        n: Integer to check\n",
    "    \n",
    "    Returns:\n",
    "        True if n is divisible by 2\n",
    "    \"\"\"\n",
    "    # Even numbers have no remainder when divided by 2\n",
    "    return n % 2 == 0''',\n",
    "        '''def max_of_two(a, b):\n",
    "    \"\"\"Return the larger of two numbers.\n",
    "    \n",
    "    Args:\n",
    "        a: First number\n",
    "        b: Second number\n",
    "    \n",
    "    Returns:\n",
    "        The larger value\n",
    "    \"\"\"\n",
    "    # Compare and return larger\n",
    "    if a > b:\n",
    "        return a\n",
    "    return b''',\n",
    "    ],\n",
    "    'string': [\n",
    "        '''def get_length(s):\n",
    "    \"\"\"Get the length of a string.\n",
    "    \n",
    "    Args:\n",
    "        s: Input string\n",
    "    \n",
    "    Returns:\n",
    "        Number of characters in s\n",
    "    \"\"\"\n",
    "    # Use len() to count characters\n",
    "    return len(s)''',\n",
    "        '''def to_upper(s):\n",
    "    \"\"\"Convert string to uppercase.\n",
    "    \n",
    "    Args:\n",
    "        s: Input string\n",
    "    \n",
    "    Returns:\n",
    "        String in all uppercase\n",
    "    \"\"\"\n",
    "    # Convert to uppercase\n",
    "    return s.upper()''',\n",
    "    ],\n",
    "}\n",
    "\n",
    "def generate_phase1_variations(templates, num_variations=200):\n",
    "    \"\"\"Generate variations of templates with different variable names.\"\"\"\n",
    "    examples = []\n",
    "    var_names = ['x', 'y', 'n', 'm', 'val', 'num', 'value', 'input_val', 'data']\n",
    "    \n",
    "    for category, funcs in templates.items():\n",
    "        for func in funcs:\n",
    "            # Add original\n",
    "            examples.append({\n",
    "                'code': func,\n",
    "                'category': category,\n",
    "                'difficulty': 0.1,\n",
    "                'phase': 1\n",
    "            })\n",
    "            \n",
    "            # Add variations\n",
    "            for _ in range(num_variations // len(funcs)):\n",
    "                varied = func\n",
    "                # Simple variable substitution\n",
    "                old_var = random.choice(['a', 'b', 'x', 'n'])\n",
    "                new_var = random.choice(var_names)\n",
    "                if old_var in varied and new_var not in varied:\n",
    "                    varied = varied.replace(old_var, new_var)\n",
    "                examples.append({\n",
    "                    'code': varied,\n",
    "                    'category': category,\n",
    "                    'difficulty': 0.1,\n",
    "                    'phase': 1\n",
    "                })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Generate Phase 1\n",
    "phase1_dir = DATA_DIR / 'phase1_primitives'\n",
    "phase1_file = phase1_dir / 'phase1_primitives.jsonl'\n",
    "\n",
    "if not phase1_file.exists():\n",
    "    print('Generating Phase 1: Primitives...')\n",
    "    examples = generate_phase1_variations(PHASE1_TEMPLATES, num_variations=500)\n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    with open(phase1_file, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    \n",
    "    # Stats\n",
    "    tokens = sum(len(ex['code']) // 4 for ex in examples)\n",
    "    stats = {'total': len(examples), 'tokens': tokens}\n",
    "    with open(phase1_dir / 'phase1_stats.json', 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    print(f'  Generated {len(examples)} examples, ~{tokens:,} tokens')\n",
    "else:\n",
    "    with open(phase1_dir / 'phase1_stats.json') as f:\n",
    "        stats = json.load(f)\n",
    "    print(f'Phase 1 exists: {stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Composition\n",
    "Functions with loops, recursion, and control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Composition Generator\n",
    "# Loops, recursion, control flow\n",
    "\n",
    "PHASE2_TEMPLATES = {\n",
    "    'loops': [\n",
    "        '''def sum_list(numbers):\n",
    "    \"\"\"Calculate the sum of all numbers in a list.\n",
    "    \n",
    "    Args:\n",
    "        numbers: List of numbers to sum\n",
    "    \n",
    "    Returns:\n",
    "        Total sum of all numbers\n",
    "    \"\"\"\n",
    "    # Initialize sum to zero\n",
    "    total = 0\n",
    "    \n",
    "    # Add each number to the total\n",
    "    for num in numbers:\n",
    "        total = total + num\n",
    "    \n",
    "    return total''',\n",
    "        '''def find_max(numbers):\n",
    "    \"\"\"Find the maximum value in a list.\n",
    "    \n",
    "    Args:\n",
    "        numbers: List of numbers\n",
    "    \n",
    "    Returns:\n",
    "        The largest number in the list\n",
    "    \"\"\"\n",
    "    # Start with first element\n",
    "    max_val = numbers[0]\n",
    "    \n",
    "    # Check each number\n",
    "    for num in numbers:\n",
    "        if num > max_val:\n",
    "            max_val = num\n",
    "    \n",
    "    return max_val''',\n",
    "        '''def count_occurrences(items, target):\n",
    "    \"\"\"Count how many times target appears in items.\n",
    "    \n",
    "    Args:\n",
    "        items: List to search\n",
    "        target: Value to count\n",
    "    \n",
    "    Returns:\n",
    "        Number of occurrences\n",
    "    \"\"\"\n",
    "    # Initialize counter\n",
    "    count = 0\n",
    "    \n",
    "    # Check each item\n",
    "    for item in items:\n",
    "        if item == target:\n",
    "            count = count + 1\n",
    "    \n",
    "    return count''',\n",
    "    ],\n",
    "    'recursion': [\n",
    "        '''def factorial(n):\n",
    "    \"\"\"Calculate n factorial (n!).\n",
    "    \n",
    "    Args:\n",
    "        n: Non-negative integer\n",
    "    \n",
    "    Returns:\n",
    "        n! = n * (n-1) * ... * 1\n",
    "    \"\"\"\n",
    "    # Base case: 0! = 1\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    \n",
    "    # Recursive case: n! = n * (n-1)!\n",
    "    return n * factorial(n - 1)''',\n",
    "        '''def fibonacci(n):\n",
    "    \"\"\"Calculate the nth Fibonacci number.\n",
    "    \n",
    "    Args:\n",
    "        n: Index in Fibonacci sequence\n",
    "    \n",
    "    Returns:\n",
    "        The nth Fibonacci number\n",
    "    \"\"\"\n",
    "    # Base cases\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    \n",
    "    # Recursive case: F(n) = F(n-1) + F(n-2)\n",
    "    return fibonacci(n - 1) + fibonacci(n - 2)''',\n",
    "    ],\n",
    "    'control_flow': [\n",
    "        '''def grade_score(score):\n",
    "    \"\"\"Convert numeric score to letter grade.\n",
    "    \n",
    "    Args:\n",
    "        score: Numeric score (0-100)\n",
    "    \n",
    "    Returns:\n",
    "        Letter grade (A, B, C, D, or F)\n",
    "    \"\"\"\n",
    "    # Check score ranges\n",
    "    if score >= 90:\n",
    "        return \"A\"\n",
    "    elif score >= 80:\n",
    "        return \"B\"\n",
    "    elif score >= 70:\n",
    "        return \"C\"\n",
    "    elif score >= 60:\n",
    "        return \"D\"\n",
    "    else:\n",
    "        return \"F\"''',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Generate Phase 2\n",
    "phase2_dir = DATA_DIR / 'phase2_composition'\n",
    "phase2_file = phase2_dir / 'phase2_composition.jsonl'\n",
    "\n",
    "if not phase2_file.exists():\n",
    "    print('Generating Phase 2: Composition...')\n",
    "    examples = []\n",
    "    \n",
    "    for category, funcs in PHASE2_TEMPLATES.items():\n",
    "        for func in funcs:\n",
    "            # Add multiple copies with slight variations\n",
    "            for i in range(100):\n",
    "                examples.append({\n",
    "                    'code': func,\n",
    "                    'category': category,\n",
    "                    'difficulty': 0.25,\n",
    "                    'phase': 2\n",
    "                })\n",
    "    \n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    with open(phase2_file, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    \n",
    "    tokens = sum(len(ex['code']) // 4 for ex in examples)\n",
    "    stats = {'total': len(examples), 'tokens': tokens}\n",
    "    with open(phase2_dir / 'phase2_stats.json', 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    print(f'  Generated {len(examples)} examples, ~{tokens:,} tokens')\n",
    "else:\n",
    "    with open(phase2_dir / 'phase2_stats.json') as f:\n",
    "        stats = json.load(f)\n",
    "    print(f'Phase 2 exists: {stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Algorithms\n",
    "Classic CS algorithms - sorting, searching, data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Algorithms\n",
    "\n",
    "PHASE3_TEMPLATES = {\n",
    "    'sorting': [\n",
    "        '''def bubble_sort(arr):\n",
    "    \"\"\"Sort array using bubble sort algorithm.\n",
    "    \n",
    "    Bubble sort repeatedly steps through the list, compares adjacent\n",
    "    elements and swaps them if they are in the wrong order.\n",
    "    \n",
    "    Time complexity: O(n^2)\n",
    "    Space complexity: O(1)\n",
    "    \n",
    "    Args:\n",
    "        arr: List of comparable elements\n",
    "    \n",
    "    Returns:\n",
    "        Sorted list in ascending order\n",
    "    \"\"\"\n",
    "    n = len(arr)\n",
    "    \n",
    "    # Traverse through all elements\n",
    "    for i in range(n):\n",
    "        # Last i elements are already in place\n",
    "        for j in range(0, n - i - 1):\n",
    "            # Swap if current element is greater than next\n",
    "            if arr[j] > arr[j + 1]:\n",
    "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
    "    \n",
    "    return arr''',\n",
    "        '''def insertion_sort(arr):\n",
    "    \"\"\"Sort array using insertion sort algorithm.\n",
    "    \n",
    "    Builds sorted array one element at a time by inserting\n",
    "    each element into its correct position.\n",
    "    \n",
    "    Time complexity: O(n^2)\n",
    "    Space complexity: O(1)\n",
    "    \n",
    "    Args:\n",
    "        arr: List of comparable elements\n",
    "    \n",
    "    Returns:\n",
    "        Sorted list in ascending order\n",
    "    \"\"\"\n",
    "    # Start from second element\n",
    "    for i in range(1, len(arr)):\n",
    "        key = arr[i]\n",
    "        j = i - 1\n",
    "        \n",
    "        # Move elements greater than key ahead\n",
    "        while j >= 0 and arr[j] > key:\n",
    "            arr[j + 1] = arr[j]\n",
    "            j = j - 1\n",
    "        \n",
    "        # Insert key at correct position\n",
    "        arr[j + 1] = key\n",
    "    \n",
    "    return arr''',\n",
    "    ],\n",
    "    'searching': [\n",
    "        '''def binary_search(arr, target):\n",
    "    \"\"\"Search for target in sorted array using binary search.\n",
    "    \n",
    "    Binary search divides the search interval in half repeatedly.\n",
    "    \n",
    "    Time complexity: O(log n)\n",
    "    Space complexity: O(1)\n",
    "    \n",
    "    Args:\n",
    "        arr: Sorted list of elements\n",
    "        target: Element to find\n",
    "    \n",
    "    Returns:\n",
    "        Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    left = 0\n",
    "    right = len(arr) - 1\n",
    "    \n",
    "    while left <= right:\n",
    "        # Find middle index\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return -1''',\n",
    "    ],\n",
    "    'data_structures': [\n",
    "        '''class Stack:\n",
    "    \"\"\"A simple stack implementation using a list.\n",
    "    \n",
    "    Stack follows Last-In-First-Out (LIFO) principle.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty stack.\"\"\"\n",
    "        self.items = []\n",
    "    \n",
    "    def push(self, item):\n",
    "        \"\"\"Add item to top of stack.\"\"\"\n",
    "        self.items.append(item)\n",
    "    \n",
    "    def pop(self):\n",
    "        \"\"\"Remove and return top item.\"\"\"\n",
    "        if not self.is_empty():\n",
    "            return self.items.pop()\n",
    "        return None\n",
    "    \n",
    "    def peek(self):\n",
    "        \"\"\"Return top item without removing.\"\"\"\n",
    "        if not self.is_empty():\n",
    "            return self.items[-1]\n",
    "        return None\n",
    "    \n",
    "    def is_empty(self):\n",
    "        \"\"\"Check if stack is empty.\"\"\"\n",
    "        return len(self.items) == 0''',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Generate Phase 3\n",
    "phase3_dir = DATA_DIR / 'phase3_algorithms'\n",
    "phase3_file = phase3_dir / 'phase3_algorithms.jsonl'\n",
    "\n",
    "if not phase3_file.exists():\n",
    "    print('Generating Phase 3: Algorithms...')\n",
    "    examples = []\n",
    "    \n",
    "    for category, funcs in PHASE3_TEMPLATES.items():\n",
    "        for func in funcs:\n",
    "            for i in range(50):\n",
    "                examples.append({\n",
    "                    'code': func,\n",
    "                    'category': category,\n",
    "                    'difficulty': 0.5,\n",
    "                    'phase': 3\n",
    "                })\n",
    "    \n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    with open(phase3_file, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    \n",
    "    tokens = sum(len(ex['code']) // 4 for ex in examples)\n",
    "    stats = {'total': len(examples), 'tokens': tokens}\n",
    "    with open(phase3_dir / 'phase3_stats.json', 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    print(f'  Generated {len(examples)} examples, ~{tokens:,} tokens')\n",
    "else:\n",
    "    with open(phase3_dir / 'phase3_stats.json') as f:\n",
    "        stats = json.load(f)\n",
    "    print(f'Phase 3 exists: {stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Systems\n",
    "OOP, design patterns, multi-function modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Systems\n",
    "\n",
    "PHASE4_TEMPLATES = [\n",
    "    '''class BankAccount:\n",
    "    \"\"\"A simple bank account with deposit and withdraw operations.\n",
    "    \n",
    "    Attributes:\n",
    "        owner: Name of account holder\n",
    "        balance: Current account balance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, owner, initial_balance=0):\n",
    "        \"\"\"Create a new bank account.\n",
    "        \n",
    "        Args:\n",
    "            owner: Name of account holder\n",
    "            initial_balance: Starting balance (default 0)\n",
    "        \"\"\"\n",
    "        self.owner = owner\n",
    "        self.balance = initial_balance\n",
    "    \n",
    "    def deposit(self, amount):\n",
    "        \"\"\"Add money to the account.\n",
    "        \n",
    "        Args:\n",
    "            amount: Amount to deposit (must be positive)\n",
    "        \n",
    "        Returns:\n",
    "            New balance after deposit\n",
    "        \"\"\"\n",
    "        if amount > 0:\n",
    "            self.balance += amount\n",
    "        return self.balance\n",
    "    \n",
    "    def withdraw(self, amount):\n",
    "        \"\"\"Remove money from the account.\n",
    "        \n",
    "        Args:\n",
    "            amount: Amount to withdraw\n",
    "        \n",
    "        Returns:\n",
    "            New balance, or None if insufficient funds\n",
    "        \"\"\"\n",
    "        if amount <= self.balance:\n",
    "            self.balance -= amount\n",
    "            return self.balance\n",
    "        return None\n",
    "    \n",
    "    def get_balance(self):\n",
    "        \"\"\"Return current balance.\"\"\"\n",
    "        return self.balance''',\n",
    "    '''class Logger:\n",
    "    \"\"\"A simple logging utility.\n",
    "    \n",
    "    Supports different log levels: DEBUG, INFO, WARNING, ERROR.\n",
    "    \"\"\"\n",
    "    \n",
    "    DEBUG = 0\n",
    "    INFO = 1\n",
    "    WARNING = 2\n",
    "    ERROR = 3\n",
    "    \n",
    "    def __init__(self, name, level=1):\n",
    "        \"\"\"Create a new logger.\n",
    "        \n",
    "        Args:\n",
    "            name: Logger name (usually module name)\n",
    "            level: Minimum log level to output\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.level = level\n",
    "        self.messages = []\n",
    "    \n",
    "    def log(self, level, message):\n",
    "        \"\"\"Log a message at the specified level.\"\"\"\n",
    "        if level >= self.level:\n",
    "            entry = f\"[{self.name}] {message}\"\n",
    "            self.messages.append(entry)\n",
    "            print(entry)\n",
    "    \n",
    "    def debug(self, message):\n",
    "        \"\"\"Log a debug message.\"\"\"\n",
    "        self.log(self.DEBUG, message)\n",
    "    \n",
    "    def info(self, message):\n",
    "        \"\"\"Log an info message.\"\"\"\n",
    "        self.log(self.INFO, message)\n",
    "    \n",
    "    def warning(self, message):\n",
    "        \"\"\"Log a warning message.\"\"\"\n",
    "        self.log(self.WARNING, message)\n",
    "    \n",
    "    def error(self, message):\n",
    "        \"\"\"Log an error message.\"\"\"\n",
    "        self.log(self.ERROR, message)''',\n",
    "]\n",
    "\n",
    "# Generate Phase 4\n",
    "phase4_dir = DATA_DIR / 'phase4_systems'\n",
    "phase4_file = phase4_dir / 'phase4_systems.jsonl'\n",
    "\n",
    "if not phase4_file.exists():\n",
    "    print('Generating Phase 4: Systems...')\n",
    "    examples = []\n",
    "    \n",
    "    for func in PHASE4_TEMPLATES:\n",
    "        for i in range(50):\n",
    "            examples.append({\n",
    "                'code': func,\n",
    "                'category': 'oop',\n",
    "                'difficulty': 0.7,\n",
    "                'phase': 4\n",
    "            })\n",
    "    \n",
    "    random.shuffle(examples)\n",
    "    \n",
    "    with open(phase4_file, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + '\\n')\n",
    "    \n",
    "    tokens = sum(len(ex['code']) // 4 for ex in examples)\n",
    "    stats = {'total': len(examples), 'tokens': tokens}\n",
    "    with open(phase4_dir / 'phase4_stats.json', 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    print(f'  Generated {len(examples)} examples, ~{tokens:,} tokens')\n",
    "else:\n",
    "    with open(phase4_dir / 'phase4_stats.json') as f:\n",
    "        stats = json.load(f)\n",
    "    print(f'Phase 4 exists: {stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5: Natural Language (WikiText-2)\n",
    "Download and prepare WikiText-2 for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: WikiText-2\n",
    "from datasets import load_dataset\n",
    "\n",
    "phase5_dir = DATA_DIR / 'phase5_language'\n",
    "phase5_train = phase5_dir / 'phase5_train.jsonl'\n",
    "\n",
    "if not phase5_train.exists():\n",
    "    print('Downloading WikiText-2...')\n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_examples = {}\n",
    "    \n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        out_name = 'valid' if split == 'validation' else split\n",
    "        out_file = phase5_dir / f'phase5_{out_name}.jsonl'\n",
    "        \n",
    "        count = 0\n",
    "        with open(out_file, 'w') as f:\n",
    "            for item in dataset[split]:\n",
    "                text = item['text'].strip()\n",
    "                if len(text) > 50:  # Filter very short\n",
    "                    f.write(json.dumps({\n",
    "                        'text': text,\n",
    "                        'phase': 5,\n",
    "                        'difficulty': 1.0\n",
    "                    }) + '\\n')\n",
    "                    count += 1\n",
    "                    total_tokens += len(text) // 4\n",
    "        \n",
    "        total_examples[out_name] = count\n",
    "        print(f'  {split}: {count} examples')\n",
    "    \n",
    "    stats = {\n",
    "        'total': sum(total_examples.values()),\n",
    "        'by_split': total_examples,\n",
    "        'tokens': total_tokens\n",
    "    }\n",
    "    with open(phase5_dir / 'phase5_stats.json', 'w') as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    print(f'  Total: {stats[\"total\"]} examples, ~{total_tokens:,} tokens')\n",
    "else:\n",
    "    with open(phase5_dir / 'phase5_stats.json') as f:\n",
    "        stats = json.load(f)\n",
    "    print(f'Phase 5 exists: {stats}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify curriculum data\n",
    "print('\\n' + '='*60)\n",
    "print('CURRICULUM DATA SUMMARY')\n",
    "print('='*60 + '\\n')\n",
    "\n",
    "total_examples = 0\n",
    "total_tokens = 0\n",
    "\n",
    "phases = [\n",
    "    ('Phase 1: Primitives', 'phase1_primitives', 'phase1_primitives.jsonl'),\n",
    "    ('Phase 2: Composition', 'phase2_composition', 'phase2_composition.jsonl'),\n",
    "    ('Phase 3: Algorithms', 'phase3_algorithms', 'phase3_algorithms.jsonl'),\n",
    "    ('Phase 4: Systems', 'phase4_systems', 'phase4_systems.jsonl'),\n",
    "    ('Phase 5: Language', 'phase5_language', 'phase5_train.jsonl'),\n",
    "]\n",
    "\n",
    "for name, dir_name, file_name in phases:\n",
    "    phase_dir = DATA_DIR / dir_name\n",
    "    jsonl_file = phase_dir / file_name\n",
    "    stats_file = phase_dir / f'{dir_name.split(\"_\")[0]}_stats.json'\n",
    "    \n",
    "    if jsonl_file.exists():\n",
    "        # Count lines\n",
    "        with open(jsonl_file) as f:\n",
    "            count = sum(1 for _ in f)\n",
    "        \n",
    "        # Load stats\n",
    "        if stats_file.exists():\n",
    "            with open(stats_file) as f:\n",
    "                stats = json.load(f)\n",
    "            tokens = stats.get('tokens', 0)\n",
    "        else:\n",
    "            tokens = 0\n",
    "        \n",
    "        print(f'\u2705 {name}')\n",
    "        print(f'   Examples: {count:,}')\n",
    "        print(f'   Tokens: {tokens:,}')\n",
    "        total_examples += count\n",
    "        total_tokens += tokens\n",
    "    else:\n",
    "        print(f'\u274c {name} - NOT FOUND')\n",
    "    print()\n",
    "\n",
    "print('='*60)\n",
    "print(f'TOTAL: {total_examples:,} examples, ~{total_tokens:,} tokens')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BDH model\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from bdh import BDH, BDHConfig\n",
    "\n",
    "# Model configuration\n",
    "config = BDHConfig(\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    n_embd=256,\n",
    "    dropout=0.1,\n",
    "    vocab_size=256,  # Byte-level\n",
    "    mlp_internal_dim_multiplier=32,\n",
    ")\n",
    "\n",
    "model = BDH(config)\n",
    "model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model: {config.n_layer}L, {config.n_head}H, {config.n_embd}D')\n",
    "print(f'Parameters: {n_params:,} ({n_params/1e6:.2f}M)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CurriculumDataset(Dataset):\n",
    "    \"\"\"Dataset for curriculum training.\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_paths, block_size=256, max_examples=None):\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "        \n",
    "        for path in jsonl_paths:\n",
    "            if not Path(path).exists():\n",
    "                continue\n",
    "            with open(path) as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    text = data.get('code') or data.get('text', '')\n",
    "                    if text:\n",
    "                        self.examples.append(text)\n",
    "                        if max_examples and len(self.examples) >= max_examples:\n",
    "                            break\n",
    "            if max_examples and len(self.examples) >= max_examples:\n",
    "                break\n",
    "        \n",
    "        print(f'Loaded {len(self.examples)} examples')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        \n",
    "        # Pad or crop\n",
    "        if len(tokens) < self.block_size + 1:\n",
    "            tokens = tokens + [0] * (self.block_size + 1 - len(tokens))\n",
    "        else:\n",
    "            start = random.randint(0, len(tokens) - self.block_size - 1)\n",
    "            tokens = tokens[start:start + self.block_size + 1]\n",
    "        \n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    batch_size: int = 16\n",
    "    block_size: int = 256\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    learning_rate: float = 3e-4\n",
    "    min_lr: float = 1e-5\n",
    "    warmup_iters: int = 200\n",
    "    max_iters: int = 5000\n",
    "    eval_interval: int = 250\n",
    "    eval_iters: int = 50\n",
    "    log_interval: int = 50\n",
    "    grad_clip: float = 1.0\n",
    "    weight_decay: float = 0.1\n",
    "\n",
    "train_config = TrainConfig()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.learning_rate,\n",
    "    weight_decay=train_config.weight_decay,\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "def get_lr(it, phase_lr):\n",
    "    \"\"\"Cosine learning rate schedule with warmup.\"\"\"\n",
    "    if it < train_config.warmup_iters:\n",
    "        return phase_lr * (it + 1) / train_config.warmup_iters\n",
    "    if it > train_config.max_iters:\n",
    "        return train_config.min_lr\n",
    "    decay_ratio = (it - train_config.warmup_iters) / (train_config.max_iters - train_config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return train_config.min_lr + coeff * (phase_lr - train_config.min_lr)\n",
    "\n",
    "print('Training config ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Curriculum Training Loop\n",
    "\n",
    "Train through all 5 phases sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase configurations\n",
    "PHASES = [\n",
    "    {\n",
    "        'name': 'Phase 1: Primitives',\n",
    "        'files': [DATA_DIR / 'phase1_primitives/phase1_primitives.jsonl'],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 5,\n",
    "        'target_loss': 1.5,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 2: Composition',\n",
    "        'files': [DATA_DIR / 'phase2_composition/phase2_composition.jsonl'],\n",
    "        'lr': 2e-4,\n",
    "        'epochs': 5,\n",
    "        'target_loss': 1.4,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 3: Algorithms',\n",
    "        'files': [DATA_DIR / 'phase3_algorithms/phase3_algorithms.jsonl'],\n",
    "        'lr': 1.5e-4,\n",
    "        'epochs': 5,\n",
    "        'target_loss': 1.3,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 4: Systems',\n",
    "        'files': [DATA_DIR / 'phase4_systems/phase4_systems.jsonl'],\n",
    "        'lr': 1e-4,\n",
    "        'epochs': 5,\n",
    "        'target_loss': 1.2,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Phase 5: Language',\n",
    "        'files': [DATA_DIR / 'phase5_language/phase5_train.jsonl'],\n",
    "        'val_files': [DATA_DIR / 'phase5_language/phase5_valid.jsonl'],\n",
    "        'lr': 1e-4,\n",
    "        'epochs': 10,\n",
    "        'target_loss': 1.1,\n",
    "    },\n",
    "]\n",
    "\n",
    "print('Phase configurations loaded')\n",
    "for i, phase in enumerate(PHASES, 1):\n",
    "    print(f\"  {i}. {phase['name']} (lr={phase['lr']}, epochs={phase['epochs']})\")" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = Path('checkpoints_curriculum')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'phase': [],\n",
    "    'iteration': [],\n",
    "}\n",
    "\n",
    "global_iter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=50):\n",
    "    \"\"\"Evaluate model on dataset.\"\"\"\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    return sum(losses) / len(losses) if losses else 0\n",
    "\n",
    "def train_phase(phase_config, phase_idx):\n",
    "    \"\"\"Train on a single curriculum phase.\"\"\"\n",
    "    global global_iter, best_val_loss\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PHASE {phase_idx + 1}: {phase_config['name']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Load dataset\n",
    "    train_dataset = CurriculumDataset(\n",
    "        phase_config['files'],\n",
    "        block_size=train_config.block_size\n",
    "    )\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        print('No data, skipping...')\n",
    "        return\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Validation loader (if available)\n",
    "    val_files = phase_config.get('val_files', phase_config['files'])\n",
    "    val_dataset = CurriculumDataset(val_files, block_size=train_config.block_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_config.batch_size, shuffle=False)\n",
    "    \n",
    "    phase_lr = phase_config['lr']\n",
    "    \n",
    "    for epoch in range(phase_config['epochs']):\n",
    "        epoch_losses = []\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{phase_config['epochs']}\")\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(pbar):\n",
    "            # Update learning rate\n",
    "            lr = get_lr(global_iter, phase_lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            # Forward\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / train_config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_losses.append(loss.item() * train_config.gradient_accumulation_steps)\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if (batch_idx + 1) % train_config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.grad_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_iter += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                avg_loss = sum(epoch_losses[-20:]) / min(len(epoch_losses), 20)\n",
    "                pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.2e}'})\n",
    "        \n",
    "        # End of epoch evaluation\n",
    "        train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        val_loss = evaluate(model, val_loader)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['phase'].append(phase_idx)\n",
    "        history['iteration'].append(global_iter)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'phase': phase_idx,\n",
    "                'epoch': epoch,\n",
    "                'val_loss': val_loss,\n",
    "                'global_iter': global_iter,\n",
    "            }, CHECKPOINT_DIR / 'best.pt')\n",
    "            print(f'  \u2192 New best model saved!')\n",
    "        \n",
    "        # Check if target reached\n",
    "        if val_loss < phase_config['target_loss']:\n",
    "            print(f\"Target loss {phase_config['target_loss']} reached, advancing...\")\n",
    "            break\n",
    "    \n",
    "    # Save phase checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'phase': phase_idx,\n",
    "        'val_loss': val_loss,\n",
    "    }, CHECKPOINT_DIR / f'phase{phase_idx+1}_complete.pt')\n",
    "\n",
    "print('Training function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run curriculum training!\n",
    "print('\\n' + '#'*60)\n",
    "print('STARTING CURRICULUM TRAINING')\n",
    "print('#'*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for phase_idx, phase_config in enumerate(PHASES):\n",
    "    train_phase(phase_config, phase_idx)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f'TRAINING COMPLETE')\n",
    "print(f'Total time: {total_time/60:.1f} minutes')\n",
    "print(f'Best validation loss: {best_val_loss:.4f}')\n",
    "print(f\"{'#'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train', alpha=0.8)\n",
    "ax.plot(history['val_loss'], label='Validation', alpha=0.8)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss by Epoch')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add phase boundaries\n",
    "phase_changes = []\n",
    "for i in range(1, len(history['phase'])):\n",
    "    if history['phase'][i] != history['phase'][i-1]:\n",
    "        phase_changes.append(i)\n",
    "        ax.axvline(x=i, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Loss by phase\n",
    "ax = axes[1]\n",
    "phase_names = ['P1: Primitives', 'P2: Composition', 'P3: Algorithms', 'P4: Systems', 'P5: Language']\n",
    "phase_final_losses = {}\n",
    "for i, p in enumerate(history['phase']):\n",
    "    if p not in phase_final_losses:\n",
    "        phase_final_losses[p] = []\n",
    "    phase_final_losses[p].append(history['val_loss'][i])\n",
    "\n",
    "final_losses = [phase_final_losses.get(i, [0])[-1] for i in range(5)]\n",
    "ax.bar(range(5), final_losses, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6'])\n",
    "ax.set_xticks(range(5))\n",
    "ax.set_xticklabels(phase_names, rotation=15)\n",
    "ax.set_ylabel('Final Validation Loss')\n",
    "ax.set_title('Final Loss by Phase')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHECKPOINT_DIR / 'training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "def generate_text(prompt, max_tokens=200, temperature=0.8):\n",
    "    \"\"\"Generate text from prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode\n",
    "    tokens = list(prompt.encode('utf-8'))\n",
    "    idx = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_tokens, temperature=temperature, top_k=50)\n",
    "    \n",
    "    # Decode\n",
    "    return bytes(output[0].tolist()).decode('utf-8', errors='replace')\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    'The dragon emerged from the cave and',\n",
    "    'def fibonacci(n):',\n",
    "    'In 1945, the war ended when',\n",
    "]\n",
    "\n",
    "print('='*60)\n",
    "print('GENERATION SAMPLES')\n",
    "print('='*60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print('-'*40)\n",
    "    output = generate_text(prompt, max_tokens=150)\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open(CHECKPOINT_DIR / 'history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Save config\n",
    "config_dict = {\n",
    "    'n_layer': config.n_layer,\n",
    "    'n_head': config.n_head,\n",
    "    'n_embd': config.n_embd,\n",
    "    'vocab_size': config.vocab_size,\n",
    "    'dropout': config.dropout,\n",
    "    'mlp_internal_dim_multiplier': config.mlp_internal_dim_multiplier,\n",
    "}\n",
    "with open(CHECKPOINT_DIR / 'config.json', 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "print(f'Results saved to {CHECKPOINT_DIR}')\n",
    "!ls -la {CHECKPOINT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    !cp -r {CHECKPOINT_DIR} /content/drive/MyDrive/bdh_curriculum/\n",
    "    print('Checkpoints copied to Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook trained BDH through 5 curriculum phases:\n",
    "\n",
    "1. **Primitives** - Simple functions (add, subtract, etc.)\n",
    "2. **Composition** - Loops, recursion, control flow\n",
    "3. **Algorithms** - Sorting, searching, data structures\n",
    "4. **Systems** - OOP classes, design patterns\n",
    "5. **Language** - WikiText-2 natural language\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare with baseline (WikiText-2 only) training\n",
    "- Evaluate on downstream tasks\n",
    "- Try different phase orderings\n",
    "- Experiment with phase mixing strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}