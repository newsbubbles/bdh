{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Training on WikiText-2\n",
    "\n",
    "Train BDH on WikiText-2 (~2M tokens) to reduce overfitting observed on Shakespeare (1M tokens).\n",
    "\n",
    "**Changes from Shakespeare run:**\n",
    "- 2x more training data\n",
    "- Increased dropout (0.1 → 0.2)\n",
    "- Added weight decay\n",
    "- More frequent validation\n",
    "- Early stopping patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (or upload files)\n",
    "!git clone https://github.com/newsbubbles/bdh.git\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import BDH\n",
    "from bdh import BDH, BDHConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WikiText-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading WikiText-2...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Concatenate all text\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "test_text = '\\n'.join(dataset['test']['text'])\n",
    "\n",
    "train_data = text_to_bytes(train_text)\n",
    "val_data = text_to_bytes(val_text)\n",
    "test_data = text_to_bytes(test_text)\n",
    "\n",
    "print(f'Train: {len(train_data):,} bytes')\n",
    "print(f'Val: {len(val_data):,} bytes')\n",
    "print(f'Test: {len(test_data):,} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config - Improved Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "config = {\n",
    "    # Model\n",
    "    'n_layer': 6,\n",
    "    'n_head': 8,\n",
    "    'n_embd': 256,\n",
    "    'block_size': 512,\n",
    "    'vocab_size': 256,\n",
    "    'dropout': 0.2,  # Increased from 0.1\n",
    "    'bias': False,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,  # Added weight decay\n",
    "    'max_steps': 5000,\n",
    "    'warmup_steps': 200,\n",
    "    \n",
    "    # Validation\n",
    "    'val_interval': 100,  # More frequent\n",
    "    'val_batches': 50,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 10,  # Stop if no improvement for 10 evals\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 50,\n",
    "}\n",
    "\n",
    "print('Config:')\n",
    "for k, v in config.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = BDHConfig(\n",
    "    n_layer=config['n_layer'],\n",
    "    n_head=config['n_head'],\n",
    "    n_embd=config['n_embd'],\n",
    "    block_size=config['block_size'],\n",
    "    vocab_size=config['vocab_size'],\n",
    "    dropout=config['dropout'],\n",
    "    bias=config['bias'],\n",
    ")\n",
    "\n",
    "model = BDH(model_config).to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {n_params:,} ({n_params/1e6:.1f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_dataset = ByteDataset(train_data, config['block_size'])\n",
    "val_dataset = ByteDataset(val_data, config['block_size'])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f'Train batches: {len(train_loader):,}')\n",
    "print(f'Val batches: {len(val_loader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup + cosine decay\n",
    "def get_lr(step):\n",
    "    # Warmup\n",
    "    if step < config['warmup_steps']:\n",
    "        return config['learning_rate'] * step / config['warmup_steps']\n",
    "    # Cosine decay\n",
    "    progress = (step - config['warmup_steps']) / (config['max_steps'] - config['warmup_steps'])\n",
    "    return config['learning_rate'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "print('Optimizer: AdamW with weight_decay={}'.format(config['weight_decay']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    return avg_loss, math.exp(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training state\n",
    "history = {\n",
    "    'step': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_ppl': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "step = 0\n",
    "\n",
    "# Checkpoints dir\n",
    "ckpt_dir = Path('checkpoints_wikitext2')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print('Starting training...')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "running_loss = 0\n",
    "\n",
    "pbar = tqdm(range(config['max_steps']), desc='Training')\n",
    "\n",
    "for step in pbar:\n",
    "    # Get batch (cycle through data)\n",
    "    try:\n",
    "        x, y = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        x, y = next(train_iter)\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Forward + backward\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # Logging\n",
    "    if (step + 1) % config['log_interval'] == 0:\n",
    "        avg_loss = running_loss / config['log_interval']\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.3f}', 'lr': f'{lr:.2e}'})\n",
    "        running_loss = 0\n",
    "    \n",
    "    # Validation\n",
    "    if (step + 1) % config['val_interval'] == 0:\n",
    "        val_loss, val_ppl = evaluate(model, val_loader, config['val_batches'])\n",
    "        train_loss, _ = evaluate(model, train_loader, config['val_batches'])\n",
    "        \n",
    "        history['step'].append(step + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        history['lr'].append(lr)\n",
    "        \n",
    "        gap = val_loss - train_loss\n",
    "        print(f'\\nStep {step+1}: train={train_loss:.3f}, val={val_loss:.3f}, ppl={val_ppl:.2f}, gap={gap:.3f}')\n",
    "        \n",
    "        # Best model?\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model_config': model_config.__dict__,\n",
    "                'val_loss': val_loss,\n",
    "                'val_ppl': val_ppl,\n",
    "            }, ckpt_dir / 'best.pt')\n",
    "            print(f'  ✓ New best! Saved to {ckpt_dir}/best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement ({patience_counter}/{config[\"patience\"]})')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f'\\n⚠️ Early stopping at step {step+1}')\n",
    "            break\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('Training complete!')\n",
    "print(f'Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = ckpt_dir / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'Saved history to {history_path}')\n",
    "\n",
    "# Save config\n",
    "config_path = ckpt_dir / 'config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f'Saved config to {config_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['step'], history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(history['step'], history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "ax = axes[1]\n",
    "ax.plot(history['step'], history['val_ppl'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('Validation Perplexity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "ax = axes[2]\n",
    "gaps = [v - t for v, t in zip(history['val_loss'], history['train_loss'])]\n",
    "ax.fill_between(history['step'], gaps, alpha=0.5, color='orange')\n",
    "ax.plot(history['step'], gaps, 'orange', linewidth=2)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', label='Overfitting threshold')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Val - Train Loss')\n",
    "ax.set_title('Overfitting Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / 'training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'Saved plot to {ckpt_dir}/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "ckpt = torch.load(ckpt_dir / 'best.pt')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "# Full validation\n",
    "print('Evaluating best model on full validation set...')\n",
    "val_loss, val_ppl = evaluate(model, val_loader)\n",
    "print(f'Val Loss: {val_loss:.4f}')\n",
    "print(f'Val Perplexity: {val_ppl:.2f}')\n",
    "\n",
    "# Test set\n",
    "test_dataset = ByteDataset(test_data, config['block_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print('\\nEvaluating on test set...')\n",
    "test_loss, test_ppl = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Perplexity: {test_ppl:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoint\n",
    "\n",
    "Run this to download the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Zip checkpoints\n",
    "!zip -r checkpoints_wikitext2.zip checkpoints_wikitext2/\n",
    "\n",
    "# Download\n",
    "files.download('checkpoints_wikitext2.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}