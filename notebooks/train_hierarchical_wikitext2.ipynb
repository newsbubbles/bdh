{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical BDH Training on WikiText-2\n",
    "\n",
    "Train **Hierarchical BDH** (MEGABYTE-style architecture) on WikiText-2.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Input bytes (B, T)\n",
    "       ↓\n",
    "Byte Embedding (B, T, D_local)\n",
    "       ↓\n",
    "Patch Embedder (B, T/P, D_global)  <- Groups P bytes into patches\n",
    "       ↓\n",
    "Global BDH (6L, 512D, 8H)          <- Cross-patch semantics\n",
    "       ↓\n",
    "Global-to-Local Adapter            <- Injects global context\n",
    "       ↓\n",
    "Local BDH (4L, 256D, 4H)           <- Intra-patch refinement\n",
    "       ↓\n",
    "LM Head (B, T, 256)\n",
    "```\n",
    "\n",
    "**Key features:**\n",
    "- Both global and local models use full BDH attention (bottleneck + ReLU + gating)\n",
    "- Patch size P=8 (power of 2, ~1 token equivalent)\n",
    "- ~3:1 global/local parameter ratio (following MEGABYTE)\n",
    "- Size presets: tiny (~3M), small (~30M), base (~73M), large (~278M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || (cd bdh && git pull)\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import Hierarchical BDH\n",
    "from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WikiText-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading WikiText-2...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Concatenate all text\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "test_text = '\\n'.join(dataset['test']['text'])\n",
    "\n",
    "train_data = text_to_bytes(train_text)\n",
    "val_data = text_to_bytes(val_text)\n",
    "test_data = text_to_bytes(test_text)\n",
    "\n",
    "print(f'Train: {len(train_data):,} bytes ({len(train_data)/1e6:.1f}M)')\n",
    "print(f'Val: {len(val_data):,} bytes')\n",
    "print(f'Test: {len(test_data):,} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "\n",
    "**Important**: Block size must be divisible by patch size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size, patch_size=8):\n",
    "        # Ensure block_size is divisible by patch_size\n",
    "        assert block_size % patch_size == 0, \\\n",
    "            f'block_size ({block_size}) must be divisible by patch_size ({patch_size})'\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose a model size preset. Available options:\n",
    "- `tiny`: ~3M params, good for testing\n",
    "- `small`: ~30M params, matches original BDH, good for Colab T4\n",
    "- `base`: ~73M params, good for A100/V100\n",
    "- `large`: ~278M params, multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CHOOSE MODEL SIZE HERE\n",
    "# =============================================================\n",
    "MODEL_SIZE = 'small'  # Options: 'tiny', 'small', 'base', 'large'\n",
    "# =============================================================\n",
    "\n",
    "# Get model config from preset\n",
    "preset_map = {\n",
    "    'tiny': HierarchicalBDHConfig.tiny,\n",
    "    'small': HierarchicalBDHConfig.small,\n",
    "    'base': HierarchicalBDHConfig.base,\n",
    "    'large': HierarchicalBDHConfig.large,\n",
    "}\n",
    "model_config = preset_map[MODEL_SIZE](dropout=0.2)\n",
    "\n",
    "# Training config\n",
    "config = {\n",
    "    # Model (from preset)\n",
    "    'model_size': MODEL_SIZE,\n",
    "    'patch_size': model_config.patch_size,\n",
    "    \n",
    "    # Sequence\n",
    "    'block_size': 512,  # Must be divisible by patch_size\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 32 if MODEL_SIZE in ['tiny', 'small'] else 16,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'max_steps': 5000,\n",
    "    'warmup_steps': 200,\n",
    "    \n",
    "    # Validation\n",
    "    'val_interval': 100,\n",
    "    'val_batches': 50,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 10,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 50,\n",
    "}\n",
    "\n",
    "# Validate block_size\n",
    "assert config['block_size'] % config['patch_size'] == 0, \\\n",
    "    f\"block_size must be divisible by patch_size\"\n",
    "\n",
    "print(f'Model size: {MODEL_SIZE}')\n",
    "print(f'Patch size: {config[\"patch_size\"]}')\n",
    "print(f'Block size: {config[\"block_size\"]} ({config[\"block_size\"] // config[\"patch_size\"]} patches)')\n",
    "print()\n",
    "print('Training config:')\n",
    "for k, v in config.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = HierarchicalBDH(model_config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "params = model.count_parameters()\n",
    "\n",
    "print(f'\\n{\"-\"*50}')\n",
    "print(f'HIERARCHICAL BDH - {MODEL_SIZE.upper()}')\n",
    "print(f'{\"-\"*50}')\n",
    "print(f'Global model: {model_config.global_n_layer}L x {model_config.global_n_embd}D x {model_config.global_n_head}H')\n",
    "print(f'Local model:  {model_config.local_n_layer}L x {model_config.local_n_embd}D x {model_config.local_n_head}H')\n",
    "print(f'Patch size:   {model_config.patch_size}')\n",
    "print()\n",
    "print(f'Parameters:')\n",
    "print(f'  Global:    {params[\"global\"]:>12,}')\n",
    "print(f'  Local:     {params[\"local\"]:>12,}')\n",
    "print(f'  Embedding: {params[\"embedding\"]:>12,}')\n",
    "print(f'  LM Head:   {params[\"lm_head\"]:>12,}')\n",
    "print(f'  {\"-\"*25}')\n",
    "print(f'  Total:     {params[\"total\"]:>12,} ({params[\"total\"]/1e6:.1f}M)')\n",
    "print(f'\\nGlobal/Local ratio: {params[\"global_local_ratio\"]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (num_workers=0 for Colab compatibility)\n",
    "train_dataset = ByteDataset(train_data, config['block_size'], config['patch_size'])\n",
    "val_dataset = ByteDataset(val_data, config['block_size'], config['patch_size'])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Colab-safe\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f'Train batches: {len(train_loader):,}')\n",
    "print(f'Val batches: {len(val_loader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup + cosine decay\n",
    "def get_lr(step):\n",
    "    # Warmup\n",
    "    if step < config['warmup_steps']:\n",
    "        return config['learning_rate'] * step / config['warmup_steps']\n",
    "    # Cosine decay\n",
    "    progress = (step - config['warmup_steps']) / (config['max_steps'] - config['warmup_steps'])\n",
    "    return config['learning_rate'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "print('Optimizer: AdamW')\n",
    "print(f'  lr: {config[\"learning_rate\"]}')\n",
    "print(f'  weight_decay: {config[\"weight_decay\"]}')\n",
    "print(f'  warmup_steps: {config[\"warmup_steps\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    \"\"\"Evaluate model on dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    bpb = avg_loss / math.log(2)  # Bits per byte\n",
    "    return avg_loss, perplexity, bpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training state\n",
    "history = {\n",
    "    'step': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_ppl': [],\n",
    "    'val_bpb': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "step = 0\n",
    "\n",
    "# Checkpoints dir\n",
    "ckpt_dir = Path(f'checkpoints_hierarchical_{MODEL_SIZE}')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f'Checkpoints will be saved to: {ckpt_dir}')\n",
    "print('\\nStarting training...')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "running_loss = 0\n",
    "\n",
    "pbar = tqdm(range(config['max_steps']), desc='Training')\n",
    "\n",
    "for step in pbar:\n",
    "    # Get batch (cycle through data)\n",
    "    try:\n",
    "        x, y = next(train_iter)\n",
    "    except StopIteration:\n",
    "        train_iter = iter(train_loader)\n",
    "        x, y = next(train_iter)\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Forward + backward\n",
    "    _, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # Logging\n",
    "    if (step + 1) % config['log_interval'] == 0:\n",
    "        avg_loss = running_loss / config['log_interval']\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.3f}', 'lr': f'{lr:.2e}'})\n",
    "        running_loss = 0\n",
    "    \n",
    "    # Validation\n",
    "    if (step + 1) % config['val_interval'] == 0:\n",
    "        val_loss, val_ppl, val_bpb = evaluate(model, val_loader, config['val_batches'])\n",
    "        train_loss, _, _ = evaluate(model, train_loader, config['val_batches'])\n",
    "        \n",
    "        history['step'].append(step + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        history['val_bpb'].append(val_bpb)\n",
    "        history['lr'].append(lr)\n",
    "        \n",
    "        gap = val_loss - train_loss\n",
    "        print(f'\\nStep {step+1}: train={train_loss:.3f}, val={val_loss:.3f}, ppl={val_ppl:.2f}, bpb={val_bpb:.3f}, gap={gap:.3f}')\n",
    "        \n",
    "        # Best model?\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model_config': model_config.__dict__,\n",
    "                'val_loss': val_loss,\n",
    "                'val_ppl': val_ppl,\n",
    "                'val_bpb': val_bpb,\n",
    "            }, ckpt_dir / 'best.pt')\n",
    "            print(f'  \u2713 New best! Saved to {ckpt_dir}/best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement ({patience_counter}/{config[\"patience\"]})')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f'\\n\u26a0\ufe0f Early stopping at step {step+1}')\n",
    "            break\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('Training complete!')\n",
    "print(f'Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = ckpt_dir / 'training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'Saved history to {history_path}')\n",
    "\n",
    "# Save config\n",
    "full_config = {\n",
    "    **config,\n",
    "    'model_config': model_config.__dict__,\n",
    "    'parameters': params,\n",
    "}\n",
    "config_path = ckpt_dir / 'config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(full_config, f, indent=2)\n",
    "print(f'Saved config to {config_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['step'], history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(history['step'], history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "ax = axes[1]\n",
    "ax.plot(history['step'], history['val_ppl'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('Validation Perplexity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bits per byte\n",
    "ax = axes[2]\n",
    "ax.plot(history['step'], history['val_bpb'], 'm-', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('BPB')\n",
    "ax.set_title('Bits Per Byte')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "ax = axes[3]\n",
    "gaps = [v - t for v, t in zip(history['val_loss'], history['train_loss'])]\n",
    "ax.fill_between(history['step'], gaps, alpha=0.5, color='orange')\n",
    "ax.plot(history['step'], gaps, 'orange', linewidth=2)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', label='Overfitting threshold')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Val - Train Loss')\n",
    "ax.set_title('Overfitting Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Hierarchical BDH ({MODEL_SIZE}) - WikiText-2', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved plot to {ckpt_dir}/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "ckpt = torch.load(ckpt_dir / 'best.pt')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "# Full validation\n",
    "print('Evaluating best model on full validation set...')\n",
    "val_loss, val_ppl, val_bpb = evaluate(model, val_loader)\n",
    "print(f'Val Loss: {val_loss:.4f}')\n",
    "print(f'Val Perplexity: {val_ppl:.2f}')\n",
    "print(f'Val BPB: {val_bpb:.3f}')\n",
    "\n",
    "# Test set\n",
    "test_dataset = ByteDataset(test_data, config['block_size'], config['patch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "print('\\nEvaluating on test set...')\n",
    "test_loss, test_ppl, test_bpb = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Perplexity: {test_ppl:.2f}')\n",
    "print(f'Test BPB: {test_bpb:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_new_tokens=200, temperature=0.8, top_k=50):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    prompt_bytes = list(prompt.encode('utf-8'))\n",
    "    idx = torch.tensor([prompt_bytes], device=device, dtype=torch.long)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    \n",
    "    # Decode\n",
    "    output_bytes = output[0].tolist()\n",
    "    try:\n",
    "        text = bytes(output_bytes).decode('utf-8', errors='replace')\n",
    "    except:\n",
    "        text = ''.join(chr(b) if 32 <= b < 127 else '?' for b in output_bytes)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    'The history of',\n",
    "    'In 1920, the',\n",
    "    'Scientists discovered',\n",
    "    'The capital of France',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('GENERATION SAMPLES')\n",
    "print('=' * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\nPrompt: \"{prompt}\"')\n",
    "    print('-' * 40)\n",
    "    output = generate_text(model, prompt, max_new_tokens=150, temperature=0.8)\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Baseline BDH\n",
    "\n",
    "If you have results from the baseline BDH training, compare here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('COMPARISON SUMMARY')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print(f'Hierarchical BDH ({MODEL_SIZE}):')\n",
    "print(f'  Parameters: {params[\"total\"]:,} ({params[\"total\"]/1e6:.1f}M)')\n",
    "print(f'  Test Loss:  {test_loss:.4f}')\n",
    "print(f'  Test PPL:   {test_ppl:.2f}')\n",
    "print(f'  Test BPB:   {test_bpb:.3f}')\n",
    "print()\n",
    "print('Baseline BDH (from previous run):')\n",
    "print('  Parameters: ~25M')\n",
    "print('  Test PPL:   ~3.19')\n",
    "print('  Test BPB:   ~1.67')\n",
    "print()\n",
    "print('\u2192 Hierarchical BDH should show improved coherence at similar perplexity,')\n",
    "print('  due to multi-scale processing (global cross-patch + local intra-patch).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Create zip file\n",
    "zip_name = f'hierarchical_bdh_{MODEL_SIZE}_wikitext2.zip'\n",
    "subprocess.run(['zip', '-r', zip_name, str(ckpt_dir)], check=True)\n",
    "print(f'Created: {zip_name}')\n",
    "\n",
    "# Auto-download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print('Downloading checkpoint...')\n",
    "    files.download(zip_name)\n",
    "    print('\\n\u2705 Download started! Check your browser downloads.')\n",
    "except ImportError:\n",
    "    print('Not running in Colab.')\n",
    "    print(f'Checkpoints saved to: {ckpt_dir}')\n",
    "    print(f'Zip file: {zip_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
