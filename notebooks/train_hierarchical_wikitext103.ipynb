{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical BDH Training on WikiText-103\n",
    "\n",
    "Train **Hierarchical BDH** on WikiText-103 (~100M tokens, 100x WikiText-2).\n",
    "\n",
    "## Why WikiText-103?\n",
    "- **Scale test**: Same domain as WikiText-2 but 100x larger\n",
    "- **Standard benchmark**: Widely used for LM comparison\n",
    "- **Long articles**: Full Wikipedia articles, tests long-range modeling\n",
    "\n",
    "## Expected Outcomes\n",
    "- Higher perplexity than WikiText-2 (harder, more diverse)\n",
    "- Better generalization (more data, less overfitting)\n",
    "- Test hierarchical architecture at scale\n",
    "\n",
    "## Hardware Requirements\n",
    "- **Recommended**: A100 40GB+ or equivalent\n",
    "- **Minimum**: V100 32GB (reduce batch size)\n",
    "- Training time: ~2-4 hours for 10K steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || (cd bdh && git pull)\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import Hierarchical BDH\n",
    "from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WikiText-103\n",
    "\n",
    "This is a large dataset (~500MB). First load may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading WikiText-103 (this may take a few minutes)...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Concatenate all text\n",
    "print('Processing train split...')\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "print('Processing validation split...')\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "print('Processing test split...')\n",
    "test_text = '\\n'.join(dataset['test']['text'])\n",
    "\n",
    "print('\\nConverting to bytes...')\n",
    "train_data = text_to_bytes(train_text)\n",
    "val_data = text_to_bytes(val_text)\n",
    "test_data = text_to_bytes(test_text)\n",
    "\n",
    "print(f'\\nDataset sizes:')\n",
    "print(f'  Train: {len(train_data):,} bytes ({len(train_data)/1e6:.1f}M)')\n",
    "print(f'  Val:   {len(val_data):,} bytes ({len(val_data)/1e6:.1f}M)')\n",
    "print(f'  Test:  {len(test_data):,} bytes ({len(test_data)/1e6:.1f}M)')\n",
    "print(f'\\nTotal: {(len(train_data) + len(val_data) + len(test_data))/1e6:.1f}M bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size, patch_size=8):\n",
    "        assert block_size % patch_size == 0, \\\n",
    "            f'block_size ({block_size}) must be divisible by patch_size ({patch_size})'\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "For WikiText-103, we use longer training and larger batch sizes.\n",
    "\n",
    "| Size | Params | Batch | VRAM Est. |\n",
    "|------|--------|-------|-----------|\n",
    "| small | 30M | 32 | ~12GB |\n",
    "| base | 73M | 16 | ~24GB |\n",
    "| large | 278M | 8 | ~40GB+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CHOOSE MODEL SIZE HERE\n",
    "# =============================================================\n",
    "MODEL_SIZE = 'base'  # Options: 'tiny', 'small', 'base', 'large'\n",
    "# =============================================================\n",
    "\n",
    "# Get model config from preset\n",
    "preset_map = {\n",
    "    'tiny': HierarchicalBDHConfig.tiny,\n",
    "    'small': HierarchicalBDHConfig.small,\n",
    "    'base': HierarchicalBDHConfig.base,\n",
    "    'large': HierarchicalBDHConfig.large,\n",
    "}\n",
    "model_config = preset_map[MODEL_SIZE](dropout=0.1)  # Less dropout for larger data\n",
    "\n",
    "# Training config - adjusted for WikiText-103\n",
    "config = {\n",
    "    # Model\n",
    "    'model_size': MODEL_SIZE,\n",
    "    'patch_size': model_config.patch_size,\n",
    "    \n",
    "    # Sequence - longer for WikiText-103\n",
    "    'block_size': 1024,  # Longer context\n",
    "    \n",
    "    # Training - more steps for larger dataset\n",
    "    'batch_size': {'tiny': 64, 'small': 32, 'base': 16, 'large': 8}[MODEL_SIZE],\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'max_steps': 20000,  # More steps for larger dataset\n",
    "    'warmup_steps': 500,\n",
    "    \n",
    "    # Gradient accumulation for effective larger batches\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    \n",
    "    # Validation\n",
    "    'val_interval': 500,\n",
    "    'val_batches': 100,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 10,\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 100,\n",
    "}\n",
    "\n",
    "# Validate\n",
    "assert config['block_size'] % config['patch_size'] == 0\n",
    "\n",
    "effective_batch = config['batch_size'] * config['gradient_accumulation_steps']\n",
    "\n",
    "print(f'Model size: {MODEL_SIZE}')\n",
    "print(f'Patch size: {config[\"patch_size\"]}')\n",
    "print(f'Block size: {config[\"block_size\"]} ({config[\"block_size\"] // config[\"patch_size\"]} patches)')\n",
    "print(f'Batch size: {config[\"batch_size\"]} x {config[\"gradient_accumulation_steps\"]} = {effective_batch} effective')\n",
    "print()\n",
    "print('Training config:')\n",
    "for k, v in config.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = HierarchicalBDH(model_config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "params = model.count_parameters()\n",
    "\n",
    "print(f'\\n{\"-\"*50}')\n",
    "print(f'HIERARCHICAL BDH - {MODEL_SIZE.upper()}')\n",
    "print(f'{\"-\"*50}')\n",
    "print(f'Global model: {model_config.global_n_layer}L x {model_config.global_n_embd}D x {model_config.global_n_head}H')\n",
    "print(f'Local model:  {model_config.local_n_layer}L x {model_config.local_n_embd}D x {model_config.local_n_head}H')\n",
    "print(f'Patch size:   {model_config.patch_size}')\n",
    "print()\n",
    "print(f'Parameters:')\n",
    "print(f'  Global:    {params[\"global_model\"]:>12,}')\n",
    "print(f'  Local:     {params[\"local_model\"]:>12,}')\n",
    "print(f'  Patch Emb: {params[\"patch_embedder\"]:>12,}')\n",
    "print(f'  LM Head:   {params[\"lm_head\"]:>12,}')\n",
    "print(f'  {\"-\"*25}')\n",
    "print(f'  Total:     {params[\"total\"]:>12,} ({params[\"total\"]/1e6:.1f}M)')\n",
    "print(f'\\nGlobal/Local ratio: {params[\"global_model\"]/params[\"local_model\"]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_dataset = ByteDataset(train_data, config['block_size'], config['patch_size'])\n",
    "val_dataset = ByteDataset(val_data, config['block_size'], config['patch_size'])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f'Train batches: {len(train_loader):,}')\n",
    "print(f'Val batches: {len(val_loader):,}')\n",
    "print(f'Steps per epoch: ~{len(train_loader) // config[\"gradient_accumulation_steps\"]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step):\n",
    "    if step < config['warmup_steps']:\n",
    "        return config['learning_rate'] * step / config['warmup_steps']\n",
    "    progress = (step - config['warmup_steps']) / (config['max_steps'] - config['warmup_steps'])\n",
    "    return config['learning_rate'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "# Mixed precision for faster training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print('Optimizer: AdamW with mixed precision (fp16)')\n",
    "print(f'  lr: {config[\"learning_rate\"]}')\n",
    "print(f'  weight_decay: {config[\"weight_decay\"]}')\n",
    "print(f'  warmup_steps: {config[\"warmup_steps\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    bpb = avg_loss / math.log(2)\n",
    "    return avg_loss, perplexity, bpb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training state\n",
    "history = {\n",
    "    'step': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_ppl': [],\n",
    "    'val_bpb': [],\n",
    "    'lr': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "step = 0\n",
    "\n",
    "# Checkpoints dir\n",
    "ckpt_dir = Path(f'checkpoints_hierarchical_{MODEL_SIZE}_wikitext103')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f'Checkpoints: {ckpt_dir}')\n",
    "print('\\nStarting training...')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "running_loss = 0\n",
    "accum_steps = 0\n",
    "\n",
    "pbar = tqdm(range(config['max_steps']), desc='Training')\n",
    "\n",
    "for step in pbar:\n",
    "    # Accumulate gradients\n",
    "    optimizer.zero_grad()\n",
    "    accum_loss = 0\n",
    "    \n",
    "    for _ in range(config['gradient_accumulation_steps']):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / config['gradient_accumulation_steps']\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        accum_loss += loss.item()\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Gradient clipping and step\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    running_loss += accum_loss\n",
    "    \n",
    "    # Logging\n",
    "    if (step + 1) % config['log_interval'] == 0:\n",
    "        avg_loss = running_loss / config['log_interval']\n",
    "        pbar.set_postfix({'loss': f'{avg_loss:.3f}', 'lr': f'{lr:.2e}'})\n",
    "        running_loss = 0\n",
    "    \n",
    "    # Validation\n",
    "    if (step + 1) % config['val_interval'] == 0:\n",
    "        val_loss, val_ppl, val_bpb = evaluate(model, val_loader, config['val_batches'])\n",
    "        train_loss, _, _ = evaluate(model, train_loader, config['val_batches'])\n",
    "        \n",
    "        history['step'].append(step + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        history['val_bpb'].append(val_bpb)\n",
    "        history['lr'].append(lr)\n",
    "        \n",
    "        gap = val_loss - train_loss\n",
    "        print(f'\\nStep {step+1}: train={train_loss:.3f}, val={val_loss:.3f}, ppl={val_ppl:.2f}, bpb={val_bpb:.3f}, gap={gap:.3f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model_config': model_config.__dict__,\n",
    "                'val_loss': val_loss,\n",
    "                'val_ppl': val_ppl,\n",
    "                'val_bpb': val_bpb,\n",
    "                'dataset': 'wikitext-103',\n",
    "            }, ckpt_dir / 'best.pt')\n",
    "            print(f'  \u2713 New best! Saved to {ckpt_dir}/best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement ({patience_counter}/{config[\"patience\"]})')\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f'\\n\u26a0\ufe0f Early stopping at step {step+1}')\n",
    "            break\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('Training complete!')\n",
    "print(f'Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "with open(ckpt_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Save config\n",
    "full_config = {\n",
    "    **config,\n",
    "    'model_config': model_config.__dict__,\n",
    "    'parameters': params,\n",
    "    'dataset': 'wikitext-103',\n",
    "    'dataset_size_bytes': len(train_data),\n",
    "}\n",
    "with open(ckpt_dir / 'config.json', 'w') as f:\n",
    "    json.dump(full_config, f, indent=2)\n",
    "\n",
    "print(f'Saved history and config to {ckpt_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "ax.plot(history['step'], history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "ax.plot(history['step'], history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "ax = axes[1]\n",
    "ax.plot(history['step'], history['val_ppl'], 'g-', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('Validation Perplexity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bits per byte\n",
    "ax = axes[2]\n",
    "ax.plot(history['step'], history['val_bpb'], 'm-', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('BPB')\n",
    "ax.set_title('Bits Per Byte')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gap\n",
    "ax = axes[3]\n",
    "gaps = [v - t for v, t in zip(history['val_loss'], history['train_loss'])]\n",
    "ax.fill_between(history['step'], gaps, alpha=0.5, color='orange')\n",
    "ax.plot(history['step'], gaps, 'orange', linewidth=2)\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', label='Overfitting threshold')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Val - Train Loss')\n",
    "ax.set_title('Overfitting Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Hierarchical BDH ({MODEL_SIZE}) - WikiText-103', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "ckpt = torch.load(ckpt_dir / 'best.pt')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "# Full validation\n",
    "print('Evaluating best model on full validation set...')\n",
    "val_loss, val_ppl, val_bpb = evaluate(model, val_loader)\n",
    "print(f'Val Loss: {val_loss:.4f}')\n",
    "print(f'Val Perplexity: {val_ppl:.2f}')\n",
    "print(f'Val BPB: {val_bpb:.3f}')\n",
    "\n",
    "# Test set\n",
    "test_dataset = ByteDataset(test_data, config['block_size'], config['patch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print('\\nEvaluating on test set...')\n",
    "test_loss, test_ppl, test_bpb = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Perplexity: {test_ppl:.2f}')\n",
    "print(f'Test BPB: {test_bpb:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_new_tokens=200, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    prompt_bytes = list(prompt.encode('utf-8'))\n",
    "    idx = torch.tensor([prompt_bytes], device=device, dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    \n",
    "    output_bytes = output[0].tolist()\n",
    "    try:\n",
    "        text = bytes(output_bytes).decode('utf-8', errors='replace')\n",
    "    except:\n",
    "        text = ''.join(chr(b) if 32 <= b < 127 else '?' for b in output_bytes)\n",
    "    return text\n",
    "\n",
    "prompts = [\n",
    "    'The history of',\n",
    "    'In 1920, the',\n",
    "    'Scientists discovered',\n",
    "    'The capital of France',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('GENERATION SAMPLES')\n",
    "print('=' * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\nPrompt: \"{prompt}\"')\n",
    "    print('-' * 40)\n",
    "    output = generate_text(model, prompt, max_new_tokens=150, temperature=0.8)\n",
    "    print(output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('WIKITEXT-103 RESULTS SUMMARY')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print(f'Model: Hierarchical BDH ({MODEL_SIZE})')\n",
    "print(f'Parameters: {params[\"total\"]:,} ({params[\"total\"]/1e6:.1f}M)')\n",
    "print(f'Dataset: WikiText-103 ({len(train_data)/1e6:.1f}M bytes)')\n",
    "print()\n",
    "print('Final Metrics:')\n",
    "print(f'  Test Loss:      {test_loss:.4f}')\n",
    "print(f'  Test Perplexity: {test_ppl:.2f}')\n",
    "print(f'  Test BPB:       {test_bpb:.3f}')\n",
    "print()\n",
    "print('Comparison (expected ranges):')\n",
    "print('  WikiText-2 Hierarchical: PPL ~1.2 (smaller, easier)')\n",
    "print('  WikiText-103 should show higher PPL due to diversity')\n",
    "print('  But better generalization (larger training set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "zip_name = f'hierarchical_bdh_{MODEL_SIZE}_wikitext103.zip'\n",
    "subprocess.run(['zip', '-r', zip_name, str(ckpt_dir)], check=True)\n",
    "print(f'Created: {zip_name}')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print('Downloading checkpoint...')\n",
    "    files.download(zip_name)\n",
    "    print('\\n\u2705 Download started!')\n",
    "except ImportError:\n",
    "    print(f'Checkpoints saved to: {ckpt_dir}')\n",
    "    print(f'Zip file: {zip_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
