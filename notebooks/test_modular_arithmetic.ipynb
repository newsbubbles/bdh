{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Modular Arithmetic & Grokking Test\n",
    "\n",
    "**Goal**: Test if vanilla BDH can learn modular arithmetic and exhibit grokking behavior.\n",
    "\n",
    "## What is Grokking?\n",
    "Grokking = sudden generalization long after memorization (train acc = 100%, then later test acc jumps).\n",
    "\n",
    "## Task: (a + b) mod p\n",
    "- Input: `a b =` (as bytes)\n", 
    "- Output: `c` where c = (a + b) mod p\n",
    "- Small p (e.g., 97) so we can test all pairs\n",
    "\n",
    "## Hypotheses\n",
    "1. BDH's sparse features might form modular circuits\n",
    "2. Linear attention might help/hurt pattern discovery\n",
    "3. Neuron count (N) might affect grokking speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import dataclasses\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q matplotlib tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla BDH Implementation\n",
    "\n",
    "Direct copy from bdh.py - no modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla BDH - exact copy from bdh.py\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class BDHConfig:\n",
    "    n_layer: int = 6\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    n_head: int = 4\n",
    "    mlp_internal_dim_multiplier: int = 128  # This controls N (neurons)\n",
    "    vocab_size: int = 256\n",
    "\n",
    "\n",
    "def get_freqs(n, theta, dtype):\n",
    "    def quantize(t, q=2):\n",
    "        return (t / q).floor() * q\n",
    "    return (\n",
    "        1.0\n",
    "        / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n))\n",
    "        / (2 * math.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def phases_cos_sin(phases):\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        phases_cos = torch.cos(phases)\n",
    "        phases_sin = torch.sin(phases)\n",
    "        return phases_cos, phases_sin\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases, v):\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())\n",
    "        phases_cos, phases_sin = Attention.phases_cos_sin(phases)\n",
    "        return (v * phases_cos).to(v.dtype) + (v_rot * phases_sin).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        assert self.freqs.dtype == torch.float32\n",
    "        assert K is Q\n",
    "        _, _, T, _ = Q.size()\n",
    "        r_phases = (\n",
    "            torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype)\n",
    "            .view(1, 1, -1, 1)\n",
    "        ) * self.freqs\n",
    "        QR = self.rope(r_phases, Q)\n",
    "        KR = QR\n",
    "        scores = (QR @ KR.mT).tril(diagonal=-1)\n",
    "        return scores @ V\n",
    "\n",
    "\n",
    "class BDH(nn.Module):\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))\n",
    "        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.attn = Attention(config)\n",
    "        self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.lm_head = nn.Parameter(torch.zeros((D, config.vocab_size)).normal_(std=0.02))\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        C = self.config\n",
    "        B, T = idx.size()\n",
    "        D = C.n_embd\n",
    "        nh = C.n_head\n",
    "        N = D * C.mlp_internal_dim_multiplier // nh\n",
    "        x = self.embed(idx).unsqueeze(1)\n",
    "        x = self.ln(x)\n",
    "        for level in range(C.n_layer):\n",
    "            x_latent = x @ self.encoder\n",
    "            x_sparse = F.relu(x_latent)\n",
    "            yKV = self.attn(Q=x_sparse, K=x_sparse, V=x)\n",
    "            yKV = self.ln(yKV)\n",
    "            y_latent = yKV @ self.encoder_v\n",
    "            y_sparse = F.relu(y_latent)\n",
    "            xy_sparse = x_sparse * y_sparse\n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "            yMLP = xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)\n",
    "        logits = x.view(B, T, D) @ self.lm_head\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('BDH model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular Arithmetic Dataset\n",
    "\n",
    "Task: Given `a op b =`, predict `c` where `c = (a op b) mod p`\n",
    "\n",
    "We encode numbers as byte sequences for BDH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModularArithmeticDataset:\n",
    "    \"\"\"Dataset for modular arithmetic: (a op b) mod p\"\"\"\n",
    "    \n",
    "    def __init__(self, p=97, operation='add', train_frac=0.5, seed=42):\n",
    "        self.p = p\n",
    "        self.operation = operation\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Generate all pairs\n",
    "        all_pairs = [(a, b) for a in range(p) for b in range(p)]\n",
    "        random.seed(seed)\n",
    "        random.shuffle(all_pairs)\n",
    "        \n",
    "        # Split train/test\n",
    "        split_idx = int(len(all_pairs) * train_frac)\n",
    "        self.train_pairs = all_pairs[:split_idx]\n",
    "        self.test_pairs = all_pairs[split_idx:]\n",
    "        \n",
    "        # Operation functions\n",
    "        self.ops = {\n",
    "            'add': lambda a, b: (a + b) % p,\n",
    "            'sub': lambda a, b: (a - b) % p,\n",
    "            'mul': lambda a, b: (a * b) % p,\n",
    "            'div': lambda a, b: (a * pow(b, p-2, p)) % p if b != 0 else 0,  # Fermat's little theorem\n",
    "        }\n",
    "        self.op_fn = self.ops[operation]\n",
    "        self.op_symbol = {\n",
    "            'add': '+', 'sub': '-', 'mul': '*', 'div': '/'\n",
    "        }[operation]\n",
    "        \n",
    "        print(f'Dataset: ({operation}) mod {p}')\n",
    "        print(f'  Train pairs: {len(self.train_pairs)}')\n",
    "        print(f'  Test pairs: {len(self.test_pairs)}')\n",
    "    \n",
    "    def encode_example(self, a, b):\n",
    "        \"\"\"Encode as bytes: 'a op b = c'\"\"\"\n",
    "        c = self.op_fn(a, b)\n",
    "        # Format: \"XX+YY=ZZ\" where XX, YY, ZZ are zero-padded\n",
    "        text = f'{a:02d}{self.op_symbol}{b:02d}={c:02d}'\n",
    "        return [ord(ch) for ch in text]\n",
    "    \n",
    "    def decode_output(self, bytes_list):\n",
    "        \"\"\"Decode bytes back to string\"\"\"\n",
    "        return ''.join(chr(b) for b in bytes_list if 32 <= b < 127)\n",
    "    \n",
    "    def get_batch(self, split, batch_size):\n",
    "        \"\"\"Get a batch of examples\"\"\"\n",
    "        pairs = self.train_pairs if split == 'train' else self.test_pairs\n",
    "        batch_pairs = random.choices(pairs, k=batch_size)\n",
    "        \n",
    "        sequences = [self.encode_example(a, b) for a, b in batch_pairs]\n",
    "        max_len = max(len(s) for s in sequences)\n",
    "        \n",
    "        # Pad sequences\n",
    "        x = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            x[i, :len(seq)] = torch.tensor(seq)\n",
    "        \n",
    "        # Input is all but last, target is all but first\n",
    "        return x[:, :-1].to(device), x[:, 1:].to(device)\n",
    "    \n",
    "    def evaluate_accuracy(self, model, split='test'):\n",
    "        \"\"\"Evaluate exact match accuracy on predicting the result\"\"\"\n",
    "        model.eval()\n",
    "        pairs = self.train_pairs if split == 'train' else self.test_pairs\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for a, b in pairs:\n",
    "                c_true = self.op_fn(a, b)\n",
    "                \n",
    "                # Encode input up to '='\n",
    "                prompt = f'{a:02d}{self.op_symbol}{b:02d}='\n",
    "                prompt_bytes = torch.tensor([[ord(ch) for ch in prompt]], device=device)\n",
    "                \n",
    "                # Get model prediction for next 2 bytes (the result)\n",
    "                logits, _ = model(prompt_bytes)\n",
    "                pred_byte1 = logits[0, -1, :].argmax().item()\n",
    "                \n",
    "                # Get second byte\n",
    "                prompt_bytes2 = torch.cat([\n",
    "                    prompt_bytes, \n",
    "                    torch.tensor([[pred_byte1]], device=device)\n",
    "                ], dim=1)\n",
    "                logits2, _ = model(prompt_bytes2)\n",
    "                pred_byte2 = logits2[0, -1, :].argmax().item()\n",
    "                \n",
    "                # Decode prediction\n",
    "                try:\n",
    "                    pred_str = chr(pred_byte1) + chr(pred_byte2)\n",
    "                    c_pred = int(pred_str)\n",
    "                    if c_pred == c_true:\n",
    "                        correct += 1\n",
    "                except:\n",
    "                    pass  # Invalid prediction\n",
    "                \n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "# Test the dataset\n",
    "dataset = ModularArithmeticDataset(p=97, operation='add')\n",
    "x, y = dataset.get_batch('train', 4)\n",
    "print(f'\\nBatch shapes: x={x.shape}, y={y.shape}')\n",
    "print(f'Example: {dataset.decode_output(x[0].tolist())} -> {dataset.decode_output(y[0].tolist())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Settings optimized for observing grokking:\n",
    "- Small model (to see grokking faster)\n",
    "- High weight decay (helps grokking)\n",
    "- Long training (grokking happens late)\n",
    "- Full batch training (common in grokking papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configurations\n",
    "\n",
    "CONFIGS = {\n",
    "    'tiny': BDHConfig(\n",
    "        n_layer=2,\n",
    "        n_embd=64,\n",
    "        n_head=2,\n",
    "        mlp_internal_dim_multiplier=32,  # N = 32*64/2 = 1024 neurons\n",
    "        dropout=0.0,  # No dropout for grokking\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "    'small': BDHConfig(\n",
    "        n_layer=2,\n",
    "        n_embd=128,\n",
    "        n_head=4,\n",
    "        mlp_internal_dim_multiplier=64,  # N = 64*128/4 = 2048 neurons\n",
    "        dropout=0.0,\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "    'medium': BDHConfig(\n",
    "        n_layer=4,\n",
    "        n_embd=256,\n",
    "        n_head=4,\n",
    "        mlp_internal_dim_multiplier=128,  # N = 128*256/4 = 8192 neurons\n",
    "        dropout=0.0,\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Training settings\n",
    "TRAIN_CONFIG = {\n",
    "    'max_steps': 50000,        # Long training for grokking\n",
    "    'batch_size': 512,         # Full-ish batch\n",
    "    'learning_rate': 1e-3,     # Standard\n",
    "    'weight_decay': 1.0,       # High weight decay helps grokking!\n",
    "    'eval_interval': 100,      # Frequent eval to catch grokking\n",
    "    'log_interval': 100,\n",
    "}\n",
    "\n",
    "# Select config\n",
    "config_name = 'small'  # @param ['tiny', 'small', 'medium']\n",
    "config = CONFIGS[config_name]\n",
    "\n",
    "model = BDH(config).to(device)\n",
    "print(f'\\nModel: {config_name}')\n",
    "print(f'Parameters: {count_parameters(model):,}')\n",
    "print(f'Neurons (N): {config.mlp_internal_dim_multiplier * config.n_embd // config.n_head:,}')\n",
    "print(f'Layers: {config.n_layer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Grokking Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_track(model, dataset, config, train_config):\n",
    "    \"\"\"Train model and track metrics for grokking analysis\"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config['learning_rate'],\n",
    "        weight_decay=train_config['weight_decay'],\n",
    "        betas=(0.9, 0.98)  # Common for grokking\n",
    "    )\n",
    "    \n",
    "    # Metrics tracking\n",
    "    history = {\n",
    "        'step': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    "    \n",
    "    best_test_acc = 0.0\n",
    "    grokking_step = None\n",
    "    memorization_step = None\n",
    "    \n",
    "    pbar = tqdm(range(train_config['max_steps']), desc='Training')\n",
    "    \n",
    "    for step in pbar:\n",
    "        model.train()\n",
    "        \n",
    "        # Get batch\n",
    "        x, y = dataset.get_batch('train', train_config['batch_size'])\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate periodically\n",
    "        if step % train_config['eval_interval'] == 0:\n",
    "            train_acc = dataset.evaluate_accuracy(model, 'train')\n",
    "            test_acc = dataset.evaluate_accuracy(model, 'test')\n",
    "            \n",
    "            history['step'].append(step)\n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            \n",
    "            # Detect memorization (train acc > 99%)\n",
    "            if memorization_step is None and train_acc > 0.99:\n",
    "                memorization_step = step\n",
    "                print(f'\\n>>> MEMORIZATION at step {step} (train_acc={train_acc:.1%})')\n",
    "            \n",
    "            # Detect grokking (test acc > 90% after memorization)\n",
    "            if grokking_step is None and memorization_step is not None and test_acc > 0.90:\n",
    "                grokking_step = step\n",
    "                print(f'\\n>>> GROKKING at step {step} (test_acc={test_acc:.1%})')\n",
    "                print(f'    Steps after memorization: {step - memorization_step}')\n",
    "            \n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'train': f'{train_acc:.1%}',\n",
    "                'test': f'{test_acc:.1%}',\n",
    "            })\n",
    "        \n",
    "        # Early stopping if fully grokked\n",
    "        if grokking_step is not None and test_acc > 0.99:\n",
    "            print(f'\\n>>> FULLY GROKKED at step {step}!')\n",
    "            break\n",
    "    \n",
    "    return history, {\n",
    "        'memorization_step': memorization_step,\n",
    "        'grokking_step': grokking_step,\n",
    "        'best_test_acc': best_test_acc,\n",
    "        'final_train_acc': history['train_acc'][-1] if history['train_acc'] else 0,\n",
    "        'final_test_acc': history['test_acc'][-1] if history['test_acc'] else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "dataset = ModularArithmeticDataset(p=97, operation='add', train_frac=0.5)\n",
    "model = BDH(config).to(device)\n",
    "\n",
    "print(f'\\nStarting training...')\n",
    "print(f'Config: {config_name}')\n",
    "print(f'Parameters: {count_parameters(model):,}')\n",
    "print()\n",
    "\n",
    "history, results = train_and_track(model, dataset, config, TRAIN_CONFIG)\n",
    "\n",
    "print(f'\\n' + '='*50)\n",
    "print('RESULTS')\n",
    "print('='*50)\n",
    "print(f\"Memorization step: {results['memorization_step']}\")\n",
    "print(f\"Grokking step: {results['grokking_step']}\")\n",
    "print(f\"Best test accuracy: {results['best_test_acc']:.1%}\")\n",
    "print(f\"Final train accuracy: {results['final_train_acc']:.1%}\")\n",
    "print(f\"Final test accuracy: {results['final_test_acc']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grokking(history, results, title='BDH Grokking Analysis'):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Accuracy curves\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(history['step'], history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "    ax1.plot(history['step'], history['test_acc'], 'r-', label='Test Accuracy', linewidth=2)\n",
    "    \n",
    "    # Mark memorization and grokking\n",
    "    if results['memorization_step']:\n",
    "        ax1.axvline(results['memorization_step'], color='blue', linestyle='--', alpha=0.7, label='Memorization')\n",
    "    if results['grokking_step']:\n",
    "        ax1.axvline(results['grokking_step'], color='red', linestyle='--', alpha=0.7, label='Grokking')\n",
    "    \n",
    "    ax1.set_xlabel('Training Step')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Train vs Test Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Plot 2: Loss curve\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history['step'], history['train_loss'], 'g-', linewidth=2)\n",
    "    ax2.set_xlabel('Training Step')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('grokking_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_grokking(history, results, f'BDH ({config_name}) - Modular Addition mod 97')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Operations\n",
    "\n",
    "Test if BDH groks different modular operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare operations\n",
    "operations = ['add', 'sub', 'mul']  # div is harder\n",
    "all_results = {}\n",
    "all_histories = {}\n",
    "\n",
    "for op in operations:\n",
    "    print(f'\\n{\"="*60}')\n",
    "    print(f'Testing operation: {op}')\n",
    "    print('='*60)\n",
    "    \n",
    "    dataset = ModularArithmeticDataset(p=97, operation=op, train_frac=0.5)\n",
    "    model = BDH(config).to(device)\n",
    "    \n",
    "    history, results = train_and_track(model, dataset, config, TRAIN_CONFIG)\n",
    "    \n",
    "    all_results[op] = results\n",
    "    all_histories[op] = history\n",
    "    \n",
    "    print(f\"\\n{op}: grokking={results['grokking_step']}, best_test={results['best_test_acc']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, len(operations), figsize=(5*len(operations), 4))\n",
    "\n",
    "for i, op in enumerate(operations):\n",
    "    ax = axes[i] if len(operations) > 1 else axes\n",
    "    history = all_histories[op]\n",
    "    results = all_results[op]\n",
    "    \n",
    "    ax.plot(history['step'], history['train_acc'], 'b-', label='Train', linewidth=2)\n",
    "    ax.plot(history['step'], history['test_acc'], 'r-', label='Test', linewidth=2)\n",
    "    \n",
    "    if results['grokking_step']:\n",
    "        ax.axvline(results['grokking_step'], color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'{op.upper()} mod 97')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.suptitle('BDH Grokking: Different Modular Operations', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('grokking_operations_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print('\\n' + '='*70)\n",
    "print('GROKKING SUMMARY')\n",
    "print('='*70)\n",
    "print(f\"{'Operation':<12} {'Memorization':<15} {'Grokking':<15} {'Best Test Acc':<15}\")\n",
    "print('-'*70)\n",
    "\n",
    "for op in operations:\n",
    "    r = all_results[op]\n",
    "    mem = r['memorization_step'] if r['memorization_step'] else 'N/A'\n",
    "    grok = r['grokking_step'] if r['grokking_step'] else 'N/A'\n",
    "    print(f\"{op:<12} {str(mem):<15} {str(grok):<15} {r['best_test_acc']:.1%}\")\n",
    "\n",
    "print('='*70)\n",
    "print(f\"Model: BDH {config_name} ({count_parameters(model):,} params)\")\n",
    "print(f\"Neurons (N): {config.mlp_internal_dim_multiplier * config.n_embd // config.n_head:,}\")\n",
    "print(f\"Weight decay: {TRAIN_CONFIG['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_data = {\n",
    "    'config': {\n",
    "        'name': config_name,\n",
    "        'n_layer': config.n_layer,\n",
    "        'n_embd': config.n_embd,\n",
    "        'n_head': config.n_head,\n",
    "        'neurons': config.mlp_internal_dim_multiplier * config.n_embd // config.n_head,\n",
    "        'params': count_parameters(model),\n",
    "    },\n",
    "    'train_config': TRAIN_CONFIG,\n",
    "    'results': {op: {k: v for k, v in r.items()} for op, r in all_results.items()},\n",
    "    'histories': {op: h for op, h in all_histories.items()},\n",
    "}\n",
    "\n",
    "with open('grokking_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print('Results saved to grokking_results.json')\n",
    "\n",
    "# Download in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('grokking_results.json')\n",
    "    files.download('grokking_results.png')\n",
    "    files.download('grokking_operations_comparison.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}