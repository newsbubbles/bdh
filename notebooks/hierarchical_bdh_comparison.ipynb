{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical BDH vs Original BDH Comparison\n",
    "\n",
    "This notebook compares:\n",
    "- **Original BDH**: Single-scale byte-level attention\n",
    "- **Hierarchical BDH**: MEGABYTE-inspired global (patch) + local (byte) architecture\n",
    "\n",
    "## Architecture Comparison\n",
    "\n",
    "| Aspect | Original BDH | Hierarchical BDH |\n",
    "|--------|-------------|------------------|\n",
    "| Scale | Single (byte) | Multi (patch + byte) |\n",
    "| Attention | O(T²) on bytes | O((T/P)²) global + O(P²) local |\n",
    "| Context | Direct byte attention | Patch-level + byte-level |\n",
    "| Semantic grouping | Implicit | Explicit (patches) |\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "Hierarchical BDH should show:\n",
    "1. **Better long-range coherence** (global model captures cross-patch dependencies)\n",
    "2. **Improved perplexity** on natural language (semantic units align with patches)\n",
    "3. **Similar or better efficiency** (reduced attention complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Clone repo if in Colab\n",
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || echo 'Repo exists'\n",
    "%cd bdh\n",
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import both models\n",
    "from bdh import BDH, BDHConfig\n",
    "from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig, create_hierarchical_bdh\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load WikiText-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading WikiText-2...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Concatenate all text\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "test_text = '\\n'.join(dataset['test']['text'])\n",
    "\n",
    "train_data = text_to_bytes(train_text)\n",
    "val_data = text_to_bytes(val_text)\n",
    "test_data = text_to_bytes(test_text)\n",
    "\n",
    "print(f'Train: {len(train_data):,} bytes ({len(train_data)/1e6:.1f}M)')\n",
    "print(f'Val: {len(val_data):,} bytes')\n",
    "print(f'Test: {len(test_data):,} bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Models\n",
    "\n",
    "We'll create comparable models (similar parameter counts) for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "BLOCK_SIZE = 512  # Must be divisible by patch_size\n",
    "BATCH_SIZE = 32\n",
    "MAX_STEPS = 5000\n",
    "WARMUP_STEPS = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.1\n",
    "VAL_INTERVAL = 100\n",
    "VAL_BATCHES = 50\n",
    "PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BDH (~25M params to match hierarchical tiny/small)\n",
    "original_config = BDHConfig(\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    n_embd=256,\n",
    "    vocab_size=256,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "original_model = BDH(original_config).to(device)\n",
    "original_params = sum(p.numel() for p in original_model.parameters())\n",
    "\n",
    "print('ORIGINAL BDH')\n",
    "print(f'  Layers: {original_config.n_layer}')\n",
    "print(f'  Embed dim: {original_config.n_embd}')\n",
    "print(f'  Heads: {original_config.n_head}')\n",
    "print(f'  Parameters: {original_params:,} ({original_params/1e6:.1f}M)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical BDH (small config - ~30M params to match original)\n",
    "hier_config = HierarchicalBDHConfig.small(\n",
    "    max_seq_len=BLOCK_SIZE,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "hier_model = HierarchicalBDH(hier_config).to(device)\n",
    "hier_params = hier_model.count_parameters()\n",
    "\n",
    "print('\\nHIERARCHICAL BDH')\n",
    "print(f'  Patch size: {hier_config.patch_size}')\n",
    "print(f'  Global: {hier_config.global_n_layer}L x {hier_config.global_n_embd}D x {hier_config.global_n_head}H')\n",
    "print(f'  Local:  {hier_config.local_n_layer}L x {hier_config.local_n_embd}D x {hier_config.local_n_head}H')\n",
    "print(f'  Parameters by component:')\n",
    "for name, count in hier_params.items():\n",
    "    print(f'    {name}: {count:,} ({count/1e6:.1f}M)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare parameter counts\n",
    "print('\\nPARAMETER COMPARISON')\n",
    "print(f'  Original BDH:     {original_params:>12,} ({original_params/1e6:.1f}M)')\n",
    "print(f'  Hierarchical BDH: {hier_params[\"total\"]:>12,} ({hier_params[\"total\"]/1e6:.1f}M)')\n",
    "print(f'  Ratio: {hier_params[\"total\"]/original_params:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = ByteDataset(train_data, BLOCK_SIZE)\n",
    "val_dataset = ByteDataset(val_data, BLOCK_SIZE)\n",
    "test_dataset = ByteDataset(test_data, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f'Train batches: {len(train_loader):,}')\n",
    "print(f'Val batches: {len(val_loader):,}')\n",
    "print(f'Test batches: {len(test_loader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(step, warmup_steps=WARMUP_STEPS, max_steps=MAX_STEPS, lr=LEARNING_RATE):\n",
    "    \"\"\"Cosine LR schedule with warmup.\"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return lr * step / warmup_steps\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    return lr * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    \"\"\"Evaluate model, return loss and perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    bpb = avg_loss / math.log(2)  # Bits per byte\n",
    "    return {'loss': avg_loss, 'ppl': perplexity, 'bpb': bpb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name, max_steps=MAX_STEPS):\n",
    "    \"\"\"Train a model and return history.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {name}')\n",
    "    print('='*60)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.95),\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'step': [], 'train_loss': [], 'val_loss': [],\n",
    "        'val_ppl': [], 'val_bpb': [], 'lr': [],\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    model.train()\n",
    "    train_iter = iter(train_loader)\n",
    "    running_loss = 0\n",
    "    \n",
    "    pbar = tqdm(range(max_steps), desc=name)\n",
    "    \n",
    "    for step in pbar:\n",
    "        # Get batch\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Update LR\n",
    "        lr = get_lr(step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Forward + backward\n",
    "        _, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Logging\n",
    "        if (step + 1) % 50 == 0:\n",
    "            avg_loss = running_loss / 50\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.3f}', 'lr': f'{lr:.2e}'})\n",
    "            running_loss = 0\n",
    "        \n",
    "        # Validation\n",
    "        if (step + 1) % VAL_INTERVAL == 0:\n",
    "            val_metrics = evaluate(model, val_loader, VAL_BATCHES)\n",
    "            train_metrics = evaluate(model, train_loader, VAL_BATCHES)\n",
    "            \n",
    "            history['step'].append(step + 1)\n",
    "            history['train_loss'].append(train_metrics['loss'])\n",
    "            history['val_loss'].append(val_metrics['loss'])\n",
    "            history['val_ppl'].append(val_metrics['ppl'])\n",
    "            history['val_bpb'].append(val_metrics['bpb'])\n",
    "            history['lr'].append(lr)\n",
    "            \n",
    "            gap = val_metrics['loss'] - train_metrics['loss']\n",
    "            print(f'\\nStep {step+1}: train={train_metrics[\"loss\"]:.3f}, val={val_metrics[\"loss\"]:.3f}, ppl={val_metrics[\"ppl\"]:.2f}, bpb={val_metrics[\"bpb\"]:.3f}, gap={gap:.3f}')\n",
    "            \n",
    "            # Best model?\n",
    "            if val_metrics['loss'] < best_val_loss:\n",
    "                best_val_loss = val_metrics['loss']\n",
    "                patience_counter = 0\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                print(f'  \u2713 New best!')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f'  No improvement ({patience_counter}/{PATIENCE})')\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f'\\n\u26a0\ufe0f Early stopping at step {step+1}')\n",
    "                break\n",
    "    \n",
    "    # Restore best\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return history, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Both Models\n",
    "\n",
    "**Note**: This will take a while. For quick testing, reduce `MAX_STEPS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Original BDH\n",
    "original_history, original_best = train_model(original_model, 'Original BDH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Hierarchical BDH\n",
    "hier_history, hier_best = train_model(hier_model, 'Hierarchical BDH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print('\\n' + '='*60)\n",
    "print('FINAL TEST SET EVALUATION')\n",
    "print('='*60)\n",
    "\n",
    "original_test = evaluate(original_model, test_loader)\n",
    "hier_test = evaluate(hier_model, test_loader)\n",
    "\n",
    "print(f'\\nOriginal BDH:')\n",
    "print(f'  Loss: {original_test[\"loss\"]:.4f}')\n",
    "print(f'  Perplexity: {original_test[\"ppl\"]:.2f}')\n",
    "print(f'  Bits/Byte: {original_test[\"bpb\"]:.3f}')\n",
    "\n",
    "print(f'\\nHierarchical BDH:')\n",
    "print(f'  Loss: {hier_test[\"loss\"]:.4f}')\n",
    "print(f'  Perplexity: {hier_test[\"ppl\"]:.2f}')\n",
    "print(f'  Bits/Byte: {hier_test[\"bpb\"]:.3f}')\n",
    "\n",
    "print(f'\\nImprovement:')\n",
    "ppl_improvement = (original_test['ppl'] - hier_test['ppl']) / original_test['ppl'] * 100\n",
    "bpb_improvement = (original_test['bpb'] - hier_test['bpb']) / original_test['bpb'] * 100\n",
    "print(f'  Perplexity: {ppl_improvement:+.1f}%')\n",
    "print(f'  Bits/Byte: {bpb_improvement:+.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0, 0]\n",
    "ax.plot(original_history['step'], original_history['val_loss'], 'b-', label='Original BDH', linewidth=2)\n",
    "ax.plot(hier_history['step'], hier_history['val_loss'], 'r-', label='Hierarchical BDH', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Validation Loss')\n",
    "ax.set_title('Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "ax = axes[0, 1]\n",
    "ax.plot(original_history['step'], original_history['val_ppl'], 'b-', label='Original BDH', linewidth=2)\n",
    "ax.plot(hier_history['step'], hier_history['val_ppl'], 'r-', label='Hierarchical BDH', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Perplexity')\n",
    "ax.set_title('Validation Perplexity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bits per byte\n",
    "ax = axes[1, 0]\n",
    "ax.plot(original_history['step'], original_history['val_bpb'], 'b-', label='Original BDH', linewidth=2)\n",
    "ax.plot(hier_history['step'], hier_history['val_bpb'], 'r-', label='Hierarchical BDH', linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Bits per Byte')\n",
    "ax.set_title('Validation BPB')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "ax = axes[1, 1]\n",
    "original_gap = [v - t for v, t in zip(original_history['val_loss'], original_history['train_loss'])]\n",
    "hier_gap = [v - t for v, t in zip(hier_history['val_loss'], hier_history['train_loss'])]\n",
    "ax.plot(original_history['step'], original_gap, 'b-', label='Original BDH', linewidth=2)\n",
    "ax.plot(hier_history['step'], hier_gap, 'r-', label='Hierarchical BDH', linewidth=2)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Val - Train Loss')\n",
    "ax.set_title('Overfitting Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hierarchical_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generation Comparison\n",
    "\n",
    "Compare text generation quality between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_tokens=200, temperature=0.8):\n",
    "    \"\"\"Generate text from prompt.\"\"\"\n",
    "    model.eval()\n",
    "    prompt_bytes = torch.tensor([list(prompt.encode('utf-8'))], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(prompt_bytes, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    \n",
    "    # Decode, handling invalid UTF-8\n",
    "    output_bytes = bytes(output[0].tolist())\n",
    "    return output_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    'The history of',\n",
    "    'In the year 2024,',\n",
    "    'Scientists discovered that',\n",
    "    'The quick brown fox',\n",
    "]\n",
    "\n",
    "print('='*70)\n",
    "print('GENERATION COMPARISON')\n",
    "print('='*70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\n--- Prompt: \"{prompt}\" ---')\n",
    "    \n",
    "    print('\\nOriginal BDH:')\n",
    "    print(generate_text(original_model, prompt))\n",
    "    \n",
    "    print('\\nHierarchical BDH:')\n",
    "    print(generate_text(hier_model, prompt))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'original': {\n",
    "        'config': original_config.__dict__,\n",
    "        'params': original_params,\n",
    "        'test_loss': original_test['loss'],\n",
    "        'test_ppl': original_test['ppl'],\n",
    "        'test_bpb': original_test['bpb'],\n",
    "        'history': original_history,\n",
    "    },\n",
    "    'hierarchical': {\n",
    "        'config': {k: v for k, v in hier_config.__dict__.items()},\n",
    "        'params': hier_params,\n",
    "        'test_loss': hier_test['loss'],\n",
    "        'test_ppl': hier_test['ppl'],\n",
    "        'test_bpb': hier_test['bpb'],\n",
    "        'history': hier_history,\n",
    "    },\n",
    "}\n",
    "\n",
    "results_dir = Path('results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_dir / 'hierarchical_comparison.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f'Results saved to {results_dir}/hierarchical_comparison.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoints\n",
    "ckpt_dir = Path('checkpoints')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': original_model.state_dict(),\n",
    "    'config': original_config.__dict__,\n",
    "    'test_metrics': original_test,\n",
    "}, ckpt_dir / 'original_bdh_wikitext2.pt')\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': hier_model.state_dict(),\n",
    "    'config': hier_config.__dict__,\n",
    "    'test_metrics': hier_test,\n",
    "}, ckpt_dir / 'hierarchical_bdh_wikitext2.pt')\n",
    "\n",
    "print(f'Checkpoints saved to {ckpt_dir}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Metric | Original BDH | Hierarchical BDH | Winner |\n",
    "|--------|-------------|------------------|--------|\n",
    "| Parameters | - | - | - |\n",
    "| Test Loss | - | - | - |\n",
    "| Test Perplexity | - | - | - |\n",
    "| Test BPB | - | - | - |\n",
    "\n",
    "**Observations:**\n",
    "- (Fill in after running)\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different patch sizes (4, 8, 16)\n",
    "- Test on code datasets (curriculum)\n",
    "- Scale up model size\n",
    "- Compare generation coherence qualitatively"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
