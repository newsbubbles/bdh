{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Training BDH for Tool Calling\n",
    "\n",
    "Train a specialized BDH model for function/tool calling that can be composed with other models.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Create a model that can:\n",
    "1. Recognize when a tool call is needed\n",
    "2. Generate valid JSON function calls\n",
    "3. Parse tool definitions and match to queries\n",
    "\n",
    "## Target Format (pydantic-ai compatible)\n",
    "\n",
    "```\n",
    "User: What's the weather in Tokyo?\n",
    "Tools: [{\"name\": \"get_weather\", \"params\": {\"location\": \"string\"}}]\n",
    "Assistant: {\"name\": \"get_weather\", \"arguments\": {\"location\": \"Tokyo\"}}\n",
    "```\n",
    "\n",
    "## Hardware\n",
    "- A100 80GB recommended for Large config\n",
    "- ~40GB VRAM expected usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install torch datasets transformers tqdm matplotlib -q\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone BDH repo if needed\n",
    "if not os.path.exists('hierarchical_bdh.py'):\n",
    "    !git clone https://github.com/newsbubbles/bdh.git temp_bdh\n",
    "    !cp temp_bdh/bdh.py .\n",
    "    !cp temp_bdh/hierarchical_bdh.py .\n",
    "    !rm -rf temp_bdh\n",
    "\n",
    "from hierarchical_bdh import HierarchicalBDH, HierarchicalBDHConfig\n",
    "print('BDH loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load Tool-Calling Datasets\n",
    "\n",
    "We'll combine multiple high-quality function-calling datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load Glaive function calling dataset (high quality)\n",
    "print('Loading Glaive function-calling dataset...')\n",
    "try:\n",
    "    glaive = load_dataset('glaiveai/glaive-function-calling-v2', split='train')\n",
    "    print(f'Glaive: {len(glaive)} examples')\n",
    "except Exception as e:\n",
    "    print(f'Could not load Glaive: {e}')\n",
    "    glaive = None\n",
    "\n",
    "# Load Gorilla OpenFunctions\n",
    "print('Loading Gorilla OpenFunctions...')\n",
    "try:\n",
    "    gorilla = load_dataset('gorilla-llm/Berkeley-Function-Calling-Leaderboard', split='train')\n",
    "    print(f'Gorilla: {len(gorilla)} examples')\n",
    "except Exception as e:\n",
    "    print(f'Could not load Gorilla: {e}')\n",
    "    gorilla = None\n",
    "\n",
    "# Load NousResearch Hermes function calling\n",
    "print('Loading Hermes function-calling...')\n",
    "try:\n",
    "    hermes = load_dataset('NousResearch/hermes-function-calling-v1', split='train')\n",
    "    print(f'Hermes: {len(hermes)} examples')\n",
    "except Exception as e:\n",
    "    print(f'Could not load Hermes: {e}')\n",
    "    hermes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine dataset formats\n",
    "print('='*60)\n",
    "print('DATASET FORMAT INSPECTION')\n",
    "print('='*60)\n",
    "\n",
    "if glaive:\n",
    "    print('\\nGlaive sample:')\n",
    "    print(glaive[0])\n",
    "\n",
    "if gorilla:\n",
    "    print('\\nGorilla sample:')\n",
    "    print(gorilla[0])\n",
    "\n",
    "if hermes:\n",
    "    print('\\nHermes sample:')\n",
    "    print(hermes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize format for training\n",
    "# Target format:\n",
    "# <|system|>You are a helpful assistant with access to tools.\n",
    "# <|tools|>[{\"name\": \"func\", \"description\": \"...\", \"parameters\": {...}}]\n",
    "# <|user|>User query here\n",
    "# <|assistant|>{\"name\": \"func\", \"arguments\": {...}}\n",
    "\n",
    "def format_glaive_example(example):\n",
    "    \"\"\"Convert Glaive format to our standard format.\"\"\"\n",
    "    try:\n",
    "        # Glaive has 'system', 'chat' fields\n",
    "        system = example.get('system', '')\n",
    "        chat = example.get('chat', '')\n",
    "        \n",
    "        # Extract tools from system prompt\n",
    "        tools_str = ''\n",
    "        if 'functions' in system.lower():\n",
    "            # Try to extract JSON\n",
    "            import re\n",
    "            match = re.search(r'\\[.*?\\]', system, re.DOTALL)\n",
    "            if match:\n",
    "                tools_str = match.group(0)\n",
    "        \n",
    "        formatted = f\"<|system|>You are a helpful assistant with tool access.\\n\"\n",
    "        if tools_str:\n",
    "            formatted += f\"<|tools|>{tools_str}\\n\"\n",
    "        formatted += chat\n",
    "        \n",
    "        return formatted\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def format_hermes_example(example):\n",
    "    \"\"\"Convert Hermes format to our standard format.\"\"\"\n",
    "    try:\n",
    "        conversations = example.get('conversations', [])\n",
    "        tools = example.get('tools', [])\n",
    "        \n",
    "        formatted = \"<|system|>You are a helpful assistant with tool access.\\n\"\n",
    "        if tools:\n",
    "            formatted += f\"<|tools|>{json.dumps(tools)}\\n\"\n",
    "        \n",
    "        for conv in conversations:\n",
    "            role = conv.get('from', conv.get('role', ''))\n",
    "            content = conv.get('value', conv.get('content', ''))\n",
    "            \n",
    "            if role in ['human', 'user']:\n",
    "                formatted += f\"<|user|>{content}\\n\"\n",
    "            elif role in ['gpt', 'assistant']:\n",
    "                formatted += f\"<|assistant|>{content}\\n\"\n",
    "        \n",
    "        return formatted\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and combine datasets\n",
    "all_examples = []\n",
    "\n",
    "if glaive:\n",
    "    print('Processing Glaive...')\n",
    "    for ex in tqdm(glaive):\n",
    "        formatted = format_glaive_example(ex)\n",
    "        if formatted and len(formatted) > 100:\n",
    "            all_examples.append(formatted)\n",
    "    print(f'Added {len(all_examples)} from Glaive')\n",
    "\n",
    "prev_count = len(all_examples)\n",
    "if hermes:\n",
    "    print('Processing Hermes...')\n",
    "    for ex in tqdm(hermes):\n",
    "        formatted = format_hermes_example(ex)\n",
    "        if formatted and len(formatted) > 100:\n",
    "            all_examples.append(formatted)\n",
    "    print(f'Added {len(all_examples) - prev_count} from Hermes')\n",
    "\n",
    "print(f'\\nTotal examples: {len(all_examples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If datasets didn't load, create synthetic examples\n",
    "if len(all_examples) < 1000:\n",
    "    print('Creating synthetic tool-calling examples...')\n",
    "    \n",
    "    # Define some tools\n",
    "    tools = [\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"search_web\",\n",
    "            \"description\": \"Search the web for information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "                    \"num_results\": {\"type\": \"integer\", \"default\": 5}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"send_email\",\n",
    "            \"description\": \"Send an email to a recipient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"to\": {\"type\": \"string\", \"description\": \"Recipient email\"},\n",
    "                    \"subject\": {\"type\": \"string\", \"description\": \"Email subject\"},\n",
    "                    \"body\": {\"type\": \"string\", \"description\": \"Email body\"}\n",
    "                },\n",
    "                \"required\": [\"to\", \"subject\", \"body\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Perform a mathematical calculation\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\"type\": \"string\", \"description\": \"Math expression\"}\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"get_stock_price\",\n",
    "            \"description\": \"Get current stock price\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"symbol\": {\"type\": \"string\", \"description\": \"Stock ticker symbol\"}\n",
    "                },\n",
    "                \"required\": [\"symbol\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"create_reminder\",\n",
    "            \"description\": \"Create a reminder\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"message\": {\"type\": \"string\", \"description\": \"Reminder message\"},\n",
    "                    \"time\": {\"type\": \"string\", \"description\": \"When to remind\"}\n",
    "                },\n",
    "                \"required\": [\"message\", \"time\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"translate\",\n",
    "            \"description\": \"Translate text between languages\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"string\", \"description\": \"Text to translate\"},\n",
    "                    \"source_lang\": {\"type\": \"string\", \"description\": \"Source language\"},\n",
    "                    \"target_lang\": {\"type\": \"string\", \"description\": \"Target language\"}\n",
    "                },\n",
    "                \"required\": [\"text\", \"target_lang\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"read_file\",\n",
    "            \"description\": \"Read contents of a file\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"path\": {\"type\": \"string\", \"description\": \"File path\"}\n",
    "                },\n",
    "                \"required\": [\"path\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"write_file\",\n",
    "            \"description\": \"Write content to a file\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"path\": {\"type\": \"string\", \"description\": \"File path\"},\n",
    "                    \"content\": {\"type\": \"string\", \"description\": \"Content to write\"}\n",
    "                },\n",
    "                \"required\": [\"path\", \"content\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"run_python\",\n",
    "            \"description\": \"Execute Python code\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\"type\": \"string\", \"description\": \"Python code to execute\"}\n",
    "                },\n",
    "                \"required\": [\"code\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Example queries and responses for each tool\n",
    "    examples_per_tool = {\n",
    "        \"get_weather\": [\n",
    "            (\"What's the weather in Tokyo?\", {\"location\": \"Tokyo\"}),\n",
    "            (\"How's the weather in New York today?\", {\"location\": \"New York\"}),\n",
    "            (\"Is it cold in London?\", {\"location\": \"London\"}),\n",
    "            (\"Weather forecast for Paris please\", {\"location\": \"Paris\"}),\n",
    "            (\"What's the temperature in celsius in Berlin?\", {\"location\": \"Berlin\", \"unit\": \"celsius\"}),\n",
    "            (\"Give me weather in fahrenheit for Miami\", {\"location\": \"Miami\", \"unit\": \"fahrenheit\"}),\n",
    "        ],\n",
    "        \"search_web\": [\n",
    "            (\"Search for Python tutorials\", {\"query\": \"Python tutorials\"}),\n",
    "            (\"Find information about machine learning\", {\"query\": \"machine learning\"}),\n",
    "            (\"Look up the latest news on AI\", {\"query\": \"latest AI news\"}),\n",
    "            (\"Search for best restaurants nearby\", {\"query\": \"best restaurants nearby\"}),\n",
    "            (\"Find 10 results about climate change\", {\"query\": \"climate change\", \"num_results\": 10}),\n",
    "        ],\n",
    "        \"send_email\": [\n",
    "            (\"Send an email to john@example.com about the meeting tomorrow\", {\"to\": \"john@example.com\", \"subject\": \"Meeting Tomorrow\", \"body\": \"Hi John, just a reminder about our meeting tomorrow.\"}),\n",
    "            (\"Email sarah@company.com the project update\", {\"to\": \"sarah@company.com\", \"subject\": \"Project Update\", \"body\": \"Here is the latest project update.\"}),\n",
    "        ],\n",
    "        \"calculate\": [\n",
    "            (\"What's 25 times 47?\", {\"expression\": \"25 * 47\"}),\n",
    "            (\"Calculate 15% of 200\", {\"expression\": \"0.15 * 200\"}),\n",
    "            (\"What's the square root of 144?\", {\"expression\": \"sqrt(144)\"}),\n",
    "            (\"Compute 2 to the power of 10\", {\"expression\": \"2 ** 10\"}),\n",
    "        ],\n",
    "        \"get_stock_price\": [\n",
    "            (\"What's Apple's stock price?\", {\"symbol\": \"AAPL\"}),\n",
    "            (\"Get me the price of TSLA\", {\"symbol\": \"TSLA\"}),\n",
    "            (\"How is Microsoft stock doing?\", {\"symbol\": \"MSFT\"}),\n",
    "            (\"Check NVDA stock\", {\"symbol\": \"NVDA\"}),\n",
    "        ],\n",
    "        \"create_reminder\": [\n",
    "            (\"Remind me to call mom at 5pm\", {\"message\": \"Call mom\", \"time\": \"5pm\"}),\n",
    "            (\"Set a reminder for the meeting in 2 hours\", {\"message\": \"Meeting\", \"time\": \"in 2 hours\"}),\n",
    "            (\"Remind me to take medication tomorrow morning\", {\"message\": \"Take medication\", \"time\": \"tomorrow morning\"}),\n",
    "        ],\n",
    "        \"translate\": [\n",
    "            (\"Translate 'Hello world' to Spanish\", {\"text\": \"Hello world\", \"target_lang\": \"Spanish\"}),\n",
    "            (\"How do you say 'thank you' in French?\", {\"text\": \"thank you\", \"target_lang\": \"French\"}),\n",
    "            (\"Translate this to German: Good morning\", {\"text\": \"Good morning\", \"target_lang\": \"German\"}),\n",
    "        ],\n",
    "        \"read_file\": [\n",
    "            (\"Read the contents of config.json\", {\"path\": \"config.json\"}),\n",
    "            (\"Show me what's in /home/user/notes.txt\", {\"path\": \"/home/user/notes.txt\"}),\n",
    "            (\"Open and read data.csv\", {\"path\": \"data.csv\"}),\n",
    "        ],\n",
    "        \"write_file\": [\n",
    "            (\"Write 'Hello World' to output.txt\", {\"path\": \"output.txt\", \"content\": \"Hello World\"}),\n",
    "            (\"Save this note to notes.md: Remember to review PR\", {\"path\": \"notes.md\", \"content\": \"Remember to review PR\"}),\n",
    "        ],\n",
    "        \"run_python\": [\n",
    "            (\"Run this Python code: print('Hello')\", {\"code\": \"print('Hello')\"}),\n",
    "            (\"Execute: for i in range(5): print(i)\", {\"code\": \"for i in range(5): print(i)\"}),\n",
    "            (\"Run a Python script to list files\", {\"code\": \"import os; print(os.listdir('.'))\"}),\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Generate training examples\n",
    "    import random\n",
    "    \n",
    "    for _ in range(10000):  # Generate 10K examples\n",
    "        # Pick 1-4 random tools to make available\n",
    "        num_tools = random.randint(1, 4)\n",
    "        available_tools = random.sample(tools, num_tools)\n",
    "        \n",
    "        # Pick one tool to actually use\n",
    "        tool = random.choice(available_tools)\n",
    "        tool_name = tool['name']\n",
    "        \n",
    "        if tool_name in examples_per_tool:\n",
    "            query, args = random.choice(examples_per_tool[tool_name])\n",
    "            \n",
    "            # Format the example\n",
    "            example = f\"<|system|>You are a helpful assistant with tool access.\\n\"\n",
    "            example += f\"<|tools|>{json.dumps(available_tools)}\\n\"\n",
    "            example += f\"<|user|>{query}\\n\"\n",
    "            example += f\"<|assistant|>{json.dumps({'name': tool_name, 'arguments': args})}\\n\"\n",
    "            \n",
    "            all_examples.append(example)\n",
    "    \n",
    "    print(f'Generated {len(all_examples)} synthetic examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "print('Sample training examples:')\n",
    "print('='*60)\n",
    "for i in range(3):\n",
    "    print(all_examples[i][:500])\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallingDataset(Dataset):\n",
    "    def __init__(self, examples, block_size=1024):\n",
    "        self.examples = examples\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Concatenate all examples into one big byte sequence\n",
    "        self.data = torch.tensor(\n",
    "            [b for ex in examples for b in ex.encode('utf-8')],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        print(f'Dataset size: {len(self.data):,} bytes')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(1, len(self.data) - self.block_size - 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset\n",
    "block_size = 1024  # Longer context for tool definitions\n",
    "dataset = ToolCallingDataset(all_examples, block_size=block_size)\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Configure Large Model\n",
    "\n",
    "Using Large config for better tool-calling capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large config - should fit in ~40GB VRAM on A100\n",
    "config = HierarchicalBDHConfig(\n",
    "    vocab_size=256,\n",
    "    n_embd=512,\n",
    "    local_n_layer=6,\n",
    "    global_n_layer=6,\n",
    "    n_head=8,\n",
    "    patch_size=32,  # Larger patches for longer context\n",
    "    mlp_internal_dim_multiplier=128,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = HierarchicalBDH(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model parameters: {n_params:,}')\n",
    "print(f'Estimated VRAM: ~{n_params * 4 / 1e9 * 3:.1f} GB')  # params + grads + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "batch_size = 32\n",
    "learning_rate = 3e-4\n",
    "warmup_steps = 500\n",
    "max_steps = 10000\n",
    "eval_interval = 500\n",
    "save_interval = 1000\n",
    "gradient_accumulation_steps = 2  # Effective batch = 64\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return learning_rate * step / warmup_steps\n",
    "    # Cosine decay\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    return learning_rate * 0.1 + 0.5 * (learning_rate - learning_rate * 0.1) * (1 + np.cos(np.pi * decay_ratio))\n",
    "\n",
    "print('Training config ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Training state\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Checkpoint directory\n",
    "os.makedirs('checkpoints_tool_calling', exist_ok=True)\n",
    "\n",
    "print('Starting training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for x, y in val_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Ensure divisible by patch_size\n",
    "        valid_len = (x.size(1) // config.patch_size) * config.patch_size\n",
    "        if valid_len < config.patch_size:\n",
    "            continue\n",
    "        x = x[:, :valid_len]\n",
    "        y = y[:, :valid_len]\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    model.train()\n",
    "    return np.mean(losses) if losses else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "model.train()\n",
    "step = 0\n",
    "accum_loss = 0\n",
    "\n",
    "pbar = tqdm(total=max_steps, desc='Training')\n",
    "\n",
    "while step < max_steps:\n",
    "    for x, y in train_loader:\n",
    "        if step >= max_steps:\n",
    "            break\n",
    "            \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Ensure divisible by patch_size\n",
    "        valid_len = (x.size(1) // config.patch_size) * config.patch_size\n",
    "        if valid_len < config.patch_size:\n",
    "            continue\n",
    "        x = x[:, :valid_len]\n",
    "        y = y[:, :valid_len]\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr = get_lr(step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        accum_loss += loss.item()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_losses.append(accum_loss)\n",
    "            accum_loss = 0\n",
    "        \n",
    "        # Evaluation\n",
    "        if step % eval_interval == 0 and step > 0:\n",
    "            val_loss = evaluate()\n",
    "            val_losses.append(val_loss)\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': f'{np.mean(train_losses[-100:]):.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'lr': f'{lr:.2e}'\n",
    "            })\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'config': config.__dict__,\n",
    "                    'val_loss': val_loss,\n",
    "                }, 'checkpoints_tool_calling/best.pt')\n",
    "                print(f'\\nNew best model saved! Val loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Periodic save\n",
    "        if step % save_interval == 0 and step > 0:\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'config': config.__dict__,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "            }, f'checkpoints_tool_calling/step_{step}.pt')\n",
    "        \n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, alpha=0.3)\n",
    "ax.plot(np.convolve(train_losses, np.ones(100)/100, mode='valid'), label='Train (smoothed)')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(np.arange(0, len(val_losses)) * eval_interval, val_losses, 'o-')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tool_calling_training.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tool_call(model, tools, user_query, max_new=200, temperature=0.7):\n",
    "    \"\"\"Generate a tool call given tools and user query.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = f\"<|system|>You are a helpful assistant with tool access.\\n\"\n",
    "    prompt += f\"<|tools|>{json.dumps(tools)}\\n\"\n",
    "    prompt += f\"<|user|>{user_query}\\n\"\n",
    "    prompt += \"<|assistant|>\"\n",
    "    \n",
    "    # Tokenize (bytes)\n",
    "    tokens = torch.tensor([[b for b in prompt.encode('utf-8')]], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Ensure divisible by patch_size\n",
    "    pad_len = (config.patch_size - tokens.size(1) % config.patch_size) % config.patch_size\n",
    "    if pad_len > 0:\n",
    "        tokens = F.pad(tokens, (0, pad_len), value=0)\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new):\n",
    "            # Get prediction\n",
    "            logits = model(tokens[:, -1024:])  # Use last 1024 tokens\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "            generated.append(next_token.item())\n",
    "            \n",
    "            # Stop at newline (end of tool call)\n",
    "            if next_token.item() == ord('\\n'):\n",
    "                break\n",
    "    \n",
    "    # Decode\n",
    "    response = bytes(generated).decode('utf-8', errors='replace').strip()\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        tool_call = json.loads(response)\n",
    "        return tool_call, True\n",
    "    except:\n",
    "        return response, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('checkpoints_tool_calling/best.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model (val_loss: {checkpoint['val_loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tool calling\n",
    "test_tools = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\"},\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_web\",\n",
    "        \"description\": \"Search the web\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "test_queries = [\n",
    "    \"What's the weather in San Francisco?\",\n",
    "    \"Search for the best pizza places\",\n",
    "    \"How's the temperature in London in celsius?\",\n",
    "    \"Find information about machine learning\",\n",
    "]\n",
    "\n",
    "print('='*60)\n",
    "print('TOOL CALLING TEST')\n",
    "print('='*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    result, is_valid = generate_tool_call(model, test_tools, query)\n",
    "    status = '‚úì' if is_valid else '‚úó'\n",
    "    print(f'\\nQuery: {query}')\n",
    "    print(f'Response {status}: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Model Composition Test\n",
    "\n",
    "Load a language model and compose with the tool-calling model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained language model (if available)\n",
    "language_model_path = 'checkpoints_hierarchical_small/best.pt'\n",
    "\n",
    "if os.path.exists(language_model_path):\n",
    "    print('Loading language model for composition...')\n",
    "    lang_checkpoint = torch.load(language_model_path, map_location=device)\n",
    "    lang_config = HierarchicalBDHConfig(**lang_checkpoint.get('config', {}))\n",
    "    lang_model = HierarchicalBDH(lang_config).to(device)\n",
    "    lang_model.load_state_dict(lang_checkpoint['model_state_dict'])\n",
    "    print('Language model loaded!')\n",
    "else:\n",
    "    print(f'No language model found at {language_model_path}')\n",
    "    print('Train one first using train_hierarchical_wikitext2.ipynb')\n",
    "    lang_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composed model that can do both language and tool calling\n",
    "if lang_model is not None:\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    class ComposedToolAgent(nn.Module):\n",
    "        \"\"\"\n",
    "        Compose language model with tool-calling model.\n",
    "        \n",
    "        Uses a simple routing mechanism:\n",
    "        - If tools are present in prompt, weight tool model higher\n",
    "        - Otherwise, use language model\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, lang_model, tool_model):\n",
    "            super().__init__()\n",
    "            self.lang_model = lang_model\n",
    "            self.tool_model = tool_model\n",
    "            \n",
    "            # Freeze both models\n",
    "            for p in self.lang_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            for p in self.tool_model.parameters():\n",
    "                p.requires_grad = False\n",
    "            \n",
    "            # Learnable routing weights\n",
    "            self.tool_weight = nn.Parameter(torch.tensor(0.5))\n",
    "            \n",
    "        def forward(self, x, has_tools=False):\n",
    "            # Get logits from both models\n",
    "            lang_logits = self.lang_model(x)\n",
    "            tool_logits = self.tool_model(x)\n",
    "            \n",
    "            # Route based on context\n",
    "            if has_tools:\n",
    "                w = torch.sigmoid(self.tool_weight + 1)  # Bias toward tool model\n",
    "            else:\n",
    "                w = torch.sigmoid(self.tool_weight - 1)  # Bias toward lang model\n",
    "            \n",
    "            return w * tool_logits + (1 - w) * lang_logits\n",
    "    \n",
    "    composed_agent = ComposedToolAgent(lang_model, model).to(device)\n",
    "    print('Composed agent created!')\n",
    "    print(f'Total parameters: {sum(p.numel() for p in composed_agent.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test composed agent\n",
    "if lang_model is not None:\n",
    "    print('='*60)\n",
    "    print('COMPOSED AGENT TEST')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Test language capability\n",
    "    print('\\n--- Language Test ---')\n",
    "    prompt = \"The quick brown fox\"\n",
    "    tokens = torch.tensor([[b for b in prompt.encode('utf-8')]], device=device)\n",
    "    \n",
    "    # Pad\n",
    "    pad_len = (32 - tokens.size(1) % 32) % 32\n",
    "    tokens = F.pad(tokens, (0, pad_len), value=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate with composed model\n",
    "        for _ in range(50):\n",
    "            logits = composed_agent(tokens, has_tools=False)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "    output = bytes(tokens[0].cpu().tolist()).decode('utf-8', errors='replace')\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Output: {output}')\n",
    "    \n",
    "    # Test tool calling capability\n",
    "    print('\\n--- Tool Calling Test ---')\n",
    "    result, is_valid = generate_tool_call(model, test_tools, \"What's the weather in Tokyo?\")\n",
    "    print(f'Tool call result: {result} (valid: {is_valid})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook trains a specialized tool-calling BDH model that can:\n",
    "\n",
    "1. Parse tool definitions in JSON format\n",
    "2. Match user queries to appropriate tools\n",
    "3. Generate valid JSON function calls\n",
    "\n",
    "The model can be composed with a language model for a full agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for composition\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config.__dict__,\n",
    "    'model_type': 'tool_calling',\n",
    "}, 'checkpoints_tool_calling/final_tool_model.pt')\n",
    "\n",
    "print('Model saved for composition!')\n",
    "print('\\nTo compose with another model:')\n",
    "print('1. Load this model')\n",
    "print('2. Load a language model')\n",
    "print('3. Use ComposedToolAgent class')\n",
    "print('4. Fine-tune the routing weights on mixed data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}