{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical BDH Training on Code (The Stack - Python)\n",
    "\n",
    "Train **Hierarchical BDH** on Python code from The Stack dataset.\n",
    "\n",
    "## Why Code?\n",
    "- **Highly structured**: Clear syntax, indentation, brackets\n",
    "- **Long-range dependencies**: Functions reference each other, imports at top\n",
    "- **Byte-level advantage**: Handles any syntax without tokenization issues\n",
    "- **Practical application**: Code completion, understanding\n",
    "\n",
    "## Expected Outcomes\n",
    "- Lower perplexity than natural language (more predictable syntax)\n",
    "- Tests hierarchical on structured data\n",
    "- Foundation for code fine-tuning experiments\n",
    "\n",
    "## Hardware Requirements\n",
    "- **Recommended**: A100 40GB+\n",
    "- **Minimum**: V100 32GB with small model\n",
    "- Training time: 2-4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || (cd bdh && git pull)\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Stack (Python subset)\n",
    "\n",
    "The Stack is a massive code dataset. We'll use a Python subset.\n",
    "\n",
    "**Note**: The Stack requires accepting terms at huggingface.co/datasets/bigcode/the-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# =============================================================\n",
    "# DATASET CONFIG\n",
    "# =============================================================\n",
    "# Option 1: The Stack (requires HF login and terms acceptance)\n",
    "# Option 2: CodeParrot (public, no login needed)\n",
    "USE_THE_STACK = False  # Set True if you have access\n",
    "SAMPLE_SIZE = 50000  # Number of files to sample\n",
    "# =============================================================\n",
    "\n",
    "print('Loading code dataset...')\n",
    "\n",
    "if USE_THE_STACK:\n",
    "    # The Stack - requires HF login\n",
    "    print('Loading The Stack (Python)...')\n",
    "    dataset = load_dataset(\n",
    "        'bigcode/the-stack',\n",
    "        data_dir='data/python',\n",
    "        split=f'train[:{SAMPLE_SIZE}]',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    text_column = 'content'\n",
    "else:\n",
    "    # CodeParrot - public dataset\n",
    "    print('Loading CodeParrot (Python)...')\n",
    "    dataset = load_dataset(\n",
    "        'codeparrot/codeparrot-clean',\n",
    "        split=f'train[:{SAMPLE_SIZE}]'\n",
    "    )\n",
    "    text_column = 'content'\n",
    "\n",
    "print(f'Loaded {len(dataset)} code files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Concatenate code files with clear separators\n",
    "separator = '\\n\\n# === END OF FILE ===\\n\\n'\n",
    "\n",
    "print('Processing code files...')\n",
    "all_code = separator.join(dataset[text_column])\n",
    "\n",
    "# Split into train/val/test (90/5/5)\n",
    "total_len = len(all_code)\n",
    "train_end = int(0.9 * total_len)\n",
    "val_end = int(0.95 * total_len)\n",
    "\n",
    "train_text = all_code[:train_end]\n",
    "val_text = all_code[train_end:val_end]\n",
    "test_text = all_code[val_end:]\n",
    "\n",
    "print('Converting to bytes...')\n",
    "train_data = text_to_bytes(train_text)\n",
    "val_data = text_to_bytes(val_text)\n",
    "test_data = text_to_bytes(test_text)\n",
    "\n",
    "# Free memory\n",
    "del all_code, train_text, val_text, test_text, dataset\n",
    "import gc; gc.collect()\n",
    "\n",
    "print(f'\\nDataset sizes:')\n",
    "print(f'  Train: {len(train_data):,} bytes ({len(train_data)/1e6:.1f}MB)')\n",
    "print(f'  Val:   {len(val_data):,} bytes ({len(val_data)/1e6:.1f}MB)')\n",
    "print(f'  Test:  {len(test_data):,} bytes ({len(test_data)/1e6:.1f}MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size, patch_size=8):\n",
    "        assert block_size % patch_size == 0\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        return chunk[:-1], chunk[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "MODEL_SIZE = 'base'  # Options: 'tiny', 'small', 'base', 'large'\n",
    "# =============================================================\n",
    "\n",
    "preset_map = {\n",
    "    'tiny': HierarchicalBDHConfig.tiny,\n",
    "    'small': HierarchicalBDHConfig.small,\n",
    "    'base': HierarchicalBDHConfig.base,\n",
    "    'large': HierarchicalBDHConfig.large,\n",
    "}\n",
    "model_config = preset_map[MODEL_SIZE](dropout=0.1)\n",
    "\n",
    "config = {\n",
    "    'model_size': MODEL_SIZE,\n",
    "    'patch_size': model_config.patch_size,\n",
    "    'block_size': 1024,  # Good for function-level context\n",
    "    \n",
    "    'batch_size': {'tiny': 64, 'small': 32, 'base': 16, 'large': 8}[MODEL_SIZE],\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'max_steps': 15000,\n",
    "    'warmup_steps': 400,\n",
    "    \n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'val_interval': 500,\n",
    "    'val_batches': 100,\n",
    "    'patience': 10,\n",
    "    'log_interval': 100,\n",
    "}\n",
    "\n",
    "print(f'Model: {MODEL_SIZE}')\n",
    "print(f'Block size: {config[\"block_size\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalBDH(model_config).to(device)\n",
    "params = model.count_parameters()\n",
    "\n",
    "print(f'\\nHierarchical BDH - {MODEL_SIZE.upper()}')\n",
    "print(f'Global: {model_config.global_n_layer}L x {model_config.global_n_embd}D')\n",
    "print(f'Local:  {model_config.local_n_layer}L x {model_config.local_n_embd}D')\n",
    "print(f'Total:  {params[\"total\"]:,} ({params[\"total\"]/1e6:.1f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ByteDataset(train_data, config['block_size'], config['patch_size'])\n",
    "val_dataset = ByteDataset(val_data, config['block_size'], config['patch_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'], betas=(0.9, 0.95))\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < config['warmup_steps']:\n",
    "        return config['learning_rate'] * step / config['warmup_steps']\n",
    "    progress = (step - config['warmup_steps']) / (config['max_steps'] - config['warmup_steps'])\n",
    "    return config['learning_rate'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(f'Train batches: {len(train_loader):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches: break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    return avg_loss, math.exp(avg_loss), avg_loss / math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'step': [], 'train_loss': [], 'val_loss': [], 'val_ppl': [], 'val_bpb': [], 'lr': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "ckpt_dir = Path(f'checkpoints_hierarchical_{MODEL_SIZE}_code')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f'Checkpoints: {ckpt_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "running_loss = 0\n",
    "\n",
    "pbar = tqdm(range(config['max_steps']), desc='Training')\n",
    "\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    accum_loss = 0\n",
    "    \n",
    "    for _ in range(config['gradient_accumulation_steps']):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / config['gradient_accumulation_steps']\n",
    "        scaler.scale(loss).backward()\n",
    "        accum_loss += loss.item()\n",
    "    \n",
    "    lr = get_lr(step)\n",
    "    for pg in optimizer.param_groups: pg['lr'] = lr\n",
    "    \n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    running_loss += accum_loss\n",
    "    \n",
    "    if (step + 1) % config['log_interval'] == 0:\n",
    "        pbar.set_postfix({'loss': f'{running_loss/config[\"log_interval\"]:.3f}', 'lr': f'{lr:.2e}'})\n",
    "        running_loss = 0\n",
    "    \n",
    "    if (step + 1) % config['val_interval'] == 0:\n",
    "        val_loss, val_ppl, val_bpb = evaluate(model, val_loader, config['val_batches'])\n",
    "        train_loss, _, _ = evaluate(model, train_loader, config['val_batches'])\n",
    "        \n",
    "        history['step'].append(step + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        history['val_bpb'].append(val_bpb)\n",
    "        history['lr'].append(lr)\n",
    "        \n",
    "        gap = val_loss - train_loss\n",
    "        print(f'\\nStep {step+1}: train={train_loss:.3f}, val={val_loss:.3f}, ppl={val_ppl:.2f}, bpb={val_bpb:.3f}, gap={gap:.3f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model_config': model_config.__dict__,\n",
    "                'val_loss': val_loss, 'val_ppl': val_ppl, 'val_bpb': val_bpb,\n",
    "                'dataset': 'code-python',\n",
    "            }, ckpt_dir / 'best.pt')\n",
    "            print(f'  \u2713 New best!')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement ({patience_counter}/{config[\"patience\"]})')\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f'\\nEarly stopping')\n",
    "            break\n",
    "\n",
    "print(f'\\nBest val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "with open(ckpt_dir / 'config.json', 'w') as f:\n",
    "    json.dump({**config, 'model_config': model_config.__dict__, 'parameters': params, 'dataset': 'code-python'}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "axes[0].plot(history['step'], history['train_loss'], 'b-', label='Train')\n",
    "axes[0].plot(history['step'], history['val_loss'], 'r-', label='Val')\n",
    "axes[0].set_xlabel('Step'); axes[0].set_ylabel('Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['step'], history['val_ppl'], 'g-')\n",
    "axes[1].set_xlabel('Step'); axes[1].set_ylabel('Perplexity'); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['step'], history['val_bpb'], 'm-')\n",
    "axes[2].set_xlabel('Step'); axes[2].set_ylabel('BPB'); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "gaps = [v - t for v, t in zip(history['val_loss'], history['train_loss'])]\n",
    "axes[3].fill_between(history['step'], gaps, alpha=0.5, color='orange')\n",
    "axes[3].set_xlabel('Step'); axes[3].set_ylabel('Gap'); axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Hierarchical BDH ({MODEL_SIZE}) - Python Code', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_dir / 'best.pt')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "print('Validation...')\n",
    "val_loss, val_ppl, val_bpb = evaluate(model, val_loader)\n",
    "print(f'Val: loss={val_loss:.4f}, ppl={val_ppl:.2f}, bpb={val_bpb:.3f}')\n",
    "\n",
    "test_dataset = ByteDataset(test_data, config['block_size'], config['patch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print('\\nTest...')\n",
    "test_loss, test_ppl, test_bpb = evaluate(model, test_loader)\n",
    "print(f'Test: loss={test_loss:.4f}, ppl={test_ppl:.2f}, bpb={test_bpb:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Generation Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code(model, prompt, max_new_tokens=200, temperature=0.7, top_k=40):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([list(prompt.encode('utf-8'))], device=device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    return bytes(output[0].tolist()).decode('utf-8', errors='replace')\n",
    "\n",
    "# Code prompts\n",
    "prompts = [\n",
    "    'def fibonacci(n):\\n',\n",
    "    'class DataLoader:\\n    def __init__(self',\n",
    "    'import torch\\nimport torch.nn as nn\\n\\nclass',\n",
    "    '# Function to sort a list\\ndef',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('CODE GENERATION SAMPLES')\n",
    "print('=' * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\n--- Prompt ---')\n",
    "    print(prompt)\n",
    "    print('--- Generated ---')\n",
    "    print(generate_code(model, prompt, max_new_tokens=150, temperature=0.7))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "zip_name = f'hierarchical_bdh_{MODEL_SIZE}_code.zip'\n",
    "subprocess.run(['zip', '-r', zip_name, str(ckpt_dir)], check=True)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_name)\n",
    "    print('\u2705 Download started!')\n",
    "except ImportError:\n",
    "    print(f'Saved: {zip_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
