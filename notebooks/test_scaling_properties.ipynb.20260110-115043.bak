{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Testing BDH's \"Scale-Free\" Claims\n",
    "\n",
    "The BDH paper uses \"scale\" ambiguously. Let's test what they might mean:\n",
    "\n",
    "| Interpretation | What It Means | How We Test |\n",
    "|----------------|---------------|-------------|\n",
    "| Scale-free network | Power-law degree distribution | Analyze weight matrices |\n",
    "| Parameter scaling | Efficient with more params | Train different sizes |\n",
    "| Data scaling | Learns efficiently from data | Learning curves |\n",
    "| Context scaling | Handles any context length | Already tested |\n",
    "| Attention head scaling | Not fixed like transformers | Compare architectures |\n",
    "\n",
    "## What Adrian Kosowski Said\n",
    "> \"There's one dimension which appears to be fixed... the size of the attention head vector dimension in a transformer... all concepts have to be mapped into a vector space of about 1,000 dimensions.\"\n",
    "\n",
    "> \"In BDH this can scale freely.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install torch matplotlib numpy scipy networkx tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo if needed\n",
    "import os\n",
    "if not os.path.exists('bdh.py'):\n",
    "    !git clone https://github.com/newsbubbles/bdh.git temp_bdh\n",
    "    !cp temp_bdh/bdh.py .\n",
    "    !cp temp_bdh/hierarchical_bdh.py .\n",
    "    !rm -rf temp_bdh\n",
    "\n",
    "from bdh import BDH, BDHConfig\n",
    "from hierarchical_bdh import HierarchicalBDH, HierarchicalBDHConfig\n",
    "print('Models loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Scale-Free Network Structure\n",
    "\n",
    "BDH claims to have \"scale-free\" network structure like biological neural networks.\n",
    "Scale-free networks have power-law degree distributions: P(k) ‚àù k^(-Œ≥)\n",
    "\n",
    "Let's analyze the learned weight matrices as adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weight_distribution(model):\n",
    "    \"\"\"\n",
    "    Analyze weight matrices as if they were adjacency matrices.\n",
    "    Check for power-law (scale-free) properties.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'encoder' in name or 'decoder' in name:\n",
    "            W = param.detach().cpu().numpy()\n",
    "            \n",
    "            # Treat absolute weights as connection strengths\n",
    "            W_abs = np.abs(W)\n",
    "            \n",
    "            # Compute \"degree\" - sum of connection strengths per neuron\n",
    "            # (treating rows as \"from\" and cols as \"to\")\n",
    "            out_degree = W_abs.sum(axis=1)  # sum over columns\n",
    "            in_degree = W_abs.sum(axis=0)   # sum over rows\n",
    "            \n",
    "            # Also look at number of \"strong\" connections\n",
    "            threshold = np.percentile(W_abs, 90)  # top 10% are \"connections\"\n",
    "            binary_W = (W_abs > threshold).astype(float)\n",
    "            discrete_out = binary_W.sum(axis=1)\n",
    "            discrete_in = binary_W.sum(axis=0)\n",
    "            \n",
    "            results[name] = {\n",
    "                'shape': W.shape,\n",
    "                'out_degree': out_degree,\n",
    "                'in_degree': in_degree,\n",
    "                'discrete_out': discrete_out,\n",
    "                'discrete_in': discrete_in,\n",
    "                'weight_distribution': W.flatten(),\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_power_law(data, min_val=None):\n",
    "    \"\"\"\n",
    "    Fit power law P(x) ‚àù x^(-alpha) to data.\n",
    "    Returns alpha and goodness of fit.\n",
    "    \"\"\"\n",
    "    data = data[data > 0]  # Remove zeros\n",
    "    if len(data) < 10:\n",
    "        return None, None, None\n",
    "    \n",
    "    if min_val is None:\n",
    "        min_val = np.percentile(data, 10)\n",
    "    \n",
    "    data = data[data >= min_val]\n",
    "    if len(data) < 10:\n",
    "        return None, None, None\n",
    "    \n",
    "    # MLE for power law exponent\n",
    "    # alpha = 1 + n / sum(ln(x/xmin))\n",
    "    n = len(data)\n",
    "    alpha = 1 + n / np.sum(np.log(data / min_val))\n",
    "    \n",
    "    # Kolmogorov-Smirnov test for goodness of fit\n",
    "    # Compare empirical CDF to theoretical power law CDF\n",
    "    sorted_data = np.sort(data)\n",
    "    empirical_cdf = np.arange(1, n + 1) / n\n",
    "    theoretical_cdf = 1 - (min_val / sorted_data) ** (alpha - 1)\n",
    "    ks_stat = np.max(np.abs(empirical_cdf - theoretical_cdf))\n",
    "    \n",
    "    return alpha, ks_stat, min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "checkpoint_path = 'checkpoints_hierarchical_small/best.pt'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "    \n",
    "    if any('global_model' in k for k in state_dict.keys()):\n",
    "        config = HierarchicalBDHConfig(**checkpoint.get('config', {}))\n",
    "        model = HierarchicalBDH(config).to(device)\n",
    "    else:\n",
    "        config = BDHConfig(**checkpoint.get('config', {}))\n",
    "        model = BDH(config).to(device)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    print('Loaded trained model')\n",
    "else:\n",
    "    config = BDHConfig(n_layer=4, n_embd=256, n_head=4, mlp_internal_dim_multiplier=64)\n",
    "    model = BDH(config).to(device)\n",
    "    print('Using untrained model (results less meaningful)')\n",
    "\n",
    "# Analyze\n",
    "weight_analysis = analyze_weight_distribution(model)\n",
    "print(f'Analyzed {len(weight_analysis)} weight matrices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degree distributions\n",
    "n_matrices = len(weight_analysis)\n",
    "fig, axes = plt.subplots(2, min(4, n_matrices), figsize=(16, 8))\n",
    "if n_matrices < 4:\n",
    "    axes = axes.reshape(2, -1)\n",
    "\n",
    "power_law_results = {}\n",
    "\n",
    "for idx, (name, data) in enumerate(list(weight_analysis.items())[:4]):\n",
    "    # Continuous degree (weighted)\n",
    "    ax = axes[0, idx]\n",
    "    degrees = data['out_degree']\n",
    "    ax.hist(degrees, bins=50, alpha=0.7, density=True)\n",
    "    ax.set_xlabel('Weighted Out-Degree')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{name.split(\".\")[-1]}\\n{data[\"shape\"]}')\n",
    "    \n",
    "    # Fit power law\n",
    "    alpha, ks, xmin = fit_power_law(degrees)\n",
    "    if alpha is not None:\n",
    "        ax.text(0.95, 0.95, f'Œ±={alpha:.2f}\\nKS={ks:.3f}', \n",
    "                transform=ax.transAxes, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "        power_law_results[name] = {'alpha': alpha, 'ks': ks}\n",
    "    \n",
    "    # Log-log plot for power law check\n",
    "    ax = axes[1, idx]\n",
    "    # Compute histogram for log-log\n",
    "    hist, bin_edges = np.histogram(degrees[degrees > 0], bins=30)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    mask = hist > 0\n",
    "    ax.scatter(np.log10(bin_centers[mask]), np.log10(hist[mask]), alpha=0.7)\n",
    "    \n",
    "    # Fit line\n",
    "    if mask.sum() > 2:\n",
    "        slope, intercept = np.polyfit(np.log10(bin_centers[mask]), np.log10(hist[mask]), 1)\n",
    "        x_fit = np.linspace(np.log10(bin_centers[mask].min()), np.log10(bin_centers[mask].max()), 100)\n",
    "        ax.plot(x_fit, slope * x_fit + intercept, 'r--', label=f'slope={slope:.2f}')\n",
    "        ax.legend()\n",
    "    \n",
    "    ax.set_xlabel('log10(Degree)')\n",
    "    ax.set_ylabel('log10(Count)')\n",
    "    ax.set_title('Log-Log Plot (linear = power law)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scale_free_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale-free verdict\n",
    "print('='*70)\n",
    "print('SCALE-FREE NETWORK STRUCTURE ANALYSIS')\n",
    "print('='*70)\n",
    "print()\n",
    "print('Power law exponents (Œ±) for weight matrices:')\n",
    "print('(Scale-free networks typically have 2 < Œ± < 3)')\n",
    "print()\n",
    "\n",
    "scale_free_count = 0\n",
    "for name, result in power_law_results.items():\n",
    "    alpha = result['alpha']\n",
    "    ks = result['ks']\n",
    "    is_scale_free = 2 < alpha < 3 and ks < 0.1\n",
    "    status = '‚úì' if is_scale_free else '~' if 1.5 < alpha < 4 else '‚úó'\n",
    "    if is_scale_free:\n",
    "        scale_free_count += 1\n",
    "    print(f'  {name}: Œ±={alpha:.2f}, KS={ks:.3f} {status}')\n",
    "\n",
    "print()\n",
    "if scale_free_count > len(power_law_results) // 2:\n",
    "    print('‚úì CLAIM SUPPORTED: Weight matrices show scale-free properties')\n",
    "elif scale_free_count > 0:\n",
    "    print('~ PARTIALLY SUPPORTED: Some matrices show scale-free properties')\n",
    "else:\n",
    "    print('‚úó CLAIM NOT CLEARLY SUPPORTED: No strong power-law in weights')\n",
    "    print('  (Note: Scale-free structure may emerge differently in BDH)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 2: Parameter Scaling Efficiency\n",
    "\n",
    "How efficiently does BDH use additional parameters?\n",
    "Compare loss vs parameter count for different model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model size configurations\n",
    "model_configs = {\n",
    "    'tiny': {'n_layer': 2, 'n_embd': 128, 'n_head': 2, 'mlp_internal_dim_multiplier': 32},\n",
    "    'small': {'n_layer': 4, 'n_embd': 256, 'n_head': 4, 'mlp_internal_dim_multiplier': 64},\n",
    "    'base': {'n_layer': 6, 'n_embd': 384, 'n_head': 6, 'mlp_internal_dim_multiplier': 96},\n",
    "    'large': {'n_layer': 8, 'n_embd': 512, 'n_head': 8, 'mlp_internal_dim_multiplier': 128},\n",
    "}\n",
    "\n",
    "# Count parameters for each\n",
    "param_counts = {}\n",
    "for name, cfg in model_configs.items():\n",
    "    model = BDH(BDHConfig(**cfg))\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    param_counts[name] = n_params\n",
    "    print(f'{name}: {n_params:,} parameters')\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training to measure scaling\n",
    "# (This cell takes a while - skip if you already have checkpoints)\n",
    "\n",
    "RUN_SCALING_TEST = False  # Set to True to run\n",
    "\n",
    "if RUN_SCALING_TEST:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load data\n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    text = '\\n'.join(dataset['text'])\n",
    "    data = torch.tensor([b for b in text.encode('utf-8')], dtype=torch.long)\n",
    "    \n",
    "    def quick_train(config, data, steps=500, batch_size=32, block_size=256):\n",
    "        model = BDH(BDHConfig(**config)).to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        losses = []\n",
    "        for step in range(steps):\n",
    "            # Random batch\n",
    "            idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "            x = torch.stack([data[i:i+block_size] for i in idx]).to(device)\n",
    "            y = torch.stack([data[i+1:i+block_size+1] for i in idx]).to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(losses[-50:])  # Final loss\n",
    "    \n",
    "    scaling_results = {}\n",
    "    for name, cfg in tqdm(model_configs.items()):\n",
    "        try:\n",
    "            final_loss = quick_train(cfg, data)\n",
    "            scaling_results[name] = {\n",
    "                'params': param_counts[name],\n",
    "                'loss': final_loss\n",
    "            }\n",
    "            print(f'{name}: loss={final_loss:.4f}')\n",
    "        except RuntimeError as e:\n",
    "            print(f'{name}: OOM')\n",
    "else:\n",
    "    print('Scaling test skipped. Set RUN_SCALING_TEST=True to run.')\n",
    "    print('Using placeholder data for visualization...')\n",
    "    # Placeholder based on typical results\n",
    "    scaling_results = {\n",
    "        'tiny': {'params': param_counts['tiny'], 'loss': 2.5},\n",
    "        'small': {'params': param_counts['small'], 'loss': 1.8},\n",
    "        'base': {'params': param_counts['base'], 'loss': 1.4},\n",
    "        'large': {'params': param_counts['large'], 'loss': 1.2},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaling curve\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "params = [scaling_results[k]['params'] for k in model_configs.keys() if k in scaling_results]\n",
    "losses = [scaling_results[k]['loss'] for k in model_configs.keys() if k in scaling_results]\n",
    "names = [k for k in model_configs.keys() if k in scaling_results]\n",
    "\n",
    "ax.scatter(params, losses, s=100, zorder=5)\n",
    "for i, name in enumerate(names):\n",
    "    ax.annotate(name, (params[i], losses[i]), textcoords='offset points', xytext=(5,5))\n",
    "\n",
    "# Fit power law: loss = a * params^(-b)\n",
    "log_params = np.log(params)\n",
    "log_losses = np.log(losses)\n",
    "slope, intercept = np.polyfit(log_params, log_losses, 1)\n",
    "\n",
    "x_fit = np.linspace(min(params), max(params), 100)\n",
    "y_fit = np.exp(intercept) * x_fit ** slope\n",
    "ax.plot(x_fit, y_fit, 'r--', label=f'Loss ‚àù params^{slope:.3f}')\n",
    "\n",
    "ax.set_xlabel('Parameters', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('BDH Scaling Law: Loss vs Parameters', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_scaling.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nScaling exponent: {slope:.3f}')\n",
    "print(f'(Chinchilla found ~-0.34 for transformers)')\n",
    "if slope < -0.3:\n",
    "    print('‚úì BDH shows good parameter scaling efficiency')\n",
    "elif slope < -0.2:\n",
    "    print('~ BDH shows moderate scaling')\n",
    "else:\n",
    "    print('‚úó BDH shows weak scaling with parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 3: Attention Head Dimension Flexibility\n",
    "\n",
    "Transformers have fixed attention head dimension (~64-128).\n",
    "BDH claims this can \"scale freely\". Let's test different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In BDH, the \"attention dimension\" is controlled by mlp_internal_dim_multiplier\n",
    "# Let's see how this affects performance\n",
    "\n",
    "mlp_multipliers = [16, 32, 64, 128, 256]\n",
    "\n",
    "print('BDH Latent Space Dimensions:')\n",
    "print('(This is analogous to attention head dimension in transformers)')\n",
    "print()\n",
    "\n",
    "for mult in mlp_multipliers:\n",
    "    config = BDHConfig(n_layer=4, n_embd=256, n_head=4, mlp_internal_dim_multiplier=mult)\n",
    "    model = BDH(config)\n",
    "    \n",
    "    # Get actual latent dimension\n",
    "    latent_dim = config.n_embd * mult // config.n_head\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f'Multiplier {mult:3d}: Latent dim = {latent_dim:5d}, Params = {n_params:,}')\n",
    "    del model\n",
    "\n",
    "print()\n",
    "print('For comparison, Transformer attention heads are typically 64-128 dims')\n",
    "print('BDH can use much larger latent spaces (thousands of dims)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if larger latent dimension helps\n",
    "# (Quick test - increase steps for more reliable results)\n",
    "\n",
    "RUN_LATENT_TEST = False  # Set to True to run\n",
    "\n",
    "if RUN_LATENT_TEST:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    text = '\\n'.join(dataset['text'])\n",
    "    data = torch.tensor([b for b in text.encode('utf-8')], dtype=torch.long)\n",
    "    \n",
    "    latent_results = {}\n",
    "    \n",
    "    for mult in [32, 64, 128]:\n",
    "        config = {'n_layer': 4, 'n_embd': 256, 'n_head': 4, 'mlp_internal_dim_multiplier': mult}\n",
    "        \n",
    "        try:\n",
    "            model = BDH(BDHConfig(**config)).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "            \n",
    "            losses = []\n",
    "            for step in tqdm(range(300), desc=f'mult={mult}'):\n",
    "                idx = torch.randint(0, len(data) - 256, (32,))\n",
    "                x = torch.stack([data[i:i+256] for i in idx]).to(device)\n",
    "                y = torch.stack([data[i+1:i+257] for i in idx]).to(device)\n",
    "                \n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.view(-1, 256), y.view(-1))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "            \n",
    "            latent_results[mult] = np.mean(losses[-30:])\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        except RuntimeError:\n",
    "            print(f'OOM at mult={mult}')\n",
    "    \n",
    "    print('\\nResults:')\n",
    "    for mult, loss in latent_results.items():\n",
    "        print(f'  Multiplier {mult}: Loss = {loss:.4f}')\n",
    "else:\n",
    "    print('Latent dimension test skipped.')\n",
    "    print('Set RUN_LATENT_TEST=True to run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 4: Data Efficiency (Learning Curves)\n",
    "\n",
    "How efficiently does BDH learn from data compared to parameter count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have training logs, load and analyze them\n",
    "# Otherwise, this shows how to analyze learning curves\n",
    "\n",
    "print('='*70)\n",
    "print('DATA EFFICIENCY ANALYSIS')\n",
    "print('='*70)\n",
    "print()\n",
    "print('To properly test data efficiency, compare:')\n",
    "print('1. Loss vs tokens seen (should follow power law)')\n",
    "print('2. Optimal model size vs dataset size (Chinchilla scaling)')\n",
    "print()\n",
    "print('From your WikiText-2 training:')\n",
    "print('- Started at loss ~3.2')\n",
    "print('- Reached loss ~0.18 in 5000 steps')\n",
    "print('- With batch_size=64, block_size=512')\n",
    "print('- Thats ~160M tokens processed')\n",
    "print()\n",
    "print('This suggests BDH learns efficiently from data,')\n",
    "print('achieving low perplexity with relatively few tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('BDH \"SCALE-FREE\" CLAIMS SUMMARY')\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "print('The term \"scale-free\" in BDH appears to mean multiple things:')\n",
    "print()\n",
    "\n",
    "print('1. SCALE-FREE NETWORK STRUCTURE')\n",
    "print('   Claim: Power-law connectivity like biological networks')\n",
    "print('   Finding: [Run analysis above for verdict]')\n",
    "print()\n",
    "\n",
    "print('2. FLEXIBLE LATENT DIMENSION')\n",
    "print('   Claim: Not limited to ~1000 dims like transformer attention')\n",
    "print('   Finding: ‚úì BDH can use arbitrary latent dimensions')\n",
    "print('            (mlp_internal_dim_multiplier controls this)')\n",
    "print()\n",
    "\n",
    "print('3. CONTEXT LENGTH SCALING')\n",
    "print('   Claim: Handles unlimited context efficiently')\n",
    "print('   Finding: [See unlimited context notebook]')\n",
    "print()\n",
    "\n",
    "print('4. PARAMETER SCALING')\n",
    "print('   Claim: Efficient scaling with model size')\n",
    "print('   Finding: [Run scaling test above for verdict]')\n",
    "print()\n",
    "\n",
    "print('='*70)\n",
    "print('INTERPRETATION')\n",
    "print('='*70)\n",
    "print()\n",
    "print('The BDH paper uses \"scale-free\" loosely. The strongest claim is:')\n",
    "print('- BDHs latent dimension is not artificially constrained')\n",
    "print('- Transformers fix attention head dim at ~64-128')\n",
    "print('- BDH can use thousands of dimensions in its sparse space')\n",
    "print()\n",
    "print('This flexibility + sparsity may enable better concept representation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}