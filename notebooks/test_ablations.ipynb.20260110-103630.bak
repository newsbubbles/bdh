{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Ablation Study\n",
    "\n",
    "**Goal**: Understand which components of BDH are essential for its performance.\n",
    "\n",
    "## Components to Ablate\n",
    "\n",
    "| Component | Default | Ablation |\n",
    "|-----------|---------|----------|\n",
    "| Activation | ReLU (sparse) | GELU, None |\n",
    "| Gating | x_sparse * y_sparse | Just y_sparse |\n",
    "| Q=K sharing | Shared | Separate Q, K |\n",
    "| Attention | Linear (no softmax) | Softmax attention |\n",
    "| RoPE location | Neuron space (N dims) | Embedding space (D dims) |\n",
    "| LayerNorm | After each op | Remove some/all |\n",
    "\n",
    "## Hypothesis\n",
    "The sparse ReLU encoding and gating mechanism are the key innovations.\n",
    "Linear attention may be less critical than the sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import dataclasses\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q datasets matplotlib tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable BDH with Ablation Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class BDHAblationConfig:\n",
    "    # Standard BDH config\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    n_head: int = 4\n",
    "    mlp_internal_dim_multiplier: int = 64\n",
    "    vocab_size: int = 256\n",
    "    \n",
    "    # Ablation options\n",
    "    activation: str = 'relu'      # 'relu', 'gelu', 'none'\n",
    "    use_gating: bool = True       # x*y gating vs just y\n",
    "    share_qk: bool = True         # Q=K vs separate\n",
    "    attention_type: str = 'linear'  # 'linear', 'softmax'\n",
    "    rope_space: str = 'neuron'    # 'neuron', 'embedding'\n",
    "    use_layernorm: bool = True    # Use LayerNorm\n",
    "\n",
    "\n",
    "def get_freqs(n, theta, dtype):\n",
    "    def quantize(t, q=2):\n",
    "        return (t / q).floor() * q\n",
    "    return 1.0 / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n)) / (2 * math.pi)\n",
    "\n",
    "\n",
    "class AblationAttention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        \n",
    "        # RoPE frequencies - in neuron space (N) or embedding space (D)\n",
    "        if config.rope_space == 'neuron':\n",
    "            freq_dim = N\n",
    "        else:\n",
    "            freq_dim = D // nh  # Per-head embedding dim\n",
    "        \n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(freq_dim, theta=2**16, dtype=torch.float32).view(1, 1, 1, freq_dim)\n",
    "        )\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        self.nh = nh\n",
    "        \n",
    "        # For separate Q, K (if not sharing)\n",
    "        if not config.share_qk:\n",
    "            self.Wk = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases, v):\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        return (v * torch.cos(phases)).to(v.dtype) + (v_rot * torch.sin(phases)).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q, K, V, encoder=None, x=None):\n",
    "        config = self.config\n",
    "        _, _, T, _ = Q.size()\n",
    "        \n",
    "        # Handle separate Q, K\n",
    "        if not config.share_qk and x is not None:\n",
    "            K = F.relu(x @ self.Wk) if config.activation == 'relu' else x @ self.Wk\n",
    "        \n",
    "        # RoPE\n",
    "        r_phases = torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype).view(1, 1, -1, 1) * self.freqs\n",
    "        \n",
    "        if config.rope_space == 'neuron':\n",
    "            QR = self.rope(r_phases, Q)\n",
    "            KR = self.rope(r_phases, K) if not config.share_qk else QR\n",
    "        else:\n",
    "            # RoPE in embedding space - need to reshape\n",
    "            QR = Q  # Skip RoPE in sparse space\n",
    "            KR = K if not config.share_qk else Q\n",
    "        \n",
    "        # Attention computation\n",
    "        if config.attention_type == 'linear':\n",
    "            scores = (QR @ KR.mT).tril(diagonal=-1)\n",
    "            return scores @ V\n",
    "        else:  # softmax\n",
    "            scores = (QR @ KR.mT) / math.sqrt(QR.size(-1))\n",
    "            # Causal mask\n",
    "            mask = torch.triu(torch.ones(T, T, device=scores.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "            attn = F.softmax(scores, dim=-1)\n",
    "            return attn @ V\n",
    "\n",
    "\n",
    "class BDHAblation(nn.Module):\n",
    "    def __init__(self, config: BDHAblationConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        \n",
    "        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))\n",
    "        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.attn = AblationAttention(config)\n",
    "        \n",
    "        if config.use_layernorm:\n",
    "            self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        else:\n",
    "            self.ln = nn.Identity()\n",
    "        \n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.lm_head = nn.Parameter(torch.zeros((D, config.vocab_size)).normal_(std=0.02))\n",
    "        \n",
    "        # Activation function\n",
    "        if config.activation == 'relu':\n",
    "            self.act = F.relu\n",
    "        elif config.activation == 'gelu':\n",
    "            self.act = F.gelu\n",
    "        else:\n",
    "            self.act = lambda x: x  # No activation\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        C = self.config\n",
    "        B, T = idx.size()\n",
    "        D = C.n_embd\n",
    "        nh = C.n_head\n",
    "        N = D * C.mlp_internal_dim_multiplier // nh\n",
    "        \n",
    "        x = self.embed(idx).unsqueeze(1)\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        for level in range(C.n_layer):\n",
    "            x_latent = x @ self.encoder\n",
    "            x_sparse = self.act(x_latent)\n",
    "            \n",
    "            yKV = self.attn(Q=x_sparse, K=x_sparse, V=x, encoder=self.encoder, x=x)\n",
    "            yKV = self.ln(yKV)\n",
    "            \n",
    "            y_latent = yKV @ self.encoder_v\n",
    "            y_sparse = self.act(y_latent)\n",
    "            \n",
    "            # Gating\n",
    "            if C.use_gating:\n",
    "                xy_sparse = x_sparse * y_sparse\n",
    "            else:\n",
    "                xy_sparse = y_sparse\n",
    "            \n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "            yMLP = xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)\n",
    "        \n",
    "        logits = x.view(B, T, D) @ self.lm_head\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Ablation BDH defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Ablation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base config (vanilla BDH)\n",
    "BASE_CONFIG = BDHAblationConfig(\n",
    "    n_layer=4,\n",
    "    n_embd=256,\n",
    "    n_head=4,\n",
    "    mlp_internal_dim_multiplier=64,\n",
    "    dropout=0.1,\n",
    "    vocab_size=256,\n",
    "    # Default (vanilla) settings\n",
    "    activation='relu',\n",
    "    use_gating=True,\n",
    "    share_qk=True,\n",
    "    attention_type='linear',\n",
    "    rope_space='neuron',\n",
    "    use_layernorm=True,\n",
    ")\n",
    "\n",
    "# Define ablations\n",
    "ABLATIONS = {\n",
    "    'baseline': BASE_CONFIG,\n",
    "    \n",
    "    # Activation ablations\n",
    "    'gelu_activation': dataclasses.replace(BASE_CONFIG, activation='gelu'),\n",
    "    'no_activation': dataclasses.replace(BASE_CONFIG, activation='none'),\n",
    "    \n",
    "    # Gating ablation\n",
    "    'no_gating': dataclasses.replace(BASE_CONFIG, use_gating=False),\n",
    "    \n",
    "    # Q=K ablation\n",
    "    'separate_qk': dataclasses.replace(BASE_CONFIG, share_qk=False),\n",
    "    \n",
    "    # Attention type ablation\n",
    "    'softmax_attention': dataclasses.replace(BASE_CONFIG, attention_type='softmax'),\n",
    "    \n",
    "    # LayerNorm ablation\n",
    "    'no_layernorm': dataclasses.replace(BASE_CONFIG, use_layernorm=False),\n",
    "}\n",
    "\n",
    "print('Ablation experiments defined:')\n",
    "for name in ABLATIONS:\n",
    "    print(f'  - {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print('Loading WikiText-2...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def encode_text(text):\n",
    "    return [b for b in text.encode('utf-8')]\n",
    "\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "\n",
    "train_data = torch.tensor(encode_text(train_text), dtype=torch.long)\n",
    "val_data = torch.tensor(encode_text(val_text), dtype=torch.long)\n",
    "\n",
    "print(f'Train: {len(train_data):,} bytes')\n",
    "print(f'Val: {len(val_data):,} bytes')\n",
    "\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CONFIG = {\n",
    "    'max_steps': 2000,\n",
    "    'batch_size': 32,\n",
    "    'block_size': 512,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'eval_interval': 100,\n",
    "    'eval_iters': 20,\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_size, block_size):\n",
    "    model.eval()\n",
    "    losses = {'train': [], 'val': []}\n",
    "    for split in ['train', 'val']:\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split, batch_size, block_size)\n",
    "            _, loss = model(x, y)\n",
    "            losses[split].append(loss.item())\n",
    "    return {k: np.mean(v) for k, v in losses.items()}\n",
    "\n",
    "\n",
    "def train_ablation(name, config, train_config):\n",
    "    print(f'\\n{\"="*60}')\n",
    "    print(f'Ablation: {name}')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Print config diff from baseline\n",
    "    if name != 'baseline':\n",
    "        for field in dataclasses.fields(config):\n",
    "            base_val = getattr(BASE_CONFIG, field.name)\n",
    "            curr_val = getattr(config, field.name)\n",
    "            if base_val != curr_val:\n",
    "                print(f'  Changed: {field.name} = {base_val} -> {curr_val}')\n",
    "    \n",
    "    model = BDHAblation(config).to(device)\n",
    "    n_params = count_parameters(model)\n",
    "    print(f'Parameters: {n_params:,}')\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config['learning_rate'],\n",
    "        weight_decay=train_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    history = {'step': [], 'train_loss': [], 'val_loss': [], 'bpb': []}\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(train_config['max_steps']), desc=name)\n",
    "    \n",
    "    for step in pbar:\n",
    "        model.train()\n",
    "        x, y = get_batch('train', train_config['batch_size'], train_config['block_size'])\n",
    "        \n",
    "        try:\n",
    "            _, loss = model(x, y)\n",
    "            \n",
    "            # Check for NaN\n",
    "            if torch.isnan(loss):\n",
    "                print(f'\\nNaN loss at step {step}!')\n",
    "                break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(f'\\nError at step {step}: {e}')\n",
    "            break\n",
    "        \n",
    "        if step % train_config['eval_interval'] == 0:\n",
    "            losses = estimate_loss(model, train_config['eval_iters'],\n",
    "                                   train_config['batch_size'], train_config['block_size'])\n",
    "            bpb = losses['val'] / math.log(2)\n",
    "            \n",
    "            history['step'].append(step)\n",
    "            history['train_loss'].append(losses['train'])\n",
    "            history['val_loss'].append(losses['val'])\n",
    "            history['bpb'].append(bpb)\n",
    "            \n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "            \n",
    "            pbar.set_postfix({'val': f\"{losses['val']:.3f}\", 'bpb': f'{bpb:.3f}'})\n",
    "    \n",
    "    # Final eval\n",
    "    final_losses = estimate_loss(model, 50, train_config['batch_size'], train_config['block_size'])\n",
    "    \n",
    "    results = {\n",
    "        'name': name,\n",
    "        'params': n_params,\n",
    "        'final_val_loss': final_losses['val'],\n",
    "        'final_bpb': final_losses['val'] / math.log(2),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'converged': not torch.isnan(torch.tensor(final_losses['val'])),\n",
    "    }\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return history, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all ablations\n",
    "all_histories = {}\n",
    "all_results = {}\n",
    "\n",
    "for name, config in ABLATIONS.items():\n",
    "    try:\n",
    "        history, results = train_ablation(name, config, TRAIN_CONFIG)\n",
    "        all_histories[name] = history\n",
    "        all_results[name] = results\n",
    "    except Exception as e:\n",
    "        print(f'Failed {name}: {e}')\n",
    "        all_results[name] = {'name': name, 'error': str(e), 'converged': False}\n",
    "\n",
    "print('\\nAll ablations complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print('\\n' + '='*80)\n",
    "print('ABLATION STUDY RESULTS')\n",
    "print('='*80)\n",
    "print(f\"{'Ablation':<25} {'Val Loss':<12} {'BPB':<10} {'vs Baseline':<15} {'Status':<10}\")\n",
    "print('-'*80)\n",
    "\n",
    "baseline_bpb = all_results.get('baseline', {}).get('final_bpb', float('inf'))\n",
    "\n",
    "for name, r in all_results.items():\n",
    "    if r.get('converged', False):\n",
    "        bpb = r['final_bpb']\n",
    "        diff = bpb - baseline_bpb\n",
    "        diff_str = f\"{diff:+.3f}\" if name != 'baseline' else 'N/A'\n",
    "        status = '✓' if diff <= 0.1 else '✗ worse'\n",
    "        print(f\"{name:<25} {r['final_val_loss']:<12.4f} {bpb:<10.3f} {diff_str:<15} {status:<10}\")\n",
    "    else:\n",
    "        print(f\"{name:<25} {'N/A':<12} {'N/A':<10} {'N/A':<15} {'FAILED':<10}\")\n",
    "\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training curves\n",
    "ax1 = axes[0]\n",
    "for name, history in all_histories.items():\n",
    "    if history['val_loss']:\n",
    "        ax1.plot(history['step'], history['val_loss'], label=name, linewidth=2)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Validation Loss')\n",
    "ax1.set_title('Training Curves by Ablation')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart of final BPB\n",
    "ax2 = axes[1]\n",
    "names = [n for n, r in all_results.items() if r.get('converged', False)]\n",
    "bpbs = [all_results[n]['final_bpb'] for n in names]\n",
    "colors = ['green' if n == 'baseline' else 'steelblue' for n in names]\n",
    "\n",
    "bars = ax2.barh(names, bpbs, color=colors)\n",
    "ax2.axvline(baseline_bpb, color='red', linestyle='--', label='Baseline')\n",
    "ax2.set_xlabel('Bits per Byte')\n",
    "ax2.set_title('Final BPB by Ablation')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component importance ranking\n",
    "print('\\n' + '='*60)\n",
    "print('COMPONENT IMPORTANCE RANKING')\n",
    "print('='*60)\n",
    "print('(Sorted by impact on BPB - larger = more important)')\n",
    "print()\n",
    "\n",
    "impacts = []\n",
    "for name, r in all_results.items():\n",
    "    if name != 'baseline' and r.get('converged', False):\n",
    "        impact = r['final_bpb'] - baseline_bpb\n",
    "        impacts.append((name, impact))\n",
    "\n",
    "impacts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for name, impact in impacts:\n",
    "    importance = 'CRITICAL' if impact > 0.5 else 'Important' if impact > 0.2 else 'Moderate' if impact > 0.05 else 'Minor'\n",
    "    print(f'{name:<25} BPB impact: {impact:+.3f}  ({importance})')\n",
    "\n",
    "print()\n",
    "print('Interpretation:')\n",
    "print('  - Positive impact = removing this hurts performance = component is important')\n",
    "print('  - Negative impact = removing this helps = component may be unnecessary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated conclusions\n",
    "print('\\n' + '='*60)\n",
    "print('ABLATION STUDY CONCLUSIONS')\n",
    "print('='*60)\n",
    "\n",
    "conclusions = []\n",
    "\n",
    "for name, impact in impacts:\n",
    "    if 'activation' in name:\n",
    "        if impact > 0.1:\n",
    "            conclusions.append(f'ReLU activation is important (removing costs {impact:.2f} BPB)')\n",
    "        else:\n",
    "            conclusions.append(f'Activation function choice has minor impact ({impact:.2f} BPB)')\n",
    "    elif 'gating' in name:\n",
    "        if impact > 0.1:\n",
    "            conclusions.append(f'Gating (x*y) is CRITICAL (removing costs {impact:.2f} BPB)')\n",
    "        else:\n",
    "            conclusions.append(f'Gating has minor impact ({impact:.2f} BPB)')\n",
    "    elif 'qk' in name:\n",
    "        if impact < -0.05:\n",
    "            conclusions.append(f'Separate Q,K might be BETTER than shared ({impact:.2f} BPB)')\n",
    "        elif impact > 0.1:\n",
    "            conclusions.append(f'Shared Q=K is important ({impact:.2f} BPB)')\n",
    "    elif 'softmax' in name:\n",
    "        if impact > 0.1:\n",
    "            conclusions.append(f'Linear attention is better than softmax ({impact:.2f} BPB)')\n",
    "        elif impact < -0.05:\n",
    "            conclusions.append(f'Softmax attention might be BETTER ({impact:.2f} BPB)')\n",
    "    elif 'layernorm' in name:\n",
    "        if impact > 0.2:\n",
    "            conclusions.append(f'LayerNorm is CRITICAL ({impact:.2f} BPB)')\n",
    "\n",
    "for i, c in enumerate(conclusions, 1):\n",
    "    print(f'{i}. {c}')\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_data = {\n",
    "    'train_config': TRAIN_CONFIG,\n",
    "    'results': all_results,\n",
    "    'histories': all_histories,\n",
    "    'impacts': dict(impacts),\n",
    "    'conclusions': conclusions,\n",
    "}\n",
    "\n",
    "with open('ablation_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print('Results saved to ablation_results.json')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('ablation_results.json')\n",
    "    files.download('ablation_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}