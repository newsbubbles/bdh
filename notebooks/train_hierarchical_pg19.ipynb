{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical BDH Training on PG-19 (Books)\n",
    "\n",
    "Train **Hierarchical BDH** on PG-19 - a long-form book corpus.\n",
    "\n",
    "## Why PG-19?\n",
    "- **Long-range dependencies**: Full books, not article snippets\n",
    "- **Standard benchmark**: Used by Transformer-XL, Longformer, etc.\n",
    "- **Perfect for hierarchical**: Tests global/local architecture on truly long context\n",
    "- **~11GB** of text from Project Gutenberg books (pre-1919)\n",
    "\n",
    "## Expected Outcomes\n",
    "- Tests hierarchical architecture's long-range modeling\n",
    "- Higher perplexity than WikiText (more diverse, literary style)\n",
    "- Global model should shine here (cross-chapter coherence)\n",
    "\n",
    "## Hardware Requirements\n",
    "- **Recommended**: A100 40GB+ (80GB for large model)\n",
    "- **Minimum**: V100 32GB with small model\n",
    "- Training time: 4-8 hours for 20K steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || (cd bdh && git pull)\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PG-19\n",
    "\n",
    "PG-19 is large (~11GB). We'll stream and sample to manage memory.\n",
    "\n",
    "**Options:**\n",
    "- Full dataset: Best results, needs more RAM\n",
    "- Sampled: Faster iteration, good for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# =============================================================\n",
    "# DATASET SIZE CONFIG\n",
    "# =============================================================\n",
    "USE_FULL_DATASET = False  # Set True for full training\n",
    "SAMPLE_SIZE = 1000  # Number of books to sample if not full\n",
    "# =============================================================\n",
    "\n",
    "print('Loading PG-19 dataset...')\n",
    "print('(This may take several minutes for first download)')\n",
    "\n",
    "if USE_FULL_DATASET:\n",
    "    dataset = load_dataset('pg19', split='train')\n",
    "    val_dataset_raw = load_dataset('pg19', split='validation')\n",
    "    test_dataset_raw = load_dataset('pg19', split='test')\n",
    "else:\n",
    "    # Stream and sample for faster iteration\n",
    "    print(f'Sampling {SAMPLE_SIZE} books...')\n",
    "    dataset = load_dataset('pg19', split=f'train[:{SAMPLE_SIZE}]')\n",
    "    val_dataset_raw = load_dataset('pg19', split='validation[:100]')\n",
    "    test_dataset_raw = load_dataset('pg19', split='test[:100]')\n",
    "\n",
    "print(f'Train books: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "# Concatenate books with separator\n",
    "print('Processing train split...')\n",
    "train_text = '\\n\\n=== BOOK SEPARATOR ===\\n\\n'.join(dataset['text'])\n",
    "print('Processing validation split...')\n",
    "val_text = '\\n\\n=== BOOK SEPARATOR ===\\n\\n'.join(val_dataset_raw['text'])\n",
    "print('Processing test split...')\n",
    "test_text = '\\n\\n=== BOOK SEPARATOR ===\\n\\n'.join(test_dataset_raw['text'])\n",
    "\n",
    "print('\\nConverting to bytes...')\n",
    "train_data = text_to_bytes(train_text)\n",
    "val_data = text_to_bytes(val_text)\n",
    "test_data = text_to_bytes(test_text)\n",
    "\n",
    "# Free memory\n",
    "del train_text, val_text, test_text, dataset, val_dataset_raw, test_dataset_raw\n",
    "import gc; gc.collect()\n",
    "\n",
    "print(f'\\nDataset sizes:')\n",
    "print(f'  Train: {len(train_data):,} bytes ({len(train_data)/1e9:.2f}GB)')\n",
    "print(f'  Val:   {len(val_data):,} bytes ({len(val_data)/1e6:.1f}MB)')\n",
    "print(f'  Test:  {len(test_data):,} bytes ({len(test_data)/1e6:.1f}MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size, patch_size=8):\n",
    "        assert block_size % patch_size == 0\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "For PG-19, we use **longer context** to capture book structure.\n",
    "\n",
    "| Size | Params | Context | Batch | VRAM Est. |\n",
    "|------|--------|---------|-------|-----------|\n",
    "| small | 30M | 2048 | 16 | ~20GB |\n",
    "| base | 73M | 2048 | 8 | ~35GB |\n",
    "| large | 278M | 2048 | 4 | ~60GB+ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "MODEL_SIZE = 'base'  # Options: 'tiny', 'small', 'base', 'large'\n",
    "# =============================================================\n",
    "\n",
    "preset_map = {\n",
    "    'tiny': HierarchicalBDHConfig.tiny,\n",
    "    'small': HierarchicalBDHConfig.small,\n",
    "    'base': HierarchicalBDHConfig.base,\n",
    "    'large': HierarchicalBDHConfig.large,\n",
    "}\n",
    "model_config = preset_map[MODEL_SIZE](dropout=0.1)\n",
    "\n",
    "# Training config - optimized for long-form text\n",
    "config = {\n",
    "    'model_size': MODEL_SIZE,\n",
    "    'patch_size': model_config.patch_size,\n",
    "    \n",
    "    # Longer context for books\n",
    "    'block_size': 2048,  # 256 patches of 8 bytes\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': {'tiny': 32, 'small': 16, 'base': 8, 'large': 4}[MODEL_SIZE],\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'max_steps': 20000,\n",
    "    'warmup_steps': 500,\n",
    "    \n",
    "    'gradient_accumulation_steps': 4,\n",
    "    \n",
    "    'val_interval': 500,\n",
    "    'val_batches': 100,\n",
    "    'patience': 10,\n",
    "    'log_interval': 100,\n",
    "}\n",
    "\n",
    "assert config['block_size'] % config['patch_size'] == 0\n",
    "effective_batch = config['batch_size'] * config['gradient_accumulation_steps']\n",
    "\n",
    "print(f'Model: {MODEL_SIZE}')\n",
    "print(f'Block size: {config[\"block_size\"]} ({config[\"block_size\"] // config[\"patch_size\"]} patches)')\n",
    "print(f'Effective batch: {effective_batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalBDH(model_config).to(device)\n",
    "params = model.count_parameters()\n",
    "\n",
    "print('\\n' + '-'*50)\n",
    "print(f'HIERARCHICAL BDH - {MODEL_SIZE.upper()}')\n",
    "print('-'*50)\n",
    "print(f'Global: {model_config.global_n_layer}L x {model_config.global_n_embd}D x {model_config.global_n_head}H')\n",
    "print(f'Local:  {model_config.local_n_layer}L x {model_config.local_n_embd}D x {model_config.local_n_head}H')\n",
    "print(f'Total:  {params[\"total\"]:,} ({params[\"total\"]/1e6:.1f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ByteDataset(train_data, config['block_size'], config['patch_size'])\n",
    "val_dataset = ByteDataset(val_data, config['block_size'], config['patch_size'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'Train batches: {len(train_loader):,}')\n",
    "print(f'Val batches: {len(val_loader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'], betas=(0.9, 0.95))\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < config['warmup_steps']:\n",
    "        return config['learning_rate'] * step / config['warmup_steps']\n",
    "    progress = (step - config['warmup_steps']) / (config['max_steps'] - config['warmup_steps'])\n",
    "    return config['learning_rate'] * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print('Optimizer ready with mixed precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    model.train()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    return avg_loss, math.exp(avg_loss), avg_loss / math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'step': [], 'train_loss': [], 'val_loss': [], 'val_ppl': [], 'val_bpb': [], 'lr': []}\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "ckpt_dir = Path(f'checkpoints_hierarchical_{MODEL_SIZE}_pg19')\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f'Checkpoints: {ckpt_dir}')\n",
    "print('Starting training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_iter = iter(train_loader)\n",
    "running_loss = 0\n",
    "\n",
    "pbar = tqdm(range(config['max_steps']), desc='Training')\n",
    "\n",
    "for step in pbar:\n",
    "    optimizer.zero_grad()\n",
    "    accum_loss = 0\n",
    "    \n",
    "    for _ in range(config['gradient_accumulation_steps']):\n",
    "        try:\n",
    "            x, y = next(train_iter)\n",
    "        except StopIteration:\n",
    "            train_iter = iter(train_loader)\n",
    "            x, y = next(train_iter)\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "            loss = loss / config['gradient_accumulation_steps']\n",
    "        scaler.scale(loss).backward()\n",
    "        accum_loss += loss.item()\n",
    "    \n",
    "    lr = get_lr(step)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = lr\n",
    "    \n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    running_loss += accum_loss\n",
    "    \n",
    "    if (step + 1) % config['log_interval'] == 0:\n",
    "        pbar.set_postfix({'loss': f'{running_loss/config[\"log_interval\"]:.3f}', 'lr': f'{lr:.2e}'})\n",
    "        running_loss = 0\n",
    "    \n",
    "    if (step + 1) % config['val_interval'] == 0:\n",
    "        val_loss, val_ppl, val_bpb = evaluate(model, val_loader, config['val_batches'])\n",
    "        train_loss, _, _ = evaluate(model, train_loader, config['val_batches'])\n",
    "        \n",
    "        history['step'].append(step + 1)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        history['val_bpb'].append(val_bpb)\n",
    "        history['lr'].append(lr)\n",
    "        \n",
    "        gap = val_loss - train_loss\n",
    "        print(f'\\nStep {step+1}: train={train_loss:.3f}, val={val_loss:.3f}, ppl={val_ppl:.2f}, bpb={val_bpb:.3f}, gap={gap:.3f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'step': step + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'model_config': model_config.__dict__,\n",
    "                'val_loss': val_loss, 'val_ppl': val_ppl, 'val_bpb': val_bpb,\n",
    "                'dataset': 'pg19',\n",
    "            }, ckpt_dir / 'best.pt')\n",
    "            print(f'  \u2713 New best!')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  No improvement ({patience_counter}/{config[\"patience\"]})')\n",
    "        \n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f'\\nEarly stopping at step {step+1}')\n",
    "            break\n",
    "\n",
    "print('\\nTraining complete!')\n",
    "print(f'Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "with open(ckpt_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "with open(ckpt_dir / 'config.json', 'w') as f:\n",
    "    json.dump({**config, 'model_config': model_config.__dict__, 'parameters': params, 'dataset': 'pg19'}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "axes[0].plot(history['step'], history['train_loss'], 'b-', label='Train')\n",
    "axes[0].plot(history['step'], history['val_loss'], 'r-', label='Val')\n",
    "axes[0].set_xlabel('Step'); axes[0].set_ylabel('Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['step'], history['val_ppl'], 'g-')\n",
    "axes[1].set_xlabel('Step'); axes[1].set_ylabel('Perplexity'); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['step'], history['val_bpb'], 'm-')\n",
    "axes[2].set_xlabel('Step'); axes[2].set_ylabel('BPB'); axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "gaps = [v - t for v, t in zip(history['val_loss'], history['train_loss'])]\n",
    "axes[3].fill_between(history['step'], gaps, alpha=0.5, color='orange')\n",
    "axes[3].set_xlabel('Step'); axes[3].set_ylabel('Gap'); axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Hierarchical BDH ({MODEL_SIZE}) - PG-19 Books', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ckpt_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(ckpt_dir / 'best.pt')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "print('Full validation...')\n",
    "val_loss, val_ppl, val_bpb = evaluate(model, val_loader)\n",
    "print(f'Val: loss={val_loss:.4f}, ppl={val_ppl:.2f}, bpb={val_bpb:.3f}')\n",
    "\n",
    "test_dataset = ByteDataset(test_data, config['block_size'], config['patch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print('\\nTest set...')\n",
    "test_loss, test_ppl, test_bpb = evaluate(model, test_loader)\n",
    "print(f'Test: loss={test_loss:.4f}, ppl={test_ppl:.2f}, bpb={test_bpb:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_new_tokens=300, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([list(prompt.encode('utf-8'))], device=device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    return bytes(output[0].tolist()).decode('utf-8', errors='replace')\n",
    "\n",
    "# Literary prompts appropriate for PG-19\n",
    "prompts = [\n",
    "    'It was a dark and stormy night',\n",
    "    'Chapter 1\\n\\nThe old house stood',\n",
    "    'She looked at him with eyes that',\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('GENERATION SAMPLES (Literary Style)')\n",
    "print('=' * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f'\\nPrompt: \"{prompt}\"')\n",
    "    print('-' * 40)\n",
    "    print(generate_text(model, prompt, max_new_tokens=200))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "zip_name = f'hierarchical_bdh_{MODEL_SIZE}_pg19.zip'\n",
    "subprocess.run(['zip', '-r', zip_name, str(ckpt_dir)], check=True)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_name)\n",
    "    print('\u2705 Download started!')\n",
    "except ImportError:\n",
    "    print(f'Saved: {zip_name}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
