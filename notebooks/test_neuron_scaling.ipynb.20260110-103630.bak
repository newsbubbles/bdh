{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Neuron Scaling Test\n",
    "\n",
    "**Goal**: Test the BDH claim that neuron count (N) matters more than layer count.\n",
    "\n",
    "## Hypothesis\n",
    "BDH's sparse encoding expands to N neurons per head. The claim is that scaling N\n",
    "(via `mlp_internal_dim_multiplier`) is more effective than adding layers.\n",
    "\n",
    "## Experiment Design\n",
    "Compare models with similar parameter counts but different N vs layer ratios:\n",
    "- **Deep & Narrow**: Many layers, few neurons\n",
    "- **Shallow & Wide**: Few layers, many neurons\n",
    "- **Balanced**: Middle ground\n",
    "\n",
    "## Metrics\n",
    "- Validation loss/BPB on WikiText-2\n",
    "- Training efficiency (steps to target loss)\n",
    "- Memory usage\n",
    "- Generation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import dataclasses\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q datasets matplotlib tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla BDH\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class BDHConfig:\n",
    "    n_layer: int = 6\n",
    "    n_embd: int = 256\n",
    "    dropout: float = 0.1\n",
    "    n_head: int = 4\n",
    "    mlp_internal_dim_multiplier: int = 128\n",
    "    vocab_size: int = 256\n",
    "\n",
    "\n",
    "def get_freqs(n, theta, dtype):\n",
    "    def quantize(t, q=2):\n",
    "        return (t / q).floor() * q\n",
    "    return (\n",
    "        1.0 / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n)) / (2 * math.pi)\n",
    "    )\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.freqs = torch.nn.Buffer(\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def phases_cos_sin(phases):\n",
    "        phases = (phases % 1) * (2 * math.pi)\n",
    "        return torch.cos(phases), torch.sin(phases)\n",
    "\n",
    "    @staticmethod\n",
    "    def rope(phases, v):\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.size())\n",
    "        phases_cos, phases_sin = Attention.phases_cos_sin(phases)\n",
    "        return (v * phases_cos).to(v.dtype) + (v_rot * phases_sin).to(v.dtype)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        assert K is Q\n",
    "        _, _, T, _ = Q.size()\n",
    "        r_phases = torch.arange(0, T, device=self.freqs.device, dtype=self.freqs.dtype).view(1, 1, -1, 1) * self.freqs\n",
    "        QR = self.rope(r_phases, Q)\n",
    "        scores = (QR @ QR.mT).tril(diagonal=-1)\n",
    "        return scores @ V\n",
    "\n",
    "\n",
    "class BDH(nn.Module):\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D = config.n_embd\n",
    "        N = config.mlp_internal_dim_multiplier * D // nh\n",
    "        \n",
    "        self.decoder = nn.Parameter(torch.zeros((nh * N, D)).normal_(std=0.02))\n",
    "        self.encoder = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.encoder_v = nn.Parameter(torch.zeros((nh, D, N)).normal_(std=0.02))\n",
    "        self.attn = Attention(config)\n",
    "        self.ln = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.lm_head = nn.Parameter(torch.zeros((D, config.vocab_size)).normal_(std=0.02))\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        C = self.config\n",
    "        B, T = idx.size()\n",
    "        D = C.n_embd\n",
    "        nh = C.n_head\n",
    "        N = D * C.mlp_internal_dim_multiplier // nh\n",
    "        \n",
    "        x = self.embed(idx).unsqueeze(1)\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        for level in range(C.n_layer):\n",
    "            x_latent = x @ self.encoder\n",
    "            x_sparse = F.relu(x_latent)\n",
    "            yKV = self.attn(Q=x_sparse, K=x_sparse, V=x)\n",
    "            yKV = self.ln(yKV)\n",
    "            y_latent = yKV @ self.encoder_v\n",
    "            y_sparse = F.relu(y_latent)\n",
    "            xy_sparse = x_sparse * y_sparse\n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "            yMLP = xy_sparse.transpose(1, 2).reshape(B, 1, T, N * nh) @ self.decoder\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)\n",
    "        \n",
    "        logits = x.view(B, T, D) @ self.lm_head\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def compute_neurons(config):\n",
    "    return config.mlp_internal_dim_multiplier * config.n_embd // config.n_head\n",
    "\n",
    "print('BDH defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "Design models with ~similar parameter counts but different depth/width ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaling configurations\n",
    "# Target: ~10-15M parameters each\n",
    "\n",
    "SCALING_CONFIGS = {\n",
    "    # Deep & Narrow: Many layers, fewer neurons\n",
    "    'deep_narrow': BDHConfig(\n",
    "        n_layer=12,           # More layers\n",
    "        n_embd=128,           # Smaller embedding\n",
    "        n_head=4,\n",
    "        mlp_internal_dim_multiplier=64,  # N = 64*128/4 = 2048 neurons\n",
    "        dropout=0.1,\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "    \n",
    "    # Balanced: Middle ground\n",
    "    'balanced': BDHConfig(\n",
    "        n_layer=6,            # Moderate layers\n",
    "        n_embd=256,           # Moderate embedding\n",
    "        n_head=4,\n",
    "        mlp_internal_dim_multiplier=64,  # N = 64*256/4 = 4096 neurons\n",
    "        dropout=0.1,\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "    \n",
    "    # Shallow & Wide: Few layers, many neurons\n",
    "    'shallow_wide': BDHConfig(\n",
    "        n_layer=2,            # Fewer layers\n",
    "        n_embd=256,           # Same embedding\n",
    "        n_head=4,\n",
    "        mlp_internal_dim_multiplier=256,  # N = 256*256/4 = 16384 neurons\n",
    "        dropout=0.1,\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "    \n",
    "    # Extra wide: Push neurons further\n",
    "    'extra_wide': BDHConfig(\n",
    "        n_layer=1,            # Single layer!\n",
    "        n_embd=256,\n",
    "        n_head=4,\n",
    "        mlp_internal_dim_multiplier=512,  # N = 512*256/4 = 32768 neurons\n",
    "        dropout=0.1,\n",
    "        vocab_size=256,\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Print config comparison\n",
    "print('Configuration Comparison:')\n",
    "print('='*80)\n",
    "print(f\"{'Config':<15} {'Layers':<8} {'Embd':<8} {'Neurons':<10} {'Params':<12} {'N/Layer':<10}\")\n",
    "print('-'*80)\n",
    "\n",
    "for name, cfg in SCALING_CONFIGS.items():\n",
    "    model = BDH(cfg)\n",
    "    n_params = count_parameters(model)\n",
    "    neurons = compute_neurons(cfg)\n",
    "    print(f\"{name:<15} {cfg.n_layer:<8} {cfg.n_embd:<8} {neurons:<10,} {n_params:<12,} {neurons//cfg.n_layer:<10,}\")\n",
    "    del model\n",
    "\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load WikiText-2\n",
    "print('Loading WikiText-2...')\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "def encode_text(text):\n",
    "    return [b for b in text.encode('utf-8')]\n",
    "\n",
    "# Prepare data\n",
    "train_text = '\\n'.join(dataset['train']['text'])\n",
    "val_text = '\\n'.join(dataset['validation']['text'])\n",
    "\n",
    "train_data = torch.tensor(encode_text(train_text), dtype=torch.long)\n",
    "val_data = torch.tensor(encode_text(val_text), dtype=torch.long)\n",
    "\n",
    "print(f'Train: {len(train_data):,} bytes')\n",
    "print(f'Val: {len(val_data):,} bytes')\n",
    "\n",
    "# Batch function\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CONFIG = {\n",
    "    'max_steps': 3000,\n",
    "    'batch_size': 32,\n",
    "    'block_size': 512,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.1,\n",
    "    'eval_interval': 100,\n",
    "    'eval_iters': 20,\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_size, block_size):\n",
    "    model.eval()\n",
    "    losses = {'train': [], 'val': []}\n",
    "    for split in ['train', 'val']:\n",
    "        for _ in range(eval_iters):\n",
    "            x, y = get_batch(split, batch_size, block_size)\n",
    "            _, loss = model(x, y)\n",
    "            losses[split].append(loss.item())\n",
    "    return {k: np.mean(v) for k, v in losses.items()}\n",
    "\n",
    "\n",
    "def train_model(config_name, config, train_config):\n",
    "    \"\"\"Train a model and return metrics history\"\"\"\n",
    "    print(f'\\n{\"="*60}')\n",
    "    print(f'Training: {config_name}')\n",
    "    print(f'Layers: {config.n_layer}, Neurons: {compute_neurons(config):,}')\n",
    "    print('='*60)\n",
    "    \n",
    "    model = BDH(config).to(device)\n",
    "    n_params = count_parameters(model)\n",
    "    print(f'Parameters: {n_params:,}')\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=train_config['learning_rate'],\n",
    "        weight_decay=train_config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'step': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'bpb': [],\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(range(train_config['max_steps']), desc=config_name)\n",
    "    \n",
    "    for step in pbar:\n",
    "        model.train()\n",
    "        x, y = get_batch('train', train_config['batch_size'], train_config['block_size'])\n",
    "        _, loss = model(x, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % train_config['eval_interval'] == 0:\n",
    "            losses = estimate_loss(model, train_config['eval_iters'], \n",
    "                                   train_config['batch_size'], train_config['block_size'])\n",
    "            bpb = losses['val'] / math.log(2)\n",
    "            \n",
    "            history['step'].append(step)\n",
    "            history['train_loss'].append(losses['train'])\n",
    "            history['val_loss'].append(losses['val'])\n",
    "            history['bpb'].append(bpb)\n",
    "            \n",
    "            if losses['val'] < best_val_loss:\n",
    "                best_val_loss = losses['val']\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'train': f\"{losses['train']:.3f}\",\n",
    "                'val': f\"{losses['val']:.3f}\",\n",
    "                'bpb': f'{bpb:.3f}',\n",
    "            })\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_losses = estimate_loss(model, 50, train_config['batch_size'], train_config['block_size'])\n",
    "    \n",
    "    results = {\n",
    "        'config_name': config_name,\n",
    "        'n_layer': config.n_layer,\n",
    "        'neurons': compute_neurons(config),\n",
    "        'params': n_params,\n",
    "        'final_train_loss': final_losses['train'],\n",
    "        'final_val_loss': final_losses['val'],\n",
    "        'final_bpb': final_losses['val'] / math.log(2),\n",
    "        'best_val_loss': best_val_loss,\n",
    "    }\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return history, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Scaling Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all configurations\n",
    "all_histories = {}\n",
    "all_results = {}\n",
    "\n",
    "for name, config in SCALING_CONFIGS.items():\n",
    "    try:\n",
    "        history, results = train_model(name, config, TRAIN_CONFIG)\n",
    "        all_histories[name] = history\n",
    "        all_results[name] = results\n",
    "    except RuntimeError as e:\n",
    "        if 'out of memory' in str(e):\n",
    "            print(f'OOM for {name}, skipping...')\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "print('\\nAll training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print('\\n' + '='*90)\n",
    "print('SCALING EXPERIMENT RESULTS')\n",
    "print('='*90)\n",
    "print(f\"{'Config':<15} {'Layers':<8} {'Neurons':<10} {'Params':<12} {'Val Loss':<10} {'BPB':<8}\")\n",
    "print('-'*90)\n",
    "\n",
    "# Sort by BPB\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1]['final_bpb'])\n",
    "\n",
    "for name, r in sorted_results:\n",
    "    print(f\"{name:<15} {r['n_layer']:<8} {r['neurons']:<10,} {r['params']:<12,} {r['final_val_loss']:<10.4f} {r['final_bpb']:<8.3f}\")\n",
    "\n",
    "print('='*90)\n",
    "print('\\nBest config:', sorted_results[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "ax1 = axes[0, 0]\n",
    "for name, history in all_histories.items():\n",
    "    ax1.plot(history['step'], history['val_loss'], label=name, linewidth=2)\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Validation Loss')\n",
    "ax1.set_title('Training Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: BPB curves\n",
    "ax2 = axes[0, 1]\n",
    "for name, history in all_histories.items():\n",
    "    ax2.plot(history['step'], history['bpb'], label=name, linewidth=2)\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Bits per Byte')\n",
    "ax2.set_title('BPB Over Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Neurons vs BPB\n",
    "ax3 = axes[1, 0]\n",
    "neurons = [r['neurons'] for r in all_results.values()]\n",
    "bpbs = [r['final_bpb'] for r in all_results.values()]\n",
    "names = list(all_results.keys())\n",
    "ax3.scatter(neurons, bpbs, s=100)\n",
    "for i, name in enumerate(names):\n",
    "    ax3.annotate(name, (neurons[i], bpbs[i]), textcoords='offset points', xytext=(5, 5))\n",
    "ax3.set_xlabel('Neurons (N)')\n",
    "ax3.set_ylabel('Final BPB')\n",
    "ax3.set_title('Neurons vs Performance')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Layers vs BPB\n",
    "ax4 = axes[1, 1]\n",
    "layers = [r['n_layer'] for r in all_results.values()]\n",
    "ax4.scatter(layers, bpbs, s=100)\n",
    "for i, name in enumerate(names):\n",
    "    ax4.annotate(name, (layers[i], bpbs[i]), textcoords='offset points', xytext=(5, 5))\n",
    "ax4.set_xlabel('Layers')\n",
    "ax4.set_ylabel('Final BPB')\n",
    "ax4.set_title('Layers vs Performance')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('BDH Scaling: Neurons vs Layers', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('neuron_scaling_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency analysis: BPB per parameter\n",
    "print('\\nEfficiency Analysis:')\n",
    "print('='*70)\n",
    "print(f\"{'Config':<15} {'Params':<12} {'BPB':<8} {'BPB/1M params':<15}\")\n",
    "print('-'*70)\n",
    "\n",
    "for name, r in sorted_results:\n",
    "    efficiency = r['final_bpb'] / (r['params'] / 1e6)\n",
    "    print(f\"{name:<15} {r['params']:<12,} {r['final_bpb']:<8.3f} {efficiency:<15.4f}\")\n",
    "\n",
    "print('='*70)\n",
    "print('Lower BPB/1M params = more efficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the claim: Does N matter more than layers?\n",
    "print('\\n' + '='*70)\n",
    "print('ANALYSIS: Does neuron count (N) matter more than layers?')\n",
    "print('='*70)\n",
    "\n",
    "# Compute correlation\n",
    "import numpy as np\n",
    "\n",
    "neurons_arr = np.array([r['neurons'] for r in all_results.values()])\n",
    "layers_arr = np.array([r['n_layer'] for r in all_results.values()])\n",
    "bpb_arr = np.array([r['final_bpb'] for r in all_results.values()])\n",
    "\n",
    "# Correlation (negative = more neurons/layers -> lower BPB = better)\n",
    "corr_neurons = np.corrcoef(neurons_arr, bpb_arr)[0, 1]\n",
    "corr_layers = np.corrcoef(layers_arr, bpb_arr)[0, 1]\n",
    "\n",
    "print(f'\\nCorrelation with BPB (negative = better):')\n",
    "print(f'  Neurons vs BPB: {corr_neurons:.3f}')\n",
    "print(f'  Layers vs BPB:  {corr_layers:.3f}')\n",
    "\n",
    "if abs(corr_neurons) > abs(corr_layers):\n",
    "    print(f'\\n>>> RESULT: Neurons have STRONGER correlation with performance')\n",
    "    print(f'    This SUPPORTS the BDH claim that N matters more than depth.')\n",
    "else:\n",
    "    print(f'\\n>>> RESULT: Layers have STRONGER correlation with performance')\n",
    "    print(f'    This CONTRADICTS the BDH claim about N importance.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_data = {\n",
    "    'train_config': TRAIN_CONFIG,\n",
    "    'results': all_results,\n",
    "    'histories': all_histories,\n",
    "    'analysis': {\n",
    "        'corr_neurons_bpb': float(corr_neurons),\n",
    "        'corr_layers_bpb': float(corr_layers),\n",
    "        'best_config': sorted_results[0][0],\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('neuron_scaling_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print('Results saved to neuron_scaling_results.json')\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('neuron_scaling_results.json')\n",
    "    files.download('neuron_scaling_results.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}