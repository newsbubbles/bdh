{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Benchmark Evaluation\n",
    "\n",
    "Evaluate trained BDH/Hierarchical BDH models on standard benchmarks.\n",
    "\n",
    "## Benchmarks Included\n",
    "\n",
    "| Benchmark | What it Tests | Metric |\n",
    "|-----------|---------------|--------|\n",
    "| **LAMBADA** | Long-range word prediction | Accuracy, PPL |\n",
    "| **WikiText-2** | General LM | PPL, BPB |\n",
    "| **WikiText-103** | Large-scale LM | PPL, BPB |\n",
    "| **PG-19** | Long-form books | PPL, BPB |\n",
    "| **1BW** | Billion Word Benchmark | PPL |\n",
    "\n",
    "## Usage\n",
    "1. Upload your trained model checkpoint (.zip or .pt)\n",
    "2. Select which benchmarks to run\n",
    "3. Get standardized metrics for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/newsbubbles/bdh.git 2>/dev/null || (cd bdh && git pull)\n",
    "%cd bdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model Checkpoint\n",
    "\n",
    "Upload your trained model. Supports:\n",
    "- `.zip` file from training notebooks\n",
    "- `.pt` checkpoint file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload checkpoint\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print('Upload your model checkpoint (.zip or .pt):')\n",
    "    uploaded = files.upload()\n",
    "    uploaded_file = list(uploaded.keys())[0]\n",
    "    print(f'Uploaded: {uploaded_file}')\n",
    "except ImportError:\n",
    "    # Not in Colab - specify path manually\n",
    "    uploaded_file = 'checkpoints_hierarchical_small/best.pt'  # Edit this\n",
    "    print(f'Using local file: {uploaded_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract if zip\n",
    "if uploaded_file.endswith('.zip'):\n",
    "    print('Extracting zip...')\n",
    "    with zipfile.ZipFile(uploaded_file, 'r') as z:\n",
    "        z.extractall('.')\n",
    "    # Find the checkpoint\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        for f in files:\n",
    "            if f == 'best.pt':\n",
    "                checkpoint_path = os.path.join(root, f)\n",
    "                break\n",
    "    print(f'Found checkpoint: {checkpoint_path}')\n",
    "else:\n",
    "    checkpoint_path = uploaded_file\n",
    "\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint...')\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "print(f'Loaded from step {ckpt.get(\"step\", \"unknown\")}')\n",
    "print(f'Original val_loss: {ckpt.get(\"val_loss\", \"unknown\")}')\n",
    "print(f'Original val_ppl: {ckpt.get(\"val_ppl\", \"unknown\")}')\n",
    "print(f'Dataset: {ckpt.get(\"dataset\", \"unknown\")}')\n",
    "print(f'\\nModel config: {ckpt[\"model_config\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect model type and load\n",
    "model_config = ckpt['model_config']\n",
    "\n",
    "# Check if hierarchical (has global_n_layer) or standard BDH\n",
    "is_hierarchical = 'global_n_layer' in model_config\n",
    "\n",
    "if is_hierarchical:\n",
    "    print('Detected: Hierarchical BDH')\n",
    "    from bdh_hierarchical import HierarchicalBDH, HierarchicalBDHConfig\n",
    "    config = HierarchicalBDHConfig(**model_config)\n",
    "    model = HierarchicalBDH(config).to(device)\n",
    "else:\n",
    "    print('Detected: Standard BDH')\n",
    "    from bdh import BDH, BDHConfig\n",
    "    config = BDHConfig(**model_config)\n",
    "    model = BDH(config).to(device)\n",
    "\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Count params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Parameters: {total_params:,} ({total_params/1e6:.1f}M)')\n",
    "\n",
    "# Get patch size\n",
    "patch_size = getattr(config, 'patch_size', 1)\n",
    "print(f'Patch size: {patch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, data, block_size, patch_size=8):\n",
    "        # Ensure divisibility\n",
    "        block_size = (block_size // patch_size) * patch_size\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(1, len(self.data) - self.block_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "def text_to_bytes(text):\n",
    "    return torch.tensor(list(text.encode('utf-8')), dtype=torch.long)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_perplexity(model, loader, desc='Evaluating'):\n",
    "    \"\"\"Compute perplexity on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for x, y in tqdm(loader, desc=desc):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _, loss = model(x, y)\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        total_tokens += y.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    bpb = avg_loss / math.log(2)\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'perplexity': perplexity,\n",
    "        'bpb': bpb,\n",
    "        'tokens': total_tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Selection\n",
    "\n",
    "Choose which benchmarks to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# SELECT BENCHMARKS\n",
    "# =============================================================\n",
    "RUN_LAMBADA = True\n",
    "RUN_WIKITEXT2 = True\n",
    "RUN_WIKITEXT103 = True\n",
    "RUN_PG19 = False  # Large, takes time\n",
    "\n",
    "# Evaluation settings\n",
    "BLOCK_SIZE = 512  # Context window\n",
    "BATCH_SIZE = 16\n",
    "# =============================================================\n",
    "\n",
    "# Ensure block_size is divisible by patch_size\n",
    "BLOCK_SIZE = (BLOCK_SIZE // patch_size) * patch_size\n",
    "print(f'Block size: {BLOCK_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMBADA Benchmark\n",
    "\n",
    "Tests long-range dependency by predicting final word of passages.\n",
    "Standard LM benchmark for context understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambada_results = None\n",
    "\n",
    "if RUN_LAMBADA:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('LAMBADA BENCHMARK')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    print('Loading LAMBADA...')\n",
    "    lambada = load_dataset('lambada', split='test')\n",
    "    print(f'Test examples: {len(lambada)}')\n",
    "    \n",
    "    # For byte-level: evaluate perplexity on full passages\n",
    "    # Also compute accuracy on final word prediction\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for example in tqdm(lambada, desc='LAMBADA'):\n",
    "        text = example['text']\n",
    "        \n",
    "        # Split into context and target word\n",
    "        words = text.rsplit(' ', 1)\n",
    "        if len(words) != 2:\n",
    "            continue\n",
    "        context, target_word = words\n",
    "        context = context + ' '  # Include space before target\n",
    "        \n",
    "        # Encode\n",
    "        context_bytes = list(context.encode('utf-8'))\n",
    "        target_bytes = list(target_word.encode('utf-8'))\n",
    "        full_bytes = context_bytes + target_bytes\n",
    "        \n",
    "        # Truncate context if too long\n",
    "        if len(full_bytes) > BLOCK_SIZE:\n",
    "            context_bytes = context_bytes[-(BLOCK_SIZE - len(target_bytes)):]\n",
    "            full_bytes = context_bytes + target_bytes\n",
    "        \n",
    "        if len(full_bytes) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Compute loss on target word bytes\n",
    "        x = torch.tensor([full_bytes[:-1]], device=device, dtype=torch.long)\n",
    "        y = torch.tensor([full_bytes[1:]], device=device, dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits, loss = model(x, y)\n",
    "        \n",
    "        # Loss on target portion only\n",
    "        target_start = len(context_bytes) - 1\n",
    "        target_logits = logits[0, target_start:]\n",
    "        target_labels = y[0, target_start:]\n",
    "        \n",
    "        # Check if predictions match\n",
    "        predictions = target_logits.argmax(dim=-1)\n",
    "        if torch.equal(predictions, target_labels):\n",
    "            correct += 1\n",
    "        \n",
    "        # Accumulate loss\n",
    "        target_loss = nn.functional.cross_entropy(\n",
    "            target_logits, target_labels, reduction='sum'\n",
    "        )\n",
    "        total_loss += target_loss.item()\n",
    "        total_tokens += len(target_labels)\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    ppl = math.exp(avg_loss)\n",
    "    \n",
    "    lambada_results = {\n",
    "        'accuracy': accuracy,\n",
    "        'perplexity': ppl,\n",
    "        'loss': avg_loss,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "    }\n",
    "    \n",
    "    print(f'\\nLAMBADA Results:')\n",
    "    print(f'  Accuracy: {accuracy:.2f}%')\n",
    "    print(f'  Perplexity: {ppl:.2f}')\n",
    "    print(f'  ({correct}/{total} correct)')\n",
    "else:\n",
    "    print('Skipping LAMBADA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText-2 Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext2_results = None\n",
    "\n",
    "if RUN_WIKITEXT2:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('WIKITEXT-2 BENCHMARK')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    print('Loading WikiText-2...')\n",
    "    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "    \n",
    "    test_text = '\\n'.join(dataset['test']['text'])\n",
    "    test_data = text_to_bytes(test_text)\n",
    "    print(f'Test size: {len(test_data):,} bytes')\n",
    "    \n",
    "    test_dataset = ByteDataset(test_data, BLOCK_SIZE, patch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    wikitext2_results = evaluate_perplexity(model, test_loader, 'WikiText-2')\n",
    "    \n",
    "    print(f'\\nWikiText-2 Results:')\n",
    "    print(f'  Loss: {wikitext2_results[\"loss\"]:.4f}')\n",
    "    print(f'  Perplexity: {wikitext2_results[\"perplexity\"]:.2f}')\n",
    "    print(f'  BPB: {wikitext2_results[\"bpb\"]:.3f}')\n",
    "    \n",
    "    del test_data, test_dataset, test_loader\n",
    "else:\n",
    "    print('Skipping WikiText-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiText-103 Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext103_results = None\n",
    "\n",
    "if RUN_WIKITEXT103:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('WIKITEXT-103 BENCHMARK')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    print('Loading WikiText-103 test set...')\n",
    "    dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "    \n",
    "    test_text = '\\n'.join(dataset['test']['text'])\n",
    "    test_data = text_to_bytes(test_text)\n",
    "    print(f'Test size: {len(test_data):,} bytes')\n",
    "    \n",
    "    test_dataset = ByteDataset(test_data, BLOCK_SIZE, patch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    wikitext103_results = evaluate_perplexity(model, test_loader, 'WikiText-103')\n",
    "    \n",
    "    print(f'\\nWikiText-103 Results:')\n",
    "    print(f'  Loss: {wikitext103_results[\"loss\"]:.4f}')\n",
    "    print(f'  Perplexity: {wikitext103_results[\"perplexity\"]:.2f}')\n",
    "    print(f'  BPB: {wikitext103_results[\"bpb\"]:.3f}')\n",
    "    \n",
    "    del test_data, test_dataset, test_loader\n",
    "else:\n",
    "    print('Skipping WikiText-103')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG-19 Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg19_results = None\n",
    "\n",
    "if RUN_PG19:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('PG-19 BENCHMARK')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    print('Loading PG-19 test set...')\n",
    "    dataset = load_dataset('pg19', split='test[:100]')  # Sample for speed\n",
    "    \n",
    "    test_text = '\\n\\n'.join(dataset['text'])\n",
    "    test_data = text_to_bytes(test_text)\n",
    "    print(f'Test size: {len(test_data):,} bytes')\n",
    "    \n",
    "    test_dataset = ByteDataset(test_data, BLOCK_SIZE, patch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    pg19_results = evaluate_perplexity(model, test_loader, 'PG-19')\n",
    "    \n",
    "    print(f'\\nPG-19 Results:')\n",
    "    print(f'  Loss: {pg19_results[\"loss\"]:.4f}')\n",
    "    print(f'  Perplexity: {pg19_results[\"perplexity\"]:.2f}')\n",
    "    print(f'  BPB: {pg19_results[\"bpb\"]:.3f}')\n",
    "    \n",
    "    del test_data, test_dataset, test_loader\n",
    "else:\n",
    "    print('Skipping PG-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print('BENCHMARK RESULTS SUMMARY')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print(f'Model: {\"Hierarchical BDH\" if is_hierarchical else \"BDH\"}')\n",
    "print(f'Parameters: {total_params:,} ({total_params/1e6:.1f}M)')\n",
    "print(f'Trained on: {ckpt.get(\"dataset\", \"unknown\")}')\n",
    "print()\n",
    "print(f'{\"Benchmark\":<20} {\"Loss\":>10} {\"PPL\":>10} {\"BPB\":>10} {\"Acc\":>10}')\n",
    "print('-' * 70)\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "if lambada_results:\n",
    "    print(f'{\"LAMBADA\":<20} {lambada_results[\"loss\"]:>10.4f} {lambada_results[\"perplexity\"]:>10.2f} {\"N/A\":>10} {lambada_results[\"accuracy\"]:>9.2f}%')\n",
    "    results_summary['lambada'] = lambada_results\n",
    "\n",
    "if wikitext2_results:\n",
    "    print(f'{\"WikiText-2\":<20} {wikitext2_results[\"loss\"]:>10.4f} {wikitext2_results[\"perplexity\"]:>10.2f} {wikitext2_results[\"bpb\"]:>10.3f} {\"N/A\":>10}')\n",
    "    results_summary['wikitext2'] = wikitext2_results\n",
    "\n",
    "if wikitext103_results:\n",
    "    print(f'{\"WikiText-103\":<20} {wikitext103_results[\"loss\"]:>10.4f} {wikitext103_results[\"perplexity\"]:>10.2f} {wikitext103_results[\"bpb\"]:>10.3f} {\"N/A\":>10}')\n",
    "    results_summary['wikitext103'] = wikitext103_results\n",
    "\n",
    "if pg19_results:\n",
    "    print(f'{\"PG-19\":<20} {pg19_results[\"loss\"]:>10.4f} {pg19_results[\"perplexity\"]:>10.2f} {pg19_results[\"bpb\"]:>10.3f} {\"N/A\":>10}')\n",
    "    results_summary['pg19'] = pg19_results\n",
    "\n",
    "print('-' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results\n",
    "output = {\n",
    "    'model_type': 'hierarchical_bdh' if is_hierarchical else 'bdh',\n",
    "    'parameters': total_params,\n",
    "    'trained_on': ckpt.get('dataset', 'unknown'),\n",
    "    'original_val_loss': ckpt.get('val_loss'),\n",
    "    'original_val_ppl': ckpt.get('val_ppl'),\n",
    "    'block_size': BLOCK_SIZE,\n",
    "    'benchmarks': results_summary,\n",
    "}\n",
    "\n",
    "output_path = 'benchmark_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f'\\nResults saved to {output_path}')\n",
    "\n",
    "# Download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: Published Results\n",
    "\n",
    "For comparison with other models:\n",
    "\n",
    "| Model | Params | WikiText-103 PPL | LAMBADA Acc |\n",
    "|-------|--------|------------------|-------------|\n",
    "| GPT-2 Small | 117M | 37.5 | 45.9% |\n",
    "| GPT-2 Medium | 345M | 26.4 | 55.5% |\n",
    "| GPT-2 Large | 762M | 22.1 | 60.1% |\n",
    "| Transformer-XL | 257M | 18.3 | - |\n",
    "| MEGABYTE (350M) | 350M | - | - |\n",
    "\n",
    "**Note**: Direct comparison is tricky because:\n",
    "- BDH uses byte-level (not BPE tokens)\n",
    "- Perplexity scales differ between tokenizations\n",
    "- BPB (bits-per-byte) is more comparable across tokenizations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "T4", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
